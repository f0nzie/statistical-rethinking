[
["index.html", "Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 This is a love letter Why this? My assumptions about you How to use and understand this project You can do this, too We have updates", " Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 A Solomon Kurz 2020-07-12 This is a love letter I love McElreath’s Statistical Rethinking text. It’s the entry-level textbook for applied researchers I spent years looking for. McElreath’s freely-available lectures on the book are really great, too. However, I prefer using Bürkner’s brms package when doing Bayesian regression in R. It’s just spectacular. I also prefer plotting with Wickham’s ggplot2, and coding with functions and principles from the tidyverse, which you might learn about here or here. So, this project is an attempt to reexpress the code in McElreath’s textbook. His models are re-fit with brms, the figures are reproduced or reimagined with ggplot2, and the general data wrangling code now predominantly follows the tidyverse style. Why this? I’m not a statistician and I have no formal background in computer science. Though I benefited from a suite of statistics courses in grad school, a large portion of my training has been outside of the classroom, working with messy real-world data, and searching online for help. One of the great resources I happened on was idre, the UCLA Institute for Digital Education, which offers an online portfolio of richly annotated textbook examples. Their online tutorials are among the earliest inspirations for this project. We need more resources like them. With that in mind, one of the strengths of McElreath’s text is its thorough integration with the rethinking package. The rethinking package is a part of the R ecosystem, which is great because R is free and open source. And McElreath has made the source code for rethinking publically available, too. Since he completed his text, many other packages have been developed to help users of the R ecosystem interface with Stan. Of those alternative packages, I think Bürkner’s brms is the best for general-purpose Bayesian data analysis. It’s flexible, uses reasonably-approachable syntax, has sensible defaults, and offers a vast array of post-processing convenience functions. And brms has only gotten better over time. To my knowledge, there are no textbooks on the market that highlight the brms package, which seems like an evil worth correcting. In addition, McElreath’s data wrangling code is based in the base R style and he made most of his figures with base R plots. Though there are benefits to sticking close to base R functions (e.g., less dependencies leading to a lower likelihood that your code will break in the future), there are downsides. For beginners, base R functions can be difficult both to learn and to read. Happily, in recent years Hadley Wickham and others have been developing a group of packages collectively called the tidyverse. These tidyverse packages (e.g., dplyr, tidyr, purrr) were developed according to an underlying philosophy and they are designed to work together coherently and seamlessly. Though not all within the R community share this opinion, I am among those who think the tydyverse style of coding is generally easier to learn and sufficiently powerful that these packages can accommodate the bulk of your data needs. I also find tydyverse-style syntax easier to read. And of course, the widely-used ggplot2 package is part of the tidyverse, too. To be clear, students can get a great education in both Bayesian statistics and programming in R with McElreath’s text just the way it is. Just go slow, work through all the examples, and read the text closely. It’s a pedagogical boon. I could not have done better or even closely so. But what I can offer is a parallel introduction on how to fit the statistical models with the ever-improving and already-quite-impressive brms package. I can throw in examples of how to perform other operations according to the ethic of the tidyverse. And I can also offer glimpses of some of the other great packages in the R + Stan ecosystem, such as loo, bayesplot, and tidybayes. My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic, or a researcher of some sort. So I’m presuming you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons before diving into Statistical Rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s R Programming for Data Science. And the best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science, which I extensively link to throughout this project. That said, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to McElreath’s Statistical Rethinking text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, some of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS, Healy’s Data Visualization: A practical introduction, or Wilke’s Fundamentals of Data Visualization. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide, and R Markdown: The Definitive Guide. R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::mode_hdi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. You can do this, too This project is powered by Yihui Xie’s bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s Happy Git and GitHub for the useR. I’ve even blogged about what it was like putting together the first version of this project. The source code of the project is available here. We have updates I released the initial 0.9.0 version of this project in September 26, 2018. In April 19, 2019 came the 1.0.0 version. Some of the major changes were: All models were refit with the current official version of brms, 2.8.0. Adopting the seed argument within the brm() function made the model results more reproducible. The loo package was updated. As a consequence, our workflow for the WAIC and LOO changed, too. I improved the brms alternative to McElreath’s coeftab() function. I made better use of the tidyverse, especially some of the purrr functions. Particularly in the later chapters, there’s a greater emphasis on functions from the tidybayes package. Chapter 11 contains the updated brms 2.8.0 workflow for making custom distributions, using the beta-binomial model as the example. Chapter 12 received a new bonus section contrasting different methods for working with multilevel posteriors. Chapter 14 received a new bonus section introducing Bayesian meta-analysis and linking it to multilevel and measurement-error models. With the help of others within the community, I corrected many typos and streamlined some of the code (e.g., dropped an unnecessary use of the mi() function in section 14.2.1) And in some cases, I corrected sections that were just plain wrong (e.g., some of my initial attempts in section 3.3 were incorrect). In response to some reader requests, we finally have a PDF version! Making that happen required some formatting adjustments, resulting in version 1.0.1. Noteworthy changes include: Major revisions to the LaTeX syntax underlying many of the in-text equations (e.g., dropping the “eqnarray” environment for &quot;align*&quot;) Adjusting some of the image syntax Updating the reference for the Bayesian \\(R^2\\) (Gelman, Goodrich, Gabry, &amp; Vehtari, 2018) Though we’re into version 1.0.1, there’s room for improvement. There are still two models that need work. The current solution for model 10.6 is wrong, which I try to make clear in the prose. It also appears that the Gaussian process model from section 13.4 is off. Both models are beyond my current skill set and friendly suggestions are welcome. In addition to modeling concerns, typos may yet be looming and I’m sure there are places where the code could be made more streamlined, more elegant, or just more in-line with the tidyverse style. Which is all to say, I hope to release better and more useful updates in the future. Before we move on, I’d like to thank the following for their helpful contributions: Paul-Christian Bürkner (@paul-buerkner), Andrew Collier (@datawookie), Jeff Hammerbacher (@hammer), Matthew Kay (@mjskay), TJ Mahr (@tjmahr), Stijn Masschelein (@stijnmasschelein), Colin Quirk (@colinquirk), Rishi Sadhir (@RishiSadhir), Richard Torkar (@torkar), Aki Vehtari (@avehtari). "],
["the-golem-of-prague.html", "1 The Golem of Prague 1 Statistical golems Reference Session info", " 1 The Golem of Prague Figure 1.1: Rabbi Loew and Golem by Mikoláš Aleš, 1899 As he opened the chapter, McElreath told us that ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. 1 Statistical golems Scientists also make golems. Our golems rarely have physical form, but they too are often made of clay, living in silicon as computer code. These golems are scientific model. But these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with truth enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations. (p. 1, emphasis in the original) There are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pour over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro. Anyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by watching McElreath present it? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.11 tools_3.6.3 ## [5] htmltools_0.3.6 yaml_2.2.0 Rcpp_1.0.1 stringi_1.4.3 ## [9] rmarkdown_1.13 highr_0.8 knitr_1.23 stringr_1.4.0 ## [13] xfun_0.7 digest_0.6.19 evaluate_0.14 "],
["small-worlds-and-large-worlds.html", "2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Reference Session info", " 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken wrote a great paper by this name. 2.1.1 Counting possibilities. Throughout this project, we’ll use the tidyverse for data wrangling. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. Other than the pipe, the other big thing to be aware of is tibbles. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of data frames. So whenever we talk about data frames, we’re also talking about tibbles. For more on the topic, check out R4SD, Chapter 10. So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) head(d) ## # A tibble: 4 x 5 ## p_1 p_2 p_3 p_4 p_5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% gather() %&gt;% mutate(x = rep(1:4, times = 5), possibility = rep(1:5, each = 4)) %&gt;% ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data Wrangling Part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and dplyr::gather(). Here’s the basic structure of the possibilities per marble draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # … with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3) / (4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # … with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # … with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # … with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x){ rowSums(x == &quot;b&quot;) } n_white &lt;- function(x){ rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # … with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index){ ifelse(index == &quot;a&quot;, position, ifelse(index == &quot;b&quot;, position + 4, position + 4 * 2) ) } If you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of R Programming for Data Science. Anyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # … with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # … with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(# `pie_index == &quot;a&quot;` rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # `pie_index == &quot;b&quot;` rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # `pie_index == &quot;c&quot;` rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() 2.1.2 Using prior information. We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) Here’s the table in the middle of page 25. t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 We might update to reproduce the table a the top of page 26, like this. t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post Data Wrangling Part 1: Basic to Advanced Ways to Select Columns. 2.1.3 From counts to probability. The opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as: plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of In other words: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} \\propto \\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p \\] But since we have to standardize the results to get them into a probability metric, the full equation is: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} = \\frac{\\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}} \\] You might make the table in the middle of page 27 like this. t %&gt;% select(p_1:p_4) %&gt;% mutate(p = seq(from = 0, to = 1, by = .25), `ways to produce data` = c(0, 3, 8, 9, 0)) %&gt;% mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 p `ways to produce data` plausibility ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 0.25 3 0.15 ## 3 b b w w 0.5 8 0.4 ## 4 b b b w 0.75 9 0.45 ## 5 b b b b 1 0 0 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == &quot;w&quot;), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials) ) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, # putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% # plot! ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here. 2.2.3 Evaluate. It’s worth repeating the Rethinking: Deflationary statistics box, here. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) 2.3 Components of the model a likelihood function: “the number of ways each conjecture could produce an observation” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data” a prior: “the initial plausibility of each conjectured cause of the data” 2.3.1 Likelihood. If you let the count of water be \\(w\\) and the number of tosses be \\(n\\), then the binomial likelihood may be expressed as: \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Given a probability of .5, the binomial likelihood of 6 out of 9 tosses coming out water is: dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Let’s do so over the parameter space. tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2 Parameters. McElreath started off his Rethinking: Datum or parameter? box with: It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34) For more in this topic, check out his lecture Understanding Bayesian Statistics without Frequentist Language. 2.3.3 Prior. So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35) To learn more about “regularizing or weakly informative priors,” check out the Prior Choice Recommendations wiki from the Stan team. 2.3.3.1 Overthinking: Prior as a probability distribution McElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\). tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% mutate(prob = 1 / (b - a)) ## # A tibble: 5 x 3 ## a b prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 0 1.5 0.667 ## 3 0 2 0.5 ## 4 0 3 0.333 ## 5 0 9 0.111 I like to verify things with plots. tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% expand(nesting(a, b), parameter_space = seq(from = 0, to = 9, length.out = 500)) %&gt;% mutate(prob = dunif(parameter_space, a, b), b = str_c(&quot;b = &quot;, b)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + scale_x_continuous(breaks = c(0, 1:3, 9)) + scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1), labels = c(&quot;0&quot;, &quot;1/9&quot;, &quot;1/3&quot;, &quot;1/2&quot;, &quot;2/3&quot;, &quot;1&quot;)) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + facet_wrap(~b, ncol = 5) And as we’ll learn much later in the project, the \\(\\text{Uniform} (0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g., tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(prob = dbeta(parameter_space, 1, 1)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + coord_cartesian(ylim = 0:2) + theme(panel.grid = element_blank()) 2.3.4 Posterior. If we continue to focus on the globe tossing example, the posterior probability a toss will be water may be expressed as: \\[\\text{Pr} (p|w) = \\frac{\\text{Pr} (w|p) \\text{Pr} (p)}{\\text{Pr} (w)}\\] More generically and in words, this is: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\] 2.4 Making the model go Here’s the data wrangling for Figure 2.6. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2), exp(-abs(probability - .5) / .25) / ( 2 * .25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% gather(key, value, -probability, -row) %&gt;% ungroup() %&gt;% mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them with gridExtra::grid.arrange(). p1 &lt;- d %&gt;% filter(key == &quot;prior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;prior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p2 &lt;- d %&gt;% filter(key == &quot;likelihood&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;likelihood&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p3 &lt;- d %&gt;% filter(key == &quot;posterior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) I’m not sure if it’s the same McElreath used in the text, but the formula I used for the tirangle-shaped prior is the Laplace distribution with a location of .5 and a dispersion of .25. Also, to learn all about dplyr::filter(), check out Baert’s Data Wrangling Part 3: Basic and more advanced ways to filter rows. 2.4.1 Grid approximation. We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. (d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the right panel of Figure 2.7. d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here it is with just 5 points, the left hand panel of Figure 2.7. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) 2.4.2 Quadratic approximation. Apply the quadratic approximation to the globe tossing data with rethinking::map(). library(rethinking) globe_qa &lt;- rethinking::map( alist( w ~ dbinom(9, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(w = 6)) # display summary of quadratic approximation precis(globe_qa) ## mean sd 5.5% 94.5% ## p 0.6666668 0.1571338 0.4155367 0.9177969 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- rethinking::map( alist( w ~ dbinom(9 * 2, p), p ~ dunif(0, 1) ), data = list(w = 6 *2)) globe_qa_36 &lt;- rethinking::map( alist( w ~ dbinom(9 * 4, p), p ~ dunif(0, 1) ), data = list(w = 6 * 4)) precis(globe_qa_18) ## mean sd 5.5% 94.5% ## p 0.6666662 0.1111104 0.4890902 0.8442421 precis(globe_qa_36) ## mean sd 5.5% 94.5% ## p 0.6666671 0.07856683 0.5411022 0.7922321 Here’s the legwork for Figure 2.8. n_grid &lt;- 100 tibble(p_grid = seq(from = 0, to = 1, length.out = n_grid) %&gt;% rep(., times = 3), prior = 1, w = rep(c(6, 12, 24), each = n_grid), n = rep(c(9, 18, 36), each = n_grid), m = .67, s = rep(c(.16, .11, .08), each = n_grid)) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.3 Markov chain Monte Carlo. Since the main goal of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) Here we’ll re-fit the last model from above wherein \\(w = 24\\) and \\(n = 36\\). globe_qa_brms &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior(beta(1, 1), class = Intercept), iter = 4000, warmup = 1000, control = list(adapt_delta = .9), seed = 4) The model output looks like so. print(globe_qa_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.50 0.80 3579 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\). Let’s plot the results of our model and compare them with those from rethinking::map(), above. posterior_samples(globe_qa_brms) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + labs(x = &quot;proportion water&quot;) + xlim(0, 1) + theme(panel.grid = element_blank()) + facet_wrap(~n) If you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models in brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 10. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 rethinking_2.01 dagitty_0.2-2 rstan_2.18.2 ## [6] StanHeaders_2.18.1 gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [11] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 ## [16] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 markdown_1.0 ## [5] base64enc_0.1-3 rstudioapi_0.10 farver_2.0.3 DT_0.7 ## [9] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 xml2_1.2.0 ## [13] codetools_0.2-16 bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [17] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 broom_0.5.2 ## [21] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 backports_1.1.4 ## [25] assertthat_0.2.1 Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 ## [29] later_0.8.0 htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [33] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 glue_1.3.1 ## [37] reshape2_1.4.3 V8_2.2 cellranger_1.1.0 vctrs_0.1.0 ## [41] nlme_3.1-144 crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [45] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 lifecycle_0.1.0 ## [49] gtools_3.8.1 MASS_7.3-51.5 zoo_1.8-6 scales_1.1.1.9000 ## [53] colourpicker_1.0 hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [57] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 curl_3.3 ## [61] loo_2.1.0 stringi_1.4.3 highr_0.8 dygraphs_1.1.1.6 ## [65] boot_1.3-24 pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [69] pkgconfig_2.0.2 matrixStats_0.54.0 evaluate_0.14 lattice_0.20-38 ## [73] rstantools_1.5.1 htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [77] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 bookdown_0.11 ## [81] R6_2.4.0 generics_0.0.2 pillar_1.4.1 haven_2.1.0 ## [85] withr_2.1.2 xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [89] crayon_1.3.4 utf8_1.1.4 rmarkdown_1.13 grid_3.6.3 ## [93] readxl_1.3.1 callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [97] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 ## [101] shinyjs_1.0 "],
["sampling-the-imaginary.html", "3 Sampling the Imaginary 3.1 Sampling from a grid-like approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice in brms Reference Session info", " 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time useing the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 3.1 Sampling from a grid-like approximate posterior Here we use grid approximation, again, to generate samples. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows Now we’ll use the dplyr::sample_n() function to sample rows from d, saving them as sample. # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) glimpse(samples) ## Observations: 10,000 ## Variables: 4 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… We’ll plot the zigzagging left panel of Figure 3.1 with geom_line(). But before we do, we’ll need to add a variable numbering the samples. samples %&gt;% mutate(sample_number = 1:n()) %&gt;% ggplot(aes(x = sample_number, y = p_grid)) + geom_line(size = 1/10) + labs(x = &quot;sample number&quot;, y = &quot;proportion of water (p)&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + coord_cartesian(xlim = 0:1) + xlab(&quot;proportion of water (p)&quot;) 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly now it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you’d first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.171 To learn more about dplyr::summarise() and related functions, check out Baert’s Data Wrangling Part 4: Summarizing and slicing your data and Chapter 5.6 of R4DS. If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.162 You can use &amp; within filter(), too. samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.602 3.2.2 Intervals of defined mass. We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), and a some careful filtering. # upper left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + # note this next line is the only difference in code from the last plot geom_ribbon(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ll come back for the lower two panels in a bit. Since we’ve saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$p_grid, prob = .8)) ## 80% ## 0.763 That value will come in handy for the lower left panel of Figure 3.2, so we saved it. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(): samples %&gt;% select(p_grid) %&gt;% pull() %&gt;% quantile(prob = .8) ## 80% ## 0.763 And we might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(p_grid, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.763 Here’s the summarise() approach with two probabilities: samples %&gt;% summarise(`10th percentile` = quantile(p_grid, p = .1), `90th percentile` = quantile(p_grid, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.452 0.814 The tydiverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(.1, .9))) ## 10% 90% ## 0.4520 0.8141 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_trials &lt;- 3 # update `d` d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) # make the next part reproducible set.seed(3) # here&#39;s our new samples tibble ( samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) ) ## # A tibble: 10,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.716 1 0.367 0.367 ## 2 0.651 1 0.276 0.276 ## 3 0.547 1 0.164 0.164 ## 4 0.999 1 0.997 0.997 ## 5 0.99 1 0.970 0.970 ## 6 0.787 1 0.487 0.487 ## 7 0.94 1 0.831 0.831 ## 8 0.817 1 0.545 0.545 ## 9 0.955 1 0.871 0.871 ## 10 0.449 1 0.0905 0.0905 ## # … with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$p_grid, prob = c(.25, .75)) ## 25% 75% ## 0.709 0.935 rethinking::PI(samples$p_grid, prob = .5) ## 25% 75% ## 0.709 0.935 Now’s a good time to introduce Matthew Kay’s tidybayes package, which offers an array of convenience functions for Bayesian models of the type we’ll be working with in this project. library(tidybayes) median_qi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.843 0.709 0.935 0.5 median qi The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.843 0.709000 0.935 0.50 median qi ## 2 0.843 0.570000 0.975 0.80 median qi ## 3 0.843 0.260985 0.999 0.99 median qi The .width column in the output indexed which line presented which interval. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$p_grid, prob = .5) ## |0.5 0.5| ## 0.842 0.999 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9562951 0.842 0.999 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions. qi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.709 0.935 hdi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.842 0.999 These are nice in that they yield simple numeric vectors, making them particularly useful to use as references within ggplot2. Now we have that skill, we can use it to make Figure 3.3. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + # check out our sweet `qi()` indexing geom_ribbon(data = d %&gt;% filter(p_grid &gt; qi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; qi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; hdi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; hdi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 ## 2 0.999 1 0.997 0.997 ## 3 0.998 1 0.994 0.994 ## 4 0.997 1 0.991 0.991 ## 5 0.996 1 0.988 0.988 ## 6 0.995 1 0.985 0.985 ## 7 0.994 1 0.982 0.982 ## 8 0.993 1 0.979 0.979 ## 9 0.992 1 0.976 0.976 ## 10 0.991 1 0.973 0.973 ## # … with 991 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 Or we could use the handy dplyr::top_n() function. d %&gt;% select(posterior) %&gt;% top_n(n = 1) ## Selecting by posterior ## # A tibble: 1 x 1 ## posterior ## &lt;dbl&gt; ## 1 1 We can get th emode with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.477 1 0.95 mode hdi samples %&gt;% mode_qi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.401 0.994 0.95 mode qi But if all you want is the mode itself, you can just use tidybayes::Mode(). Mode(samples$p_grid) ## [1] 0.9562951 But medians and means are typical, too. samples %&gt;% summarise(mean = mean(p_grid), median = median(p_grid)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- bind_rows( samples %&gt;% mean_qi(p_grid), samples %&gt;% median_qi(p_grid), samples %&gt;% mode_qi(p_grid) ) %&gt;% select(p_grid, .point) %&gt;% # these last two columns will help us annotate mutate(x = p_grid + c(-.03, .03, -.03), y = c(.1, .25, .4)) ) ## # A tibble: 3 x 4 ## p_grid .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 mean 0.773 0.1 ## 2 0.843 median 0.873 0.25 ## 3 0.956 mode 0.926 0.4 The plot: d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be: d %&gt;% mutate(loss = posterior * abs(0.5 - p_grid)) %&gt;% summarise(`expected loss` = sum(loss)) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 78.4 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package, which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() ) ## # A tibble: 1,001 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 201. ## 2 0.001 200. ## 3 0.002 200. ## 4 0.003 200. ## 5 0.004 199. ## 6 0.005 199. ## 7 0.006 199. ## 8 0.007 199. ## 9 0.008 198. ## 10 0.009 198. ## # … with 991 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.841. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(p_grid)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # ammend our loss function make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.803 3.3 Sampling to simulate prediction McElreath’s four good reasons for posterior simulation were: Model checking Software validation Research design Forecasting 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the denisties are: tibble(n = 2, probability = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = probability)) ## # A tibble: 3 x 4 ## n probability w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(3) rbinom(1, size = 2, prob = .7) ## [1] 2 Here are ten reproducible draws. set.seed(3) rbinom(10, size = 2, prob = .7) ## [1] 2 1 2 2 1 1 2 2 1 1 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% group_by(draws) %&gt;% count() %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## # Groups: draws [3] ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9000 0.09 ## 2 1 42051 0.421 ## 3 2 48949 0.489 As McElreath mused in the text, those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up. Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability){ set.seed(3) rbinom(n_draws, size = n, prob = probability) } d &lt;- tibble(n = c(3, 6, 9)) %&gt;% expand(n, probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest() head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 0 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 0 ## 5 n = 3 p = 0.3 1 ## 6 n = 3 p = 0.3 1 The results look as follows: d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). If you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari, Gelman, Simpson, Carpenter, and Bürkner Rank-normalization, folding, and localization: An improved Rˆ for assessing convergence of MCMC. 3.3.2.2 Is the model adequate? The implied predictions of the model are uncertain in two ways, and it’s important to be aware of both. First, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)). Second, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes. We’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (p. 56, emphasis in the original) All this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows We can make our version of the top of Figure 3.6 with a little tricky filtering. d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), color = &quot;grey67&quot;, fill = &quot;grey67&quot;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(xend = p_grid, y = 0, yend = posterior, size = posterior), color = &quot;grey33&quot;, show.legend = F) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(y = posterior)) + annotate(geom = &quot;text&quot;, x = .08, y = .0025, label = &quot;Posterior probability&quot;) + scale_size_continuous(range = c(0, 1)) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Note how we weighted the widths of the vertical lines by the posterior density. We’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6. n_draws &lt;- 1e5 simulate_binom &lt;- function(probability){ set.seed(3) rbinom(n_draws, size = 9, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) head(d_small) ## # A tibble: 6 x 3 ## probability draws label ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.1 0 p = 0.1 ## 2 0.1 2 p = 0.1 ## 3 0.1 0 p = 0.1 ## 4 0.1 0 p = 0.1 ## 5 0.1 1 p = 0.1 ## 6 0.1 1 p = 0.1 Now we’re ready to plot. d_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Sampling distributions&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) To make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text). # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) glimpse(samples) ## Observations: 10,000 ## Variables: 5 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… ## $ w &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, … Here’s our histogram. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9, ylim = 0:3000) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longst sequence of the sampe values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 7 5 6 8 7 5 6 3 3 4 Notice this collapses (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 0 1 1 1 0 0 0 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot. # make it reproducible set.seed(3) samples &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %&gt;% unnest(draws) glimpse(samples) ## Observations: 90,000 ## Variables: 7 ## $ p_grid &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0… ## $ posterior &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, … ## $ w &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ draws &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, … The main action is in the draws column. Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”. rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 6), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 3.4 Summary Let’s practice in brms Open brms. library(brms) In brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of the project for Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 1, # this is a flat prior prior(beta(1, 1), class = Intercept), seed = 3, control = list(adapt_delta = .999)) We’ll learn more about the beta distribution in Chapter 11. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”. posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.64 0.14 0.36 0.88 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with fitted() within the brms framework. But we will have to specify scale = &quot;linear&quot; in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. f &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(&quot;p&quot;) glimpse(f) ## Observations: 4,000 ## Variables: 1 ## $ p &lt;dbl&gt; 0.6920484, 0.5559454, 0.6096088, 0.5305334, 0.4819733, 0.6724561, 0.6402367, 0.8356569,… By default, we have a generically-named vector V1 of 4000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &quot;grey50&quot;, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .08, y = 2.5, label = &quot;Posterior probability&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6. # the simulation set.seed(3) f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) # the plot f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3), limits = c(0, 9)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&quot;Posterior predictive distribution&quot;) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0s and 1s, too. And we could even use those disaggregated 0s and 1s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 tidybayes_1.1.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [7] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rethinking_2.01 rstudioapi_0.10 farver_2.0.3 ## [10] rstan_2.18.2 svUnit_0.7-12 DT_0.7 ## [13] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 ## [16] xml2_1.2.0 codetools_0.2-16 bridgesampling_0.6-0 ## [19] knitr_1.23 shinythemes_1.1.2 zeallot_0.1.0 ## [22] bayesplot_1.7.0 jsonlite_1.6 broom_0.5.2 ## [25] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 ## [28] backports_1.1.4 assertthat_0.2.1 Matrix_1.2-17 ## [31] lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [34] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [37] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.1.0 nlme_3.1-144 crosstalk_1.0.0 ## [46] xfun_0.7 ps_1.3.0 rvest_0.3.4 ## [49] miniUI_0.1.1.1 mime_0.7 lifecycle_0.1.0 ## [52] gtools_3.8.1 MASS_7.3-51.5 zoo_1.8-6 ## [55] scales_1.1.1.9000 colourpicker_1.0 hms_0.4.2 ## [58] promises_1.0.1 Brobdingnag_1.2-6 parallel_3.6.3 ## [61] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [64] gridExtra_2.3 loo_2.1.0 StanHeaders_2.18.1 ## [67] stringi_1.4.3 dygraphs_1.1.1.6 pkgbuild_1.0.3 ## [70] shape_1.4.4 rlang_0.4.0 pkgconfig_2.0.2 ## [73] matrixStats_0.54.0 HDInterval_0.2.0 evaluate_0.14 ## [76] lattice_0.20-38 rstantools_1.5.1 htmlwidgets_1.3 ## [79] labeling_0.3 processx_3.3.1 tidyselect_0.2.5 ## [82] plyr_1.8.4 magrittr_1.5 bookdown_0.11 ## [85] R6_2.4.0 generics_0.0.2 pillar_1.4.1 ## [88] haven_2.1.0 withr_2.1.2 xts_0.11-2 ## [91] abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [94] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_1.13 ## [97] grid_3.6.3 readxl_1.3.1 callr_3.2.0 ## [100] threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [103] httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 ## [106] viridisLite_0.3.0 shinyjs_1.0 "],
["linear-models.html", "4 Linear Models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 A Gaussian model of height 4.4 Adding a predictor 4.5 Polynomial regression Reference Session info", " 4 Linear Models Linear regression is the geocentric model of applied statistics. By “linear regression”, we will mean a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Like geocentrism, linear regression can usefully describe a very large variety of natural phenomena. Like geocentrism, linear is a descriptive model that corresponds to many different process models. If we read its structure too literally, we’re likely to make mistakes. But used wisely, these little linear golems continue to be useful. (p. 71) 4.1 Why normal distributions are normal After laying out his soccer field coin toss shuffle premise, McElreath wrote: It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72) 4.1.1 Normal by addition. Here’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2. library(tidyverse) # we set the seed to make the results of `runif()` reproducible. set.seed(4) pos &lt;- replicate(100, runif(16, -1, 1)) %&gt;% # here&#39;s the simulation as_tibble() %&gt;% # for data manipulation, we&#39;ll make this a tibble rbind(0, .) %&gt;% # here we add a row of zeros above the simulation results mutate(step = 0:16) %&gt;% # this adds a step index gather(key, value, -step) %&gt;% # here we convert the data to the long format mutate(person = rep(1:100, each = 17)) %&gt;% # this adds a person id index # the next two lines allow us to make cumulative sums within each person group_by(person) %&gt;% mutate(position = cumsum(value)) %&gt;% ungroup() # ungrouping allows for further data manipulation We might glimpse() at the data. glimpse(pos) ## Observations: 1,700 ## Variables: 5 ## $ step &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7… ## $ key &lt;chr&gt; &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V… ## $ value &lt;dbl&gt; 0.00000000, 0.17160061, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47… ## $ person &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ position &lt;dbl&gt; 0.00000000, 0.17160061, -0.81050780, -1.22302857, -1.66827866, -1.04113023, -1.5… And here’s the actual plot code. ggplot(data = pos, aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&quot;step number&quot;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &quot;none&quot;) Here’s the code for the bottom three plots of Figure 4.2. # Figure 4.2.a. pos %&gt;% filter(step == 4) %&gt;% ggplot(aes(x = position)) + geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;4 steps&quot;) # Figure 4.2.b. pos %&gt;% filter(step == 8) %&gt;% ggplot(aes(x = position)) + geom_density(color = &quot;dodgerblue2&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;8 steps&quot;) # this is an intermediary step to get an SD value pos %&gt;% filter(step == 16) %&gt;% summarise(sd = sd(position)) ## # A tibble: 1 x 1 ## sd ## &lt;dbl&gt; ## 1 2.18 # Figure 4.2.c. pos %&gt;% filter(step == 16) %&gt;% ggplot(aes(x = position)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2.180408), linetype = 2) + # 2.180408 came from the previous code block geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + coord_cartesian(xlim = -6:6) + labs(title = &quot;16 steps&quot;, y = &quot;density&quot;) While we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions. 4.1.2 Normal by multiplication. Here’s McElreath’s simple random growth rate. set.seed(4) prod(1 + runif(12, 0, 0.1)) ## [1] 1.774719 In the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values and then computed their product. Consider a more explicit variant of the code. set.seed(4) tibble(a = 1, b = runif(12, 0, 0.1)) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c)) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 1.77 Same result. Rather than using base R replicate() to do this many times, let’s practice with purrr::map_dbl() instead (see here for details). set.seed(4) growth &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.1)))) ggplot(data = growth, aes(x = growth)) + geom_density() “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. # simulate set.seed(4) samples &lt;- tibble(big = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.5))), small = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.01)))) %&gt;% # wrangle gather(distribution, samples) # plot samples %&gt;% ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;, color = &quot;transparent&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) Yep, the small samples were more Gaussian. 4.1.3 Normal by log-multiplication. Instead of saving our tibble, we’ll just feed it directly into our plot. set.seed(4) tibble(samples = map_dbl(1:1e4, ~ log(prod(1 + runif(12, 0, 0.5))))) %&gt;% ggplot(aes(x = samples)) + geom_density(color = &quot;transparent&quot;, fill = &quot;gray33&quot;) What we did was really compact. Walking it out a bit, here’s what we all did within the second argument within map_dbl() (i.e., everything within log()). tibble(a = runif(12, 0, 0.5), b = 1) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c) %&gt;% log()) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 2.82 And based on the first argument within map_dbl(), we did that 10,000 times, after which we converted the results to a tibble and then fed those data into ggplot2. 4.1.4 Using Gaussian distributions. I really like the justifications in the following subsections. 4.1.4.1 Ontological justification. The Gaussian is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process… (p. 75) But they can still be useful. 4.1.4.2 Epistemological justification. Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75–76) In the Overthinking: Gaussian distribution box that follows, McElreath gave the formula. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is \\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp} \\Bigg (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\Bigg)\\] 4.2 A language for describing models Our mathy ways of summarizing models will be something like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta \\times \\text{predictor}_i \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy}(0, 1) \\end{align*}\\] And as McElreath then followed up with, “If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome applied statistics! 4.2.1 Re-describing the globe tossing model. For the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was \\[\\begin{align*} w &amp; \\sim \\text{Binomial}(n, p) \\\\ p &amp; \\sim \\text{Uniform}(0, 1) \\end{align*}\\] We can break McElreath’s R code 4.6 down a little bit with a tibble like so. # how many `p_grid` points would you like? n_points &lt;- 100 d &lt;- tibble(w = 6, n = 9, p_grid = seq(from = 0, to = 1, length.out = n_points)) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(w, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 6 ## w n p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 9 0 1 0. 0. ## 2 6 9 0.0101 1 8.65e-11 8.74e-12 ## 3 6 9 0.0202 1 5.37e- 9 5.43e-10 ## 4 6 9 0.0303 1 5.93e- 8 5.99e- 9 ## 5 6 9 0.0404 1 3.23e- 7 3.26e- 8 ## 6 6 9 0.0505 1 1.19e- 6 1.21e- 7 In case you were curious, here’s what they look like: d %&gt;% select(-w, -n) %&gt;% gather(key, value, -p_grid) %&gt;% # this line allows us to dictate the order the panels will appear in mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) + geom_ribbon() + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, scales = &quot;free&quot;) The posterior is a combination of the prior and the likelihood. And when the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see that we almost never use flat priors. 4.3 A Gaussian model of height There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. (p. 79) 4.3.1 The data. Let’s get the data from McElreath’s rethinking package. library(rethinking) data(Howell1) d &lt;- Howell1 Here we open our main statistical package, Bürkner’s brms. But before we do, we’ll need to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, we will always make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Go ahead and investigate the data with str(), the tidyverse analogue for which is glimpse(). d %&gt;% str() ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... Here are the height values. d %&gt;% select(height) %&gt;% head() ## height ## 1 151.765 ## 2 139.700 ## 3 136.525 ## 4 156.845 ## 5 145.415 ## 6 163.830 We can use filter() to make an adults-only data frame. d2 &lt;- d %&gt;% filter(age &gt;= 18) 4.3.1.1 Overthinking: Data frames. This probably reflects my training history, but the structure of a data frame seems natural and inherently appealing, to me. So I can’t relate to the “annoying” comment. But if you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan. 4.3.1.2 Overthinking: Index magic. For more on indexing, check out chapter 9 of Roger Peng’s R Programming for Data Science or even the Subsetting subsection from R4DS. 4.3.2 The model. The likelihood for our model is \\[h_i \\sim \\text{Normal}(\\mu, \\sigma)\\] Our \\(\\mu\\) prior will be \\[\\mu \\sim \\text{Normal}(178, 20)\\] And our prior for \\(\\sigma\\) will be \\[\\sigma \\sim \\text{Uniform}(0, 50)\\] Here’s the shape of the prior for \\(\\mu\\) in \\(N(178, 20)\\). ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + ylab(&quot;density&quot;) And here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the y axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous(). tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) We can simulate from both priors at once to get a prior probability distribution of heights. n &lt;- 1e4 set.seed(4) tibble(sample_mu = rnorm(n, mean = 178, sd = 20), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% ggplot(aes(x = x)) + geom_density(fill = &quot;black&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Prior predictive distribution for &quot;, italic(h[i]))), x = NULL) + theme(panel.grid = element_blank()) As McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p. 83). 4.3.3 Grid approximation of the posterior distribution. As McElreath explained, you’ll never use this for practical data analysis. But I found this helped me better understanding what exactly we’re doing with Bayesian estimation. So let’s play along. n &lt;- 200 d_grid &lt;- tibble(mu = seq(from = 140, to = 160, length.out = n), sigma = seq(from = 4, to = 9, length.out = n)) %&gt;% # we&#39;ll accomplish with `tidyr::expand()` what McElreath did with base R `expand.grid()` expand(mu, sigma) head(d_grid) ## # A tibble: 6 x 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 ## 2 140 4.03 ## 3 140 4.05 ## 4 140 4.08 ## 5 140 4.10 ## 6 140 4.13 d_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computateions by making a custom function which we’ll plug into purrr::map2(). grid_function &lt;- function(mu, sigma){ dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’re ready to complete the tibble. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest() %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) head(d_grid) ## # A tibble: 6 x 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 -3813. -5.72 -3.91 -3822. 0 ## 2 140 4.03 -3778. -5.72 -3.91 -3787. 0 ## 3 140 4.05 -3743. -5.72 -3.91 -3753. 0 ## 4 140 4.08 -3709. -5.72 -3.91 -3719. 0 ## 5 140 4.10 -3676. -5.72 -3.91 -3686. 0 ## 6 140 4.13 -3644. -5.72 -3.91 -3653. 0 In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + coord_cartesian(xlim = range(d_grid$mu), ylim = range(d_grid$sigma)) + theme(panel.grid = element_blank()) We’ll make our heat map with geom_raster(aes(fill = probability)). d_grid %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_raster(aes(fill = probability), interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior. We can use dplyr::sample_n() to sample rows, with replacement, from d_grid. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) We can use gather() and then facet_warp() to plot the densities for both mu and sigma at once. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) We’ll use the tidybayes package to compute their posterior modes and 95% HDIs. library(tidybayes) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.95 mode hdi ## 2 sigma 7.82 7.14 8.30 0.95 mode hdi Let’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5). 4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior. Since we’ll be fitting models with brms almost exclusively from here on out, this section is largely mute. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3. set.seed(4) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 144.7800 154.9400 ## [11] 151.1300 147.9550 149.8600 162.5600 161.9250 164.4650 160.9852 151.7650 163.8300 149.8600 For our first step using d3, we’ll redefine d_grid. n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- tibble(mu = seq(from = 150, to = 170, length.out = n), sigma = seq(from = 4, to = 20, length.out = n)) %&gt;% expand(mu, sigma) Second, we’ll redefine our custom grid_function() function to operate over the height values of d3. grid_function &lt;- function(mu, sigma){ dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’ll use the amended grid_function() to make the posterior. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) Did you catch our use of purrr::map2_dbl(), there, in place of purrr::map2()? It turns out that purrr::map() and purrr::map2() always return a list (see here and here). But as Phil Straforelli kindly pointed out, we can add the _dbl suffix to those functions, which will instruct the purrr package to return a double vector (i.e., a common kind of numeric vector). The advantage of that approach is we no longer need to follow our map() or map2() lines with unnest(). To learn more about the ins and outs of the map() family, check out this section from R4DS or Jenny Bryan’s purrr tutorial. Next we’ll sample_n() and plot. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) Behold the updated densities. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales= &quot;free&quot;) Sigma’s not so Gaussian with that small \\(n\\). 4.3.5 Fitting the model with map() brm(). We won’t actually use rethinking::map()–which you should not conflate with purrr::map()–, but will jumpt straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Here’s the first model. b4.1 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4, seed = 4) McElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in Chapter 8, HMC tends to work better when you default to a half Cauchy for \\(\\sigma\\). Here’s how to do so. b4.1_half_cauchy &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) This leads to an important point. After running model with Hamiltonian Monte Carlo (HMC), it’s a good idea to inspect the chains. As we’ll see, McElreath coverd this in Chapter 8. Here’s a typical way to do so in brms. plot(b4.1_half_cauchy) If you want detailed diagnostics for the HMC chains, call launch_shinystan(b4.1). That’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object. print(b4.1_half_cauchy) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 154.60 0.41 153.77 155.40 2736 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 7.75 0.29 7.20 8.33 3535 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summary() function works in a similar way. You can also get a Stan-like summary with this: b4.1_half_cauchy$fit ## Inference for Stan model: 111e8cc5645009df70b928231cdbcf0c. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 154.60 0.01 0.41 153.77 154.34 154.60 154.88 155.40 2736 1 ## sigma 7.75 0.00 0.29 7.20 7.55 7.75 7.94 8.33 3535 1 ## lp__ -1227.51 0.03 1.01 -1230.32 -1227.88 -1227.19 -1226.80 -1226.55 1433 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Jul 12 14:46:49 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Whereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print(). summary(b4.1_half_cauchy, prob = .89) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## Intercept 154.60 0.41 153.94 155.25 2736 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## sigma 7.75 0.29 7.30 8.22 3535 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Anyways, here’s the shockingly-narrow-\\(\\mu\\)-prior model. b4.2 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, .1), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 3000, warmup = 2000, chains = 4, cores = 4, seed = 4) Check the chains. plot(b4.2) I had to increase the warmup due to convergence issues. After doing so, everything looks to be on the up and up. The chains look great. And again, to learn more about these technical details, check out Chapter 8. Here’s the model summary(). summary(b4.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 3000; warmup = 2000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 177.87 0.10 177.67 178.06 3627 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 24.63 0.98 22.85 26.68 1403 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3.6 Sampling from a map() brm() fit. brms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. For example: vcov(b4.1_half_cauchy) ## Intercept ## Intercept 0.1707905 This only returns the first element in the matrix it did for rethinking. That is, it appears brms::vcov() only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters (i.e., those used to model \\(\\mu\\)). However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. post &lt;- posterior_samples(b4.1_half_cauchy) head(post) ## b_Intercept sigma lp__ ## 1 153.8263 8.111452 -1228.985 ## 2 155.2687 7.331025 -1228.936 ## 3 153.7654 8.195435 -1229.588 ## 4 153.5382 8.131402 -1230.481 ## 5 153.7429 8.219945 -1229.798 ## 6 154.6138 7.182985 -1228.449 Now select() the columns containing the draws from the desired parameters and feed them into cof(). select(post, b_Intercept:sigma) %&gt;% cov() ## b_Intercept sigma ## b_Intercept 0.170790518 -0.002620755 ## sigma -0.002620755 0.083364582 That was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix. # variances select(post, b_Intercept:sigma) %&gt;% cov() %&gt;% diag() ## b_Intercept sigma ## 0.17079052 0.08336458 # correlation post %&gt;% select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.00000000 -0.02196361 ## sigma -0.02196361 1.00000000 With our post &lt;- posterior_samples(b4.1_half_cauchy) code from a few lines above, we’ve already done the brms version of what McElreath did with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package, in brms we just extracted the iterations of the HMC chains and put them in a data frame. str(post) ## &#39;data.frame&#39;: 4000 obs. of 3 variables: ## $ b_Intercept: num 154 155 154 154 154 ... ## $ sigma : num 8.11 7.33 8.2 8.13 8.22 ... ## $ lp__ : num -1229 -1229 -1230 -1230 -1230 ... Notice how our data frame, post, includes a third vector, lp__. That’s the log posterior. See the brms reference manual or the “The Log-Posterior (function and gradient)” section of the Stan Development Team’s RStan: the R interface to Stan for details. The log posterior will largely be outside of our focus in this project. The summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. E.g., summary(post[, 1:2]) ## b_Intercept sigma ## Min. :153.0 Min. :6.806 ## 1st Qu.:154.3 1st Qu.:7.551 ## Median :154.6 Median :7.749 ## Mean :154.6 Mean :7.751 ## 3rd Qu.:154.9 3rd Qu.:7.942 ## Max. :156.0 Max. :8.905 Here’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here. t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75))) ## 50% 2.5% 75% ## b_Intercept 154.601068 153.770984 154.880786 ## sigma 7.749358 7.202063 7.941544 The base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% summarise(mean = mean(value), SD = sd(value), `2.5_percentile` = quantile(value, probs = .025), `97.5_percentile` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## # A tibble: 2 x 5 ## parameter mean SD `2.5_percentile` `97.5_percentile` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.41 154. 155. ## 2 sigma 7.75 0.290 7.2 8.33 You can always get pretty similar information by just putting the brm() fit object into posterior_summary(). posterior_summary(b4.1_half_cauchy) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.603591 0.4132681 153.770984 155.398331 ## sigma 7.751165 0.2887293 7.202063 8.334751 ## lp__ -1227.507235 1.0129942 -1230.317341 -1226.546097 And if you’re willing to drop the posterior \\(SD\\)s, you can use tidybayes::mean_qi(), too. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% mean_qi(value) ## # A tibble: 2 x 7 ## parameter value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 155. 154. 155. 0.95 mean qi ## 2 sigma 7.75 7.20 8.33 0.95 mean qi 4.3.6.1 Overthinking: Under the hood with multivariate sampling. Again, brms::posterior_samples() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the iterations from the HMC chains. McElreath coverd all of this in Chapter 8. You might also look at the brms reference manual or GitHub page for details. 4.3.6.2 Overthinking: Getting \\(\\sigma\\) right. There’s no need to fret about this in brms. With HMC, we are not constraining the posteriors to the multivariate normal distribution. Here’s our posterior density for \\(\\sigma\\). ggplot(data = post, aes(x = sigma)) + geom_density(size = 1/10, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma)) + theme(panel.grid = element_blank()) See? HMC handled the mild skew just fine. But sometimes you want to actually model \\(\\sigma\\), such as in the case where your variances are systematically heterogeneous. Bürkner calls these kinds of models distributional models, which you can learn more about in his vignette Estimating Distributional Models with brms. As he explained in the vignette, you actually model \\(\\text{log}(\\sigma)\\) in those instances. If you’re curious, we’ll practice with a model like this in Chapter 9. 4.4 Adding a predictor Here’s our scatter plot of weight and height. ggplot(data = d2, aes(x = weight, y = height)) + geom_point(shape = 1, size = 2) + theme_bw() + theme(panel.grid = element_blank()) 4.4.1 The linear model strategy In our new univariable model \\[\\begin{align*} h_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta x_i \\\\ \\alpha &amp; \\sim \\text{Normal}(178, 100) \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 50) \\end{align*}\\] 4.4.2 Fitting the model. The brms::brm() syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 95. \\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\): family = gaussian \\(\\mu_i = \\alpha + \\beta x_i\\): height ~ 1 + weight \\(\\alpha \\sim \\text{Normal}(156, 100)\\): prior(normal(156, 100), class = Intercept \\(\\beta \\sim \\text{Normal}(0, 10)\\): prior(normal(0, 10), class = b) \\(\\sigma \\sim \\text{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma) Thus, to add a predictor you just the + operator in the model formula. b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight, prior = c(prior(normal(156, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 41000, warmup = 40000, chains = 4, cores = 4, seed = 4) This was another example of how using a uniform prior for \\(\\sigma\\) required we use an unusually large number of warmup iterations before the HMC chains converged on the posterior. Change the prior to cauchy(0, 1) and the chains converge with no problem, resulting in much better effective samples, too. Here are the trace plots. plot(b4.3) 4.4.3 Interpreting the model fit. “One trouble with statistical models is that they are hard to understand” (p. 97). Welcome to the world of applied statistics. 4.4.3.1 Tables of estimates. With a little [] subsetting we can exclude the log posterior from the summary. posterior_summary(b4.3)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 113.9447973 1.91126176 110.4305908 117.6829753 ## b_weight 0.9035651 0.04199376 0.8229578 0.9813891 ## sigma 5.1046400 0.18991849 4.7442081 5.4900810 Again, brms doesn’t have a convenient corr = TRUE argument for plot() or summary(). But you can get that information after putting the chains in a data frame. posterior_samples(b4.3) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight sigma ## b_Intercept 1.00 -0.99 0.01 ## b_weight -0.99 1.00 -0.01 ## sigma 0.01 -0.01 1.00 With centering, we can reduce the correlations among the parameters. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) Fit the weight_c model, b4.4. b4.4 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 46000, warmup = 45000, chains = 4, cores = 4, seed = 4) plot(b4.4) posterior_summary(b4.4)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.5985011 0.26500034 154.0692950 155.1061797 ## b_weight_c 0.9036751 0.04170534 0.8230058 0.9844814 ## sigma 5.0981815 0.19910581 4.7314168 5.5076203 Like before, the uniform prior required extensive warmup iterations to produce a good posterior. This is easily fixed using a half Cauchy prior, instead. Anyways, the effective samples improved. Here’s the parameter correlation info. posterior_samples(b4.4) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight_c sigma ## b_Intercept 1.00 0 0.02 ## b_weight_c 0.00 1 0.00 ## sigma 0.02 0 1.00 See? Now all the correlations are quite low. Also, if you prefer a visual approach, you might do pairs(b4.4). 4.4.3.2 Plotting posterior inference against the data. Here is the code for Figure 4.4. Note our use of the fixef() function. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + theme_bw() + theme(panel.grid = element_blank()) In the brms reference manual, Bürkner described the job of thefixef() function as “extract[ing] the population-level (’fixed’) effects from a brmsfit object”. If you’re new to multilevel models, it might not be clear what he meant by “population-level” or “fixed” effects. Don’t worry. That’ll all become clear starting around Chapter 12. In the meantime, just think of them as the typical regression parameters, minus \\(\\sigma\\). 4.4.3.3 Adding uncertainty around the mean. Be default, we extract all the posterior iterations with posterior_samples(). post &lt;- posterior_samples(b4.3) post %&gt;% slice(1:5) # this serves a similar function as `head()` ## b_Intercept b_weight sigma lp__ ## 1 113.7295 0.9009082 5.342012 -1083.612 ## 2 110.4864 0.9759522 4.886545 -1084.430 ## 3 111.5634 0.9542566 5.105855 -1082.809 ## 4 112.4052 0.9426601 5.113934 -1082.784 ## 5 116.5550 0.8462851 5.151572 -1083.077 Here are the four models leading up to McElreath’s Figure 4.5. To reduce my computation time, I used a half Cauchy(0, 1) prior on \\(\\sigma\\). If you are willing to wait for the warmups, switching that out for McElreath’s uniform prior should work fine as well. n &lt;- 10 b.10 &lt;- brm(data = d2 %&gt;% slice(1:n), # note our tricky use of `n` and `slice()` family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 50 b.50 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 150 b.150 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 352 b.352 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) I’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information. plot(b.10) print(b.10) plot(b.50) print(b.50) plot(b.150) print(b.150) plot(b.352) print(b.352) We’ll need to put the chains of each model into data frames. post10 &lt;- posterior_samples(b.10) post50 &lt;- posterior_samples(b.50) post150 &lt;- posterior_samples(b.150) post352 &lt;- posterior_samples(b.352) Here is the code for the four individual plots. p10 &lt;- ggplot(data = d2[1:10 , ], aes(x = weight, y = height)) + geom_abline(intercept = post10[1:20, 1], slope = post10[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 10&quot;) + theme_bw() + theme(panel.grid = element_blank()) p50 &lt;- ggplot(data = d2[1:50 , ], aes(x = weight, y = height)) + geom_abline(intercept = post50[1:20, 1], slope = post50[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 50&quot;) + theme_bw() + theme(panel.grid = element_blank()) p150 &lt;- ggplot(data = d2[1:150 , ], aes(x = weight, y = height)) + geom_abline(intercept = post150[1:20, 1], slope = post150[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 150&quot;) + theme_bw() + theme(panel.grid = element_blank()) p352 &lt;- ggplot(data = d2[1:352 , ], aes(x = weight, y = height)) + geom_abline(intercept = post352[1:20, 1], slope = post352[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 352&quot;) + theme_bw() + theme(panel.grid = element_blank()) Note how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %&gt;% slice(1:10) instead. Anyway, we saved each of these plots as objects. With a little help of the multiplot() function we are going to arrange those plot objects into a grid in order to reproduce Figure 4.5. Behold the code for the multiplot() function: multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } We’re finally ready to use multiplot() to make Figure 4.5. multiplot(p10, p150, p50, p352, cols = 2) 4.4.3.4 Plotting regression intervals and contours. Remember, if you want to plot McElreath’s mu_at_50 with ggplot2, you’ll need to save it as a data frame or a tibble. mu_at_50 &lt;- post %&gt;% transmute(mu_at_50 = b_Intercept + b_weight * 50) head(mu_at_50) ## mu_at_50 ## 1 158.7749 ## 2 159.2840 ## 3 159.2762 ## 4 159.5382 ## 5 158.8692 ## 6 159.0430 And here is a version McElreath’s Figure 4.6 density plot. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() We’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean. mean_hdi(mu_at_50[,1], .width = c(.89, .95)) ## y ymin ymax .width .point .interval ## 1 159.1231 158.5775 159.6951 0.89 mean hdi ## 2 159.1231 158.4463 159.8119 0.95 mean hdi If you wanted to express those sweet 95% HPDIs on your density plot, you might use tidybayes::stat_pointintervalh(). Since stat_pointintervalh() also returns a point estimate, we’ll throw in the mode. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() In brms, you would use fitted() to do what McElreath accomplished with link(). mu &lt;- fitted(b4.3, summary = F) str(mu) ## num [1:4000, 1:352] 157 157 157 157 157 ... When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your data. Because we had 4000 post-warmup iterations and \\(n\\) = 352, fitted() returned a matrix of 4000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead. Much like rethinking’s link(), fitted() can accommodate custom predictor values with its newdata argument. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) mu &lt;- fitted(b4.3, summary = F, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # here we name the columns after the `weight` values from which they were computed set_names(25:70) %&gt;% mutate(iter = 1:n()) str(mu) Anticipating ggplot2, we went ahead and converted the output to a tibble. But we might do a little more data processing with the aid of tidyr::gather(). With the gather() function, we’ll convert the data from the wide format to the long format. If you’re new to the distinction between wide and long data, you can learn more here or here. mu &lt;- mu %&gt;% gather(weight, height, -iter) %&gt;% # We might reformat `weight` to numerals mutate(weight = as.numeric(weight)) head(mu) ## # A tibble: 6 x 3 ## iter weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25 136. ## 2 2 25 135. ## 3 3 25 135. ## 4 4 25 136. ## 5 5 25 138. ## 6 6 25 138. Enough data processing. Here we reproduce McElreath’s Figure 4.7.a. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), alpha = .1) # or prettied up a bit d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), color = &quot;navyblue&quot;, alpha = .05) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) With fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = T argument. mu_summary &lt;- fitted(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # let&#39;s tack on the `weight` values from `weight_seq` bind_cols(weight_seq) head(mu_summary) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 137. 0.886 135. 138. 25 ## 2 137. 0.846 136. 139. 26 ## 3 138. 0.806 137. 140. 27 ## 4 139. 0.767 138. 141. 28 ## 5 140. 0.728 139. 142. 29 ## 6 141. 0.689 140. 142. 30 Here it is, our analogue to Figure 4.7.b. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And if you wanted to use intervals other than the default 95% ones, you’d enter a probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(.25, .75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile. 4.4.3.4.1 Overthinking: How link fitted() works. Similar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. 4.4.3.5 Prediction intervals. Even though our full statistical model (omitting priors for the sake of simplicity) is \\[h_i \\sim \\text{Normal}(\\mu_i = \\alpha + \\beta x_, \\sigma)\\] we’ve only been plotting the \\(\\mu\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to predict(). Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim(). We can reuse our weight_seq data from before. But in case you forgot, here’s that code again. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) The predict() code looks a lot like what we used for fitted(). pred_height &lt;- predict(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_height %&gt;% slice(1:6) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 137. 5.18 126. 147. 25 ## 2 137. 5.15 127. 148. 26 ## 3 138. 5.16 128. 148. 27 ## 4 139. 5.20 129. 149. 28 ## 5 140. 5.12 130. 150. 29 ## 6 141. 5.18 131. 151. 30 This time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of both \\(\\mu\\) and \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Our plot for Figure 4.8: d2 %&gt;% ggplot(aes(x = weight)) + geom_ribbon(data = pred_height, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5 Polynomial regression Remember d? d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 14… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0… The quadratic is probably the most commonly used polynomial regression model. \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\] McElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so. d &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight)) / sd(weight)) Here’s the quadratic model in brms. b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) plot(b4.5) print(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + I(weight_s^2) ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 146.67 0.39 145.92 147.44 3206 1.00 ## weight_s 21.40 0.29 20.82 21.97 3203 1.00 ## Iweight_sE2 -8.42 0.29 -8.98 -7.85 3142 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 5.77 0.18 5.45 6.13 3939 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our quadratic plot requires new fitted()- and predict()-oriented wrangling. weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) f_quad &lt;- fitted(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_quad &lt;- predict(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) Behold the code for our version of Figure 4.9.a. You’ll notice how little the code changed from that for Figure 4.8, above. ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_quad, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_quad, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) From a formula perspective, the cubic model is a simple extenstion of the quadratic: \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\] Fit it like so. b4.6 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) And now we’ll fit the good old linear model. b4.7 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) Here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.c, the cubic model. f_cub &lt;- fitted(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_cub &lt;- predict(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_cub, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_cub, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.a, the linear model. f_line &lt;- fitted(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_line &lt;- predict(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_line, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5.0.0.1 Overthinking: Converting back to natural scale. You can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the cubic model. at &lt;- c(-2, -1, 0, 1, 2) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_line, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + # here it is! scale_x_continuous(&quot;standardized weight converted back&quot;, breaks = at, labels = round(at * sd(d$weight) + mean(d$weight), 1)) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 brms_2.9.0 Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 ## [6] StanHeaders_2.18.1 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [11] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 ## [22] broom_0.5.2 shiny_1.3.2 compiler_3.6.3 ## [25] httr_1.4.0 backports_1.1.4 assertthat_0.2.1 ## [28] Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 ## [31] later_0.8.0 htmltools_0.3.6 prettyunits_1.0.2 ## [34] tools_3.6.3 igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] V8_2.2 cellranger_1.1.0 vctrs_0.1.0 ## [43] nlme_3.1-144 crosstalk_1.0.0 xfun_0.7 ## [46] ps_1.3.0 rvest_0.3.4 mime_0.7 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 gtools_3.8.1 ## [52] MASS_7.3-51.5 zoo_1.8-6 scales_1.1.1.9000 ## [55] colourpicker_1.0 hms_0.4.2 promises_1.0.1 ## [58] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.0 curl_3.3 gridExtra_2.3 ## [64] loo_2.1.0 stringi_1.4.3 dygraphs_1.1.1.6 ## [67] boot_1.3-24 pkgbuild_1.0.3 shape_1.4.4 ## [70] rlang_0.4.0 pkgconfig_2.0.2 matrixStats_0.54.0 ## [73] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 ## [76] rstantools_1.5.1 htmlwidgets_1.3 labeling_0.3 ## [79] processx_3.3.1 tidyselect_0.2.5 plyr_1.8.4 ## [82] magrittr_1.5 bookdown_0.11 R6_2.4.0 ## [85] generics_0.0.2 pillar_1.4.1 haven_2.1.0 ## [88] withr_2.1.2 xts_0.11-2 abind_1.4-5 ## [91] modelr_0.1.4 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [94] utf8_1.1.4 rmarkdown_1.13 readxl_1.3.1 ## [97] callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [100] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.3 ## [103] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.0 "],
["multivariate-linear-models.html", "5 Multivariate Linear Models 5.1 Spurious associations 5.2 Masked relationship 5.3 Multicollinearity 5.4 Categorical varaibles 5.5 Ordinary least squares and lm() Reference Session info", " 5 Multivariate Linear Models McElreath’s listed reasons for multivaraiable regression include: statistical control for confounds multiple causation interactions We’ll approach the first two in this chapter. Interactions are reserved for Chapter 6. 5.1 Spurious associations Load the Waffle House data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce Unload rethinking and load brms and, while we’re at it, the tidyverse. rm(WaffleDivorce) detach(package:rethinking, unload = T) library(brms) library(tidyverse) I’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g., head(d) glimpse(d) Now we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package. But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of &quot;firebrick&quot;. # install.packages(&quot;ggrepel&quot;, depencencies = T) library(ggrepel) d %&gt;% ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, size = 1/2, color = &quot;firebrick4&quot;, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + geom_text_repel(data = d %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;OK&quot;, &quot;AR&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;NJ&quot;)), aes(label = Loc), size = 3, seed = 1042) + # this makes it reproducible scale_x_continuous(&quot;Waffle Houses per million&quot;, limits = c(0, 55)) + coord_cartesian(xlim = 0:50, ylim = 5:15) + ylab(&quot;Divorce rate&quot;) + theme_bw() + theme(panel.grid = element_blank()) With coord_map() and help from the fiftystater package (which gives us access to lat/long data for all fifty states via fifty_states), we can plot our three major variables in a map format. library(fiftystater) d %&gt;% # first we&#39;ll standardize the three variables to put them all on the same scale mutate(Divorce_z = (Divorce - mean(Divorce)) / sd(Divorce), MedianAgeMarriage_z = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage), Marriage_z = (Marriage - mean(Marriage)) / sd(Marriage), # need to make the state names lowercase to match with the map data Location = str_to_lower(Location)) %&gt;% # here we select the relevant variables and put them in the long format to facet with `facet_wrap()` select(Divorce_z:Marriage_z, Location) %&gt;% gather(key, value, -Location) %&gt;% ggplot(aes(map_id = Location)) + geom_map(aes(fill = value), map = fifty_states, color = &quot;firebrick&quot;, size = 1/15) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_gradient(low = &quot;#f8eaea&quot;, high = &quot;firebrick4&quot;) + coord_map() + theme_bw() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~key) One of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d %&gt;% distinct(Location) to see for yourself. Those missing data should motivate the skills we’ll cover in Chapter 14. But let’s get back on track. Here we’ll officially standardize the predictor, MedianAgeMarriage. d &lt;- d %&gt;% mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage)) Now we’re ready to fit the first univariable model. b5.1 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Check the summary. print(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.68 0.22 9.25 10.11 5383 1.00 ## MedianAgeMarriage_s -1.04 0.21 -1.45 -0.62 4973 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.51 0.16 1.24 1.86 5029 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll employ fitted() to make Figure 5.2.b. In preparation for fitted() we’ll make a new tibble, nd, composed of a handful of densely-packed values for our predictor, MedianAgeMarriage_s. With the newdata argument, we’ll use those values to return model-implied expected values for Divorce. # define the range of `MedianAgeMarriage_s` values we&#39;d like to feed into `fitted()` nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories f &lt;- fitted(b5.1, newdata = nd) %&gt;% as_tibble() %&gt;% # tack the `nd` data onto the `fitted()` results bind_cols(nd) # plot ggplot(data = f, aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;Divorce&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = range(d$Divorce)) + theme_bw() + theme(panel.grid = element_blank()) Before fitting the next model, we’ll standardize Marriage. d &lt;- d %&gt;% mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage)) We’re ready to fit our second univariable model. b5.2 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.68 0.24 9.21 10.16 5192 1.00 ## Marriage_s 0.64 0.25 0.15 1.11 5646 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.75 0.18 1.44 2.15 4481 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ll wangle and plot our version of Figure 5.2.a. nd &lt;- tibble(Marriage_s = seq(from = -2.5, to = 3.5, length.out = 30)) f &lt;- fitted(b5.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ggplot(data = f, aes(x = Marriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = range(d$Divorce)) + ylab(&quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other. So we’ll build a multivariate model with the goal of measuring the partial value of each predictor. The question we want answered is: What is the predictive value of a variable, once I already know all of the other predictor variables? (p. 123, emphasis in the original) 5.1.1 Multivariate notation. Now we’ll get both predictors in there with our very first multivariable model. We can write the statistical model as \\[\\begin{align*} \\text{Divorce}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{Marriage_s}_i + \\beta_2 \\text{MedianAgeMarriage_s}_i \\\\ \\alpha &amp; \\sim \\text{Normal}(10, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 10) \\end{align*}\\] It might help to read the \\(+\\) symbols as “or” and then say: A State’s divorce rate can be a function of its marriage rate or its median age at marriage. The “or” indicates independent associations, which may be purely statistical or rather causal. (p. 124, emphasis in the original) 5.1.2 Fitting the model. Much like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on. b5.3 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.22 9.26 10.11 5365 1.00 ## Marriage_s -0.12 0.30 -0.72 0.48 3798 1.00 ## MedianAgeMarriage_s -1.13 0.30 -1.71 -0.54 3908 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.53 0.16 1.25 1.89 4753 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The stanplot() function is an easy way to get a default coefficient plot. You just put the brmsfit object into the function. stanplot(b5.3) There are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package. A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the posterior_samples() instead of the brmsfit object. # install.packages(&quot;bayesplot&quot;, dependencies = T) library(bayesplot) post &lt;- posterior_samples(b5.3) color_scheme_set(&quot;red&quot;) mcmc_intervals(post[, 1:4], prob = .5, point_est = &quot;median&quot;) + labs(title = &quot;My fancy bayesplot-based coefficient plot&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.line.x = element_line(size = 1/4), axis.line.y = element_blank(), axis.ticks.y = element_blank()) Because bayesplot produces a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out this vignette. The tidybaes::stat_pointintervalh() function offers a third way, this time with a more ground-up ggplot2 workflow. library(tidybayes) post %&gt;% select(-lp__) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, size = 3/4, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = NULL, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), panel.grid.major.y = element_line(color = alpha(&quot;firebrick4&quot;, 1/4), linetype = 3), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) The substantive interpretation of all those coefficient plots is: “Once we know median age at marriage for a State, there is little or no additive predictive power in also knowing the rate of marriage in that State” (p. 126, emphasis in the original). 5.1.3 Plotting multivariate posteriors. McElreath’s prose is delightfully deflationary. “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (p. 126). Now you’re inspired, let’s learn three: Predictor residual plots Counterfactual plots Posterior prediction plots 5.1.3.1 Predictor residual plots. To get ready to make our residual plots, we’ll predict Marriage_s with MedianAgeMarriage_s. b5.4 &lt;- brm(data = d, family = gaussian, Marriage_s ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Marriage_s ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.00 0.10 -0.20 0.20 5080 1.00 ## MedianAgeMarriage_s -0.72 0.10 -0.92 -0.51 5801 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.72 0.08 0.59 0.89 5149 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With fitted(), we compute the expected values for each state (with the exception of Nevada). Since the MedianAgeMarriage_s values for each state are in the date we used to fit the model, we’ll omit the newdata argument. f &lt;- fitted(b5.4) %&gt;% as_tibble() %&gt;% bind_cols(d) head(f) ## # A tibble: 6 x 19 ## Estimate Est.Error Q2.5 Q97.5 Location Loc Population ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0.435 0.120 0.197 0.670 Alabama AL 4.78 ## 2 0.492 0.124 0.245 0.735 Alaska AK 0.71 ## 3 0.147 0.104 -0.0568 0.352 Arizona AZ 6.33 ## 4 1.01 0.178 0.655 1.35 Arkansas AR 2.92 ## 5 -0.429 0.119 -0.663 -0.199 Califor… CA 37.2 ## 6 0.204 0.106 -0.00121 0.413 Colorado CO 5.03 ## # … with 12 more variables: MedianAgeMarriage &lt;dbl&gt;, Marriage &lt;dbl&gt;, ## # Marriage.SE &lt;dbl&gt;, Divorce &lt;dbl&gt;, Divorce.SE &lt;dbl&gt;, ## # WaffleHouses &lt;int&gt;, South &lt;int&gt;, Slaves1860 &lt;int&gt;, ## # Population1860 &lt;int&gt;, PropSlaves1860 &lt;dbl&gt;, MedianAgeMarriage_s &lt;dbl&gt;, ## # Marriage_s &lt;dbl&gt; After a little data processing, we can make Figure 5.3. f %&gt;% ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + coord_cartesian(ylim = range(d$Marriage_s)) + theme_bw() + theme(panel.grid = element_blank()) We get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual for details. With our residuals in hand, we just need a little more data processing to make Figure 5.4.a. r &lt;- residuals(b5.4) %&gt;% # to use this in ggplot2, we need to make it a tibble or data frame as_tibble() %&gt;% bind_cols(d) # for the annotation at the top text &lt;- tibble(Estimate = c(- 0.5, 0.5), Divorce = 14.1, label = c(&quot;slower&quot;, &quot;faster&quot;)) # plot r %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Marriage rate residuals&quot;, limits = c(-2, 2)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) To get the MedianAgeMarriage_s residuals, we have to fit the corresponding model first. b5.4b &lt;- brm(data = d, family = gaussian, MedianAgeMarriage_s ~ 1 + Marriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) And now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to Figure 5.4.b. text &lt;- tibble(Estimate = c(- 0.7, 0.5), Divorce = 14.1, label = c(&quot;younger&quot;, &quot;older&quot;)) residuals(b5.4b) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Age of marriage residuals&quot;, limits = c(-2, 3)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) 5.1.3.2 Counterfactual plots. A second sort of inferential plot displays the implied predictions of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variable you like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 129, emphasis in the original) Making Figure 5.5.a requires a little more data wrangling than before. # we need new `nd` data nd &lt;- tibble(Marriage_s = seq(from = -3, to = 3, length.out = 30), MedianAgeMarriage_s = mean(d$MedianAgeMarriage_s)) fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since `fitted()` and `predict()` name their intervals the same way, # we&#39;ll need to `rename()` them to keep them straight rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% # note how we&#39;re just nesting the `predict()` code right inside `bind_cols()` bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since we only need the intervals, we&#39;ll use `transmute()` rather than `mutate()` transmute(p_ll = Q2.5, p_ul = Q97.5), # now tack on the `nd` data nd) %&gt;% # we&#39;re finally ready to plot ggplot(aes(x = Marriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + coord_cartesian(xlim = range(d$Marriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMedianAgeMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) We follow the same process for Figure 5.5.b. # new data nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30), Marriage_s = mean(d$Marriage_s)) # `fitted()` + `predict()` fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), nd ) %&gt;% # plot ggplot(aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) A tension with such plots, however, lies in their counterfactual nature. In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not… …If our goal is to intervene in the world, there may not be any realistic way to manipulate each predictor without also manipulating the others. This is a serious obstacle to applied science, whether you are an ecologist, an economist, or an epidemiologist [or a psychologist] (p. 131) 5.1.3.3 Posterior prediction plots. “In addition to understanding the estimates, it’s important to check the model fit against the observed data” (p. 131). For more on the topic, check out Gabry and colleagues’ Visualization in Bayesian workflow or Simpson’s related blog post Touch me, I want to feel your data. In this version of Figure 5.6.a, the thin lines are the 95% intervals and the thicker lines are +/- the posterior \\(SD\\), both of which are returned when you use fitted(). fitted(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Divorce, y = Estimate)) + geom_abline(linetype = 2, color = &quot;grey50&quot;, size = .5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 3/4) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), size = 1/4, color = &quot;firebrick4&quot;) + geom_linerange(aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), size = 1/2, color = &quot;firebrick4&quot;) + # Note our use of the dot placeholder, here: https://magrittr.tidyverse.org/reference/pipe.html geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)), aes(label = Loc), hjust = 0, nudge_x = - 0.65) + labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) In order to make Figure 5.6.b, we need to clarify the relationships among fitted(), predict(), and residuals(). Here’s my attempt in a table. tibble(`brms function` = c(&quot;fitted&quot;, &quot;predict&quot;, &quot;residual&quot;), mean = c(&quot;same as the data&quot;, &quot;same as the data&quot;, &quot;in a deviance-score metric&quot;), scale = c(&quot;excludes sigma&quot;, &quot;includes sigma&quot;, &quot;excludes sigma&quot;)) %&gt;% knitr::kable() brms function mean scale fitted same as the data excludes sigma predict same as the data includes sigma residual in a deviance-score metric excludes sigma Hopefully this clarifies that if we want to incorporate the prediction interval in a deviance metric, we’ll need to first use predict() and then subtract the intervals from their corresponding Divorce values in the data. residuals(b5.3) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), d ) %&gt;% # here we put our `predict()` intervals into a deviance metric mutate(p_ll = Divorce - p_ll, p_ul = Divorce - p_ul) %&gt;% # now plot! ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) + geom_hline(yintercept = 0, size = 1/2, color = &quot;firebrick4&quot;, alpha = 1/10) + geom_pointrange(aes(ymin = f_ll, ymax = f_ul), size = 2/5, shape = 20, color = &quot;firebrick4&quot;) + geom_segment(aes(y = Estimate - Est.Error, yend = Estimate + Est.Error, x = Loc, xend = Loc), size = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(y = p_ll, yend = p_ul, x = Loc, xend = Loc), size = 3, color = &quot;firebrick4&quot;, alpha = 1/10) + labs(x = NULL, y = NULL) + coord_flip(ylim = c(-6, 5)) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Compared to the last couple plots, Figure 5.6.c is pretty simple. residuals(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(wpc = WaffleHouses / Population) %&gt;% ggplot(aes(x = wpc, y = Estimate)) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, size = 1/2, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;AR&quot;, &quot;MS&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;ID&quot;)), aes(label = Loc), seed = 5.6) + scale_x_continuous(&quot;Waffles per capita&quot;, limits = c(0, 45)) + coord_cartesian(xlim = range(0, 40)) + ylab(&quot;Divorce error&quot;) + theme_bw() + theme(panel.grid = element_blank()) More McElreath inspiration: “No matter how many predictors you’ve already included in a regression, it’s still possible to find spurious correlations with the remaining variation” (p. 134). To keep our deflation train going, it’s worthwhile to repeat the message in McElreath’s Rethinking: Stats, huh, yeah what is it good for? box. Often people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is real or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of another predictor, we might never notice this. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 134) 5.1.3.4 Overthinking: Simulating spurious association. n &lt;- 100 # number of cases set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_real = rnorm(n), # x_real as Gaussian with mean 0 and SD 1 (i.e., the defaults) x_spur = rnorm(n, x_real), # x_spur as Gaussian with mean = x_real y = rnorm(n, x_real)) # y as Gaussian with mean = x_real Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) We may as well fit a model. brm(data = d, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) %&gt;% fixef() %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.00 0.10 -0.20 0.19 ## x_real 0.98 0.15 0.69 1.27 ## x_spur 0.06 0.09 -0.12 0.24 5.2 Masked relationship Let’s load those tasty milk data. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) You might inspect the data like this. d %&gt;% select(kcal.per.g, mass, neocortex.perc) %&gt;% pairs(col = &quot;firebrick4&quot;) By just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math. McElreath has us start of with a simple univaraible milk model. b5.5 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex.perc, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) The uniform prior was difficult on Stan. After playing around a bit, I just switched to a unit-scale half Cauchy. Similar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 14 where we’ll learn how to do better. To compliment how McElreath removed cases with missing values on our variables of interest with Base R complete.cases(), here we’ll do so with tidyr::drop_na() and a little help with ends_with(). dcc &lt;- d %&gt;% drop_na(ends_with(&quot;_s&quot;)) But anyway, let’s inspect the parameter summary. print(b5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc ## Data: d (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.339 0.559 -0.752 1.445 5955 1.000 ## neocortex.perc 0.005 0.008 -0.012 0.021 5933 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.192 0.041 0.132 0.288 2959 1.001 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice now we set digits = 3 within print() much the way McElreath set digits=3 within precis()? To get the brms answer to what McElreath did with coef(), we’ll use the fixef() function. fixef(b5.5)[2] * (76 - 55) ## [1] 0.09907782 Yes, indeed, “that’s less than 0.1 kilocalories” (p. 137). Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.7, top left. nd &lt;- tibble(neocortex.perc = 54:80) fitted(b5.5, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Do note the probs argument in the fitted() code, above. Let’s make the log_mass variable. dcc &lt;- dcc %&gt;% mutate(log_mass = log(mass)) Now we use log_mass as the new sole predictor. b5.6 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 5) print(b5.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.705 0.060 0.586 0.821 5121 1.000 ## log_mass -0.032 0.024 -0.080 0.018 4577 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.183 0.037 0.128 0.270 3816 1.001 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, top right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30)) fitted(b5.6, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Finally, we’re ready to fit with both predictors included in the “joint model.” Here’s the statistical formula \\[\\begin{align*} \\text{kcal.per.g}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex.perc}_i + \\beta_2 \\text{log}(\\text{mass}_i) \\\\ \\alpha &amp; \\sim \\text{Normal}(0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 1) \\end{align*}\\] Note, the HMC chains required a longer warmup period and a higher adapt_delta setting for the model to converge properly. Life will be much better once we ditch the uniform prior for good. b5.7 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + neocortex.perc + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), seed = 5) print(b5.7, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -1.084 0.570 -2.197 0.092 3602 1.000 ## neocortex.perc 0.028 0.009 0.010 0.045 3496 1.000 ## log_mass -0.096 0.027 -0.150 -0.040 3273 1.001 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.139 0.030 0.095 0.210 3215 1.002 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, bottom left. nd &lt;- tibble(neocortex.perc = 54:80 %&gt;% as.double(), log_mass = mean(dcc$log_mass)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) And make Figure 5.7, bottom right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30), neocortex.perc = mean(dcc$neocortex.perc)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) What [this regression model did was] ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model [asked] if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we statistically account for both. (pp. 140–141, emphasis in the original) 5.2.0.1 Overthinking: Simulating a masking relationship. n &lt;- 100 # number of cases rho &lt;- .7 # correlation between x_pos and x_neg set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_pos = rnorm(n), # x_pos as a standard Gaussian x_neg = rnorm(n, rho * x_pos, sqrt(1 - rho^2)), # x_neg correlated with x_pos y = rnorm(n, x_pos - x_neg)) # y equally associated with x_pos and x_neg Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) Here we fit the models with a little help from the update() function. b5.O.both &lt;- brm(data = d, family = gaussian, y ~ 1 + x_pos + x_neg, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), seed = 5) b5.O.pos &lt;- update(b5.O.both, formula = y ~ 1 + x_pos) b5.O.neg &lt;- update(b5.O.both, formula = y ~ 1 + x_neg) Compare the coefficients. fixef(b5.O.pos) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.12 -0.25 0.22 ## x_pos 0.26 0.12 0.02 0.51 fixef(b5.O.neg) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.01 0.12 -0.23 0.24 ## x_neg -0.29 0.11 -0.50 -0.08 fixef(b5.O.both) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.00 0.10 -0.20 0.20 ## x_pos 0.97 0.15 0.69 1.25 ## x_neg -0.90 0.13 -1.15 -0.64 5.3 Multicollinearity Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will say that a very large range of parameter values are plausible, from tiny associations to massive ones, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how statistical control works. So once you understand multicollinearity, you will better understand [multivariable] models in general. (pp. 141–142) 5.3.1 Multicollinear legs. Let’s simulate some leg data. n &lt;- 100 set.seed(5) d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) leg_left and leg_right are highly correlated. d %&gt;% select(leg_left:leg_right) %&gt;% cor() %&gt;% round(digits = 4) ## leg_left leg_right ## leg_left 1.0000 0.9996 ## leg_right 0.9996 1.0000 Have you ever even seen a \\(\\rho = .9996\\) correlation, before? Here it is in a plot. d %&gt;% ggplot(aes(x = leg_left, y = leg_right)) + geom_point(alpha = 1/2, color = &quot;firebrick4&quot;) + theme_bw() + theme(panel.grid = element_blank()) Here’s our attempt to predict height with both legs. b5.8 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Let’s inspect the damage. print(b5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left + leg_right ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.79 0.29 1.22 2.36 5544 1.00 ## leg_left 0.66 2.21 -3.52 4.96 2124 1.00 ## leg_right 1.18 2.21 -3.14 5.39 2127 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.61 0.04 0.52 0.70 3271 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior [estimates] and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 143). Here’s our coefficient plot using brms::stanplot() with a little help from bayesplot::color_scheme_set(). color_scheme_set(&quot;red&quot;) stanplot(b5.8, type = &quot;intervals&quot;, prob = .5, prob_outer = .95, point_est = &quot;median&quot;) + labs(title = &quot;The coefficient plot for the two-leg model&quot;, subtitle = &quot;Holy smokes; look at the widths of those betas!&quot;) + theme_bw() + theme(text = element_text(size = 14), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Now you can use the brms::stanplot() function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with color_scheme_set(). This is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients, Figure 6.2.a. pairs(b5.8, pars = parnames(b5.8)[2:3]) If you’d like a nicer and more focused attempt, you might have to revert to the posterior_samples() function and a little ggplot2 code. post &lt;- posterior_samples(b5.8) post %&gt;% ggplot(aes(x = b_leg_left, y = b_leg_right)) + geom_point(color = &quot;firebrick&quot;, alpha = 1/10, size = 1/3) + theme_bw() + theme(panel.grid = element_blank()) While we’re at it, you can make a similar plot with the mcmc_scatter() function. post %&gt;% mcmc_scatter(pars = c(&quot;b_leg_left&quot;, &quot;b_leg_right&quot;), size = 1/3, alpha = 1/10) + theme_bw() + theme(panel.grid = element_blank()) But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign. post %&gt;% select(b_leg_left:b_leg_right) %&gt;% cor() ## b_leg_left b_leg_right ## b_leg_left 1.0000000 -0.9995885 ## b_leg_right -0.9995885 1.0000000 On page 165, McElreath clarified that “from the computer’s perspective, this model is simply:” \\[\\begin{align*} y_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_2) x_i \\end{align*}\\] Accordingly, here’s the posterior of the sum of the two regression coefficients, Figure 6.2.b. We’ll use tidybayes::geom_halfeyeh() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base. library(tidybayes) post %&gt;% ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + geom_halfeyeh(fill = &quot;firebrick&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Sum the multicollinear coefficients&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;) + theme_bw() + theme(panel.grid = element_blank()) Now we fit the model after ditching one of the leg lengths. b5.9 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.79 0.29 1.22 2.34 5717 1.00 ## leg_left 1.84 0.06 1.72 1.96 6040 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.60 0.04 0.52 0.70 6598 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That posterior \\(SD\\) looks much better. Compare this density to the one in Figure 6.1.b. posterior_samples(b5.9) %&gt;% ggplot(aes(x = b_leg_left, y = 0)) + geom_halfeyeh(fill = &quot;firebrick&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Just one coefficient needed&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;, x = &quot;only b_leg_left, this time&quot;) + theme_bw() + theme(panel.grid = element_blank()) When two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such a case. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. (p. 145, emphasis in the original) 5.3.2 Multicollinear milk. Multicollinearity arises in real data, too. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = TRUE) library(brms) We’ll follow the text and fit the two univariable models, first. Note our use of the update() function. # `kcal.per.g` regressed on `perc.fat` b5.10 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + perc.fat, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) # `kcal.per.g` regressed on `perc.lactose` b5.11 &lt;- update(b5.10, newdata = d, formula = kcal.per.g ~ 1 + perc.lactose) Compare the coefficients. posterior_summary(b5.10) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.301 0.039 0.222 0.377 ## b_perc.fat 0.010 0.001 0.008 0.012 ## sigma 0.080 0.012 0.061 0.108 ## lp__ 24.015 1.305 20.626 25.482 posterior_summary(b5.11) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.166 0.047 1.074 1.257 ## b_perc.lactose -0.011 0.001 -0.012 -0.009 ## sigma 0.067 0.010 0.051 0.089 ## lp__ 28.792 1.285 25.491 30.290 If you’d like to get just the 95% intervals similar to the way McElreath reported them in the prose on page 146, you might use the handy posterior_interval() function. posterior_interval(b5.10)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## 0.008 0.012 posterior_interval(b5.11)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## -0.012 -0.009 Now “watch what happens when we place both predictor varaibles in the same regression model” (p. 146). b5.12 &lt;- update(b5.11, newdata = d, formula = kcal.per.g ~ 1 + perc.fat + perc.lactose) posterior_summary(b5.12) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.006 0.221 0.565 1.439 ## b_perc.fat 0.002 0.003 -0.003 0.007 ## b_perc.lactose -0.009 0.003 -0.014 -0.003 ## sigma 0.067 0.010 0.052 0.090 ## lp__ 27.685 1.443 24.197 29.533 You can make custom pairs plots with GGalley, which will also compute the point estimates for the bivariate correlations. Here’s a default plot. #install.packages(&quot;GGally&quot;, dependencies = T) library(GGally) ggpairs(data = d, columns = c(3:4, 6)) + theme(panel.grid = element_blank()) But you can customize these, too. E.g., my_diag &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;firebrick4&quot;, size = 0) } my_lower &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick4&quot;, size = 1/3, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(color = &quot;firebrick&quot;, alpha = .8, size = 1/4) } # then plug those custom functions into `ggpairs()` ggpairs(data = d, columns = c(3:4, 6), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(strip.background = element_rect(fill = &quot;white&quot;, color = &quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) Our two predictor “variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps much once you already know the other” (p. 148, emphasis in the original). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also showed the Pearson’s correlation coefficients, se we don’t need to use the cor() function like McElreath did in the text. In the next section, we’ll run the simulation necessary for our version of Figure 5.10. 5.3.2.1 Overthinking: Simulating collinearity. First we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block. sim_coll &lt;- function(seed, rho){ set.seed(seed) d &lt;- d %&gt;% mutate(x = rnorm(n(), mean = perc.fat * rho, sd = sqrt((1 - rho^2) * var(perc.fat)))) m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d) sqrt(diag(vcov(m)))[2] # parameter SD } # how many simulations per `rho`-value would you like? n_seed &lt;- 100 # how many `rho`-values from 0 to .99 would you like to evaluate the process over? n_rho &lt;- 30 d &lt;- tibble(seed = 1:n_seed) %&gt;% expand(seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) %&gt;% group_by(rho) %&gt;% # we&#39;ll `summarise()` our output by the mean and 95% intervals summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) We’ve added 95% interval bands to our version of Figure 5.10. d %&gt;% ggplot(aes(x = rho, y = mean)) + geom_smooth(aes(ymin = ll, ymax = ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + labs(x = expression(rho), y = &quot;parameter SD&quot;) + coord_cartesian(ylim = c(0, .0072)) + theme_bw() + theme(panel.grid = element_blank()) Did you notice we used the base R lm() function to fit the models? As McElreath rightly pointed out, lm() presumes flat priors. Proper Bayesian modeling could improve on that. But then we’d have to wait for a whole lot of HMC chains to run and until our personal computers or the algorithms we use to fit our Bayesian models become orders of magnitude faster, we just don’t have time for that. 5.3.3 Post-treatment bias. It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph: seed and sprout plants measure heights apply different antifungal soil treatments (i.e., the experimental manipulation) measure (a) the heights and (b) the presence of fungus Based on the design, let’s simulate our data. n &lt;- 100 set.seed(5) d &lt;- tibble(h0 = rnorm(n, mean = 10, sd = 2), treatment = rep(0:1, each = n / 2), fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4), h1 = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) We’ll use head() to peek at the data. d %&gt;% head() ## # A tibble: 6 x 4 ## h0 treatment fungus h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 8.32 0 0 14.3 ## 2 12.8 0 0 18.5 ## 3 7.49 0 1 8.97 ## 4 10.1 0 1 12.9 ## 5 13.4 0 1 14.8 ## 6 8.79 0 1 12.0 These data + the model were rough on Stan, at first, which spat out warnings about divergent transitions. The model ran fine after setting warmup = 1000 and adapt_delta = 0.99. b5.13 &lt;- brm(data = d, family = gaussian, h1 ~ 1 + h0 + treatment + fungus, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.99), seed = 5) print(b5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ 1 + h0 + treatment + fungus ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.59 0.57 4.50 6.68 2685 1.00 ## h0 0.94 0.05 0.83 1.04 3336 1.00 ## treatment 0.04 0.22 -0.40 0.48 689 1.01 ## fungus -2.77 0.26 -3.29 -2.26 1790 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.01 0.08 0.88 1.17 3146 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now fit the model after excluding fungus, our post-treatment variable. b5.14 &lt;- update(b5.13, formula = h1 ~ 1 + h0 + treatment) print(b5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 + treatment ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.49 0.84 3.83 7.11 1637 1.00 ## h0 0.82 0.08 0.67 0.98 1659 1.00 ## treatment 1.04 0.31 0.44 1.69 1080 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.53 0.11 1.33 1.76 3090 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “Now the impact of treatment is strong and positive, as it should be” (p. 152). In this case, there were really two outcomes. The first was the one we modeled, the height at the end of the experiment (i.e., h1). The second outcome, which was clearly related to h1, was the presence of fungus, captured by our binomial variable fungus. If you wanted to model that, you’d fit a logistic regression model, which we’ll learn about in Chapter 10. 5.4 Categorical varaibles Many readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are included in a model… Knowing how the machine works removes a lot of this difficulty. (p. 153, emphasis in the original) 5.4.1 Binary categories. Reload the Howell1 data. library(rethinking) data(Howell1) d &lt;- Howell1 Unload rethinking and load brms. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Just in case you forgot what these data were like: d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.830… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.9925… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, … Let’s fit the first height model with the male dummy. Note. The uniform prior McElreath used in the text in conjunction with the brms::brm() function seemed to cause problems for the HMC chains, here. After experimenting with start values, increasing warmup, and increasing adapt_delta, switching out the uniform prior did the trick. Anticipating Chapter 8, I recommend you use a weakly-regularizing half Cauchy for \\(\\sigma\\). b5.15 &lt;- brm(data = d, family = gaussian, height ~ 1 + male, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Check the summary. print(b5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + male ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 134.83 1.61 131.65 137.94 5668 1.00 ## male 7.32 2.25 2.98 11.76 5642 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.38 0.85 25.80 29.12 5179 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our samples from the posterior are already in the HMC iterations. All we need to do is put them in a data frame and then put them to work. post &lt;- posterior_samples(b5.15) post %&gt;% transmute(male_height = b_Intercept + b_male) %&gt;% mean_qi(.width = .89) ## male_height .lower .upper .width .point .interval ## 1 142.1473 139.5225 144.7803 0.89 mean qi You can also do this with fitted(). nd &lt;- tibble(male = 1) fitted(b5.15, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 142.1473 1.653697 138.9162 145.3928 And you could even plot. fitted(b5.15, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied male heights&quot;, x = expression(alpha + beta[&quot;male&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.1.1 Overthinking: Re-parameterizing the model. The reparameterized model follows the form \\[\\begin{align*} \\text{height}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_\\text{female} (1 - \\text{male}_i) + \\alpha_\\text{male} \\text{male}_i \\end{align*}\\] So then a female dummy would satisfy the condition \\(\\text{female}_i = (1 - \\text{male}_i)\\). Let’s make that dummy. d &lt;- d %&gt;% mutate(female = 1 - male) Everyone has their own idiosyncratic way of coding. One of my quirks is I always explicitly specify a model’s intercept following the form y ~ 1 + x, where y is the criterion, x stands for the predictors, and 1 is the intercept. You don’t have to do this, of course. You could just code y ~ x to get the same results. The brm() function assumes you want that intercept. One of the reasons I like the verbose version is it reminds me to think about the intercept and to include it in my priors. Another nice feature is that is helps me make sense of the code for this model: height ~ 0 + male + female. When we replace … ~ 1 + … with … ~ 0 + …, we tell brm() to remove the intercept. Removing the intercept allows us to include ALL levels of a given categorical variable in our model. In this case, we’ve expressed sex as two dummies, female and male. Taking out the intercept lets us put both dummies into the formula. b5.15b &lt;- brm(data = d, family = gaussian, height ~ 0 + male + female, prior = c(prior(normal(178, 100), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.15b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + male + female ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## male 142.35 1.71 139.10 145.69 5738 1.00 ## female 134.66 1.62 131.47 137.86 6261 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.37 0.81 25.80 29.02 6113 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If we wanted the formal difference score from such a model, we’d subtract. posterior_samples(b5.15b) %&gt;% transmute(dif = b_male - b_female) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied difference score&quot;, x = expression(alpha[&quot;male&quot;] - alpha[&quot;female&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.2 Many categories. When there are more than two categories, you’ll need more than one dummy variable. Here’s the general rule: To include \\(k\\) categories in a linear model, you require \\(k - 1\\) dummy variables. Each dummy variable indicates, with the value 1, a unique category. The category with no dummy variable assigned to it ends up again as the “intercept” category. (p. 155) We’ll practice with milk. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) With the tidyverse, we can peek at clade with distinct() in the place of base R unique(). d %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape As clade has 4 categories, let’s use ifelse() to convert these to 4 dummy variables. d &lt;- d %&gt;% mutate(clade_nwm = ifelse(clade == &quot;New World Monkey&quot;, 1, 0), clade_owm = ifelse(clade == &quot;Old World Monkey&quot;, 1, 0), clade_s = ifelse(clade == &quot;Strepsirrhine&quot;, 1, 0), clade_ape = ifelse(clade == &quot;Ape&quot;, 1, 0)) Now we’ll fit the model with three of the four dummies. In this model, clade_ape is the reference category captured by the intercept. b5.16 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.16) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.55 0.04 0.46 0.63 4863 1.00 ## clade_nwm 0.17 0.06 0.05 0.29 5173 1.00 ## clade_owm 0.24 0.07 0.11 0.37 5533 1.00 ## clade_s -0.04 0.07 -0.18 0.10 5034 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.17 4644 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we grab the chains, our draws from the posterior. post &lt;- b5.16 %&gt;% posterior_samples() head(post) ## b_Intercept b_clade_nwm b_clade_owm b_clade_s sigma lp__ ## 1 0.5089460 0.22448585 0.1852076 -0.0228514301 0.1255965 8.818860 ## 2 0.5224140 0.16995631 0.3601027 0.0031701950 0.1180354 8.879475 ## 3 0.5439904 0.14703171 0.2392020 -0.0708894669 0.1073817 10.533591 ## 4 0.5687371 0.18486763 0.1938879 0.0199769087 0.1299403 9.279736 ## 5 0.5991461 0.15058836 0.1599358 0.0003752874 0.1305265 8.433250 ## 6 0.5611899 0.08095519 0.2847117 -0.0473311617 0.1168317 8.675562 You might compute averages for each category and summarizing the results with the transpose of base R’s apply() function, rounding to two digits of precision. post$mu_ape &lt;- post$b_Intercept post$mu_nwm &lt;- post$b_Intercept + post$b_clade_nwm post$mu_owm &lt;- post$b_Intercept + post$b_clade_owm post$mu_s &lt;- post$b_Intercept + post$b_clade_s round(t(apply(post[ ,7:10], 2, quantile, c(.5, .025, .975))), digits = 2) ## 50% 2.5% 97.5% ## mu_ape 0.55 0.46 0.63 ## mu_nwm 0.71 0.63 0.80 ## mu_owm 0.79 0.69 0.89 ## mu_s 0.51 0.39 0.62 Here’s a more tidyverse sort of way to get the same thing, but this time with means and HPDIs via the tidybayes::mean_hdi() function. post %&gt;% transmute(mu_ape = b_Intercept, mu_nwm = b_Intercept + b_clade_nwm, mu_owm = b_Intercept + b_clade_owm, mu_s = b_Intercept + b_clade_s) %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu_ape 0.55 0.46 0.63 0.95 mean hdi ## 2 mu_nwm 0.71 0.62 0.8 0.95 mean hdi ## 3 mu_owm 0.79 0.69 0.89 0.95 mean hdi ## 4 mu_s 0.51 0.4 0.62 0.95 mean hdi You could also summarize with fitted(). nd &lt;- tibble(clade_nwm = c(1, 0, 0, 0), clade_owm = c(0, 1, 0, 0), clade_s = c(0, 0, 1, 0), primate = c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;)) fitted(b5.16, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(primate = rep(c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;), each = n() / 4)) %&gt;% ggplot(aes(x = value, y = reorder(primate, value))) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + labs(x = &quot;kcal.per.g&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) And there are multiple ways to compute summary statistics for the difference between NWM and OWM, too. # base R quantile(post$mu_nwm - post$mu_owm, probs = c(.5, .025, .975)) ## 50% 2.5% 97.5% ## -0.07330109 -0.20607772 0.06626330 # tidyverse + tidybayes post %&gt;% transmute(dif = mu_nwm - mu_owm) %&gt;% median_qi() ## dif .lower .upper .width .point .interval ## 1 -0.07330109 -0.2060777 0.0662633 0.95 median qi 5.4.3 Adding regular predictor variables. If we wanted to fit the model including perc.fat as an additional predictor, the basic statistical formula would be \\[ \\mu_i = \\alpha + \\beta_\\text{clade_nwm} \\text{clade_nwm}_i + \\beta_\\text{clade_owm} \\text{clade_owm}_i + \\beta_\\text{clade_s} \\text{clade_s}_i + \\beta_\\text{perc.fat} \\text{perc.fat}_i \\] The corresponding formula argument within brm() would be kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s + perc.fat. 5.4.4 Another approach: Unique intercepts. Using the code below, there’s no need to transform d$clade into d$clade_id. The advantage of this approach is the indices in the model summary are more descriptive than a[1] through a[4]. b5.16_alt &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 0 + clade, prior = c(prior(normal(.6, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.16_alt) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 0 + clade ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## cladeApe 0.55 0.04 0.46 0.63 7342 1.00 ## cladeNewWorldMonkey 0.71 0.04 0.63 0.80 7034 1.00 ## cladeOldWorldMonkey 0.79 0.05 0.68 0.89 7259 1.00 ## cladeStrepsirrhine 0.51 0.06 0.39 0.62 6890 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.18 4736 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See? This is much easier than trying to remember which one was which in an arbitrary numeric index. 5.5 Ordinary least squares and lm() Since this section centers on the frequentist lm() function, I’m going to largely ignore it. A couple things, though. You’ll note how the brms package uses the lm()-like design formula syntax. Although not as pedagogical as the more formal rethinking syntax, it has the advantage of cohering with the popular lme4 syntax for multilevel models. Also, on page 161 McElreath clarified that one cannot use the I() syntax with his rethinking package. Not so with brms. The I() syntax works just fine with brms::brm(). We’ve already made use of it in the “Polynomial regression” section of Chapter 4. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] GGally_1.4.0 tidybayes_1.1.0 bayesplot_1.7.0 ## [4] fiftystater_1.0.1 ggrepel_0.8.1 forcats_0.4.0 ## [7] stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [10] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ## [13] tidyverse_1.2.1 brms_2.9.0 Rcpp_1.0.1 ## [16] dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 ## [19] ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.13 ggstance_0.3.1 ## [5] markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 ## [9] svUnit_0.7-12 DT_0.7 ## [11] fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 ## [15] codetools_0.2-16 bridgesampling_0.6-0 ## [17] knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 jsonlite_1.6 ## [21] broom_0.5.2 shiny_1.3.2 ## [23] mapproj_1.2.6 compiler_3.6.3 ## [25] httr_1.4.0 backports_1.1.4 ## [27] assertthat_0.2.1 Matrix_1.2-17 ## [29] lazyeval_0.2.2 cli_1.1.0 ## [31] later_0.8.0 htmltools_0.3.6 ## [33] prettyunits_1.0.2 tools_3.6.3 ## [35] igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 ## [39] reshape2_1.4.3 maps_3.3.0 ## [41] V8_2.2 cellranger_1.1.0 ## [43] vctrs_0.1.0 nlme_3.1-144 ## [45] crosstalk_1.0.0 xfun_0.7 ## [47] ps_1.3.0 rvest_0.3.4 ## [49] mime_0.7 miniUI_0.1.1.1 ## [51] lifecycle_0.1.0 gtools_3.8.1 ## [53] MASS_7.3-51.5 zoo_1.8-6 ## [55] scales_1.1.1.9000 colourpicker_1.0 ## [57] hms_0.4.2 promises_1.0.1 ## [59] Brobdingnag_1.2-6 inline_0.3.15 ## [61] RColorBrewer_1.1-2 shinystan_2.5.0 ## [63] yaml_2.2.0 curl_3.3 ## [65] gridExtra_2.3 loo_2.1.0 ## [67] reshape_0.8.8 stringi_1.4.3 ## [69] highr_0.8 dygraphs_1.1.1.6 ## [71] boot_1.3-24 pkgbuild_1.0.3 ## [73] shape_1.4.4 rlang_0.4.0 ## [75] pkgconfig_2.0.2 matrixStats_0.54.0 ## [77] HDInterval_0.2.0 evaluate_0.14 ## [79] lattice_0.20-38 labeling_0.3 ## [81] rstantools_1.5.1 htmlwidgets_1.3 ## [83] processx_3.3.1 tidyselect_0.2.5 ## [85] plyr_1.8.4 magrittr_1.5 ## [87] bookdown_0.11 R6_2.4.0 ## [89] generics_0.0.2 pillar_1.4.1 ## [91] haven_2.1.0 withr_2.1.2 ## [93] xts_0.11-2 abind_1.4-5 ## [95] modelr_0.1.4 crayon_1.3.4 ## [97] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [99] rmarkdown_1.13 grid_3.6.3 ## [101] readxl_1.3.1 callr_3.2.0 ## [103] threejs_0.3.1 digest_0.6.19 ## [105] xtable_1.8-4 httpuv_1.5.1 ## [107] stats4_3.6.3 munsell_0.5.0 ## [109] shinyjs_1.0 "],
["overfitting-regularization-and-information-criteria.html", "6 Overfitting, Regularization, and Information Criteria 6.1 The problem with parameters 6.2 Information theory and model performance 6.3 Regularization 6.4 Information criteria 6.5 Using information criteria 6.6 Summary Bonus: \\(R^2\\) talk Reference Session info", " 6 Overfitting, Regularization, and Information Criteria In this chapter we contend with two contrasting kinds of statistical error: overfitting, “which leads to poor prediction by learning too much from the data” underfitting, “which leads to poor prediction by learning too little from the data” (p. 166, emphasis added) 6.1 The problem with parameters The \\(R^2\\) is a popular way to measure how well you can retrodict the data. It traditionally follows the form \\[R^2 = \\frac{\\text{var(outcome)} - \\text{var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\] By \\(\\text{var()}\\), of course, we meant variance (i.e., the var() function in R). McElreath’s not a fan of the \\(R^2\\). But it’s important in my field, so instead of a summary at the end of the chapter, we will cover the Bayesian version of \\(R^2\\) and how to use it in brms. 6.1.1 More parameters always improve fit. We’ll start off by making the data with brain size and body size for seven species. library(tidyverse) ( d &lt;- tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) ) ## # A tibble: 7 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 africanus 452 35.5 ## 3 habilis 612 34.5 ## 4 boisei 521 41.5 ## 5 rudolfensis 752 55.5 ## 6 ergaster 871 61 ## 7 sapiens 1350 53.5 Let’s get ready for Figure 6.2. The plots in this chapter will be characterized by theme_classic() + theme(text = element_text(family = &quot;Courier&quot;)). Our color palette will come from the rcartocolor package, which provides color schemes designed by ‘CARTO’. # install.packages(&quot;rcartocolor&quot;, dependencies = T) library(rcartocolor) The specific palette we’ll be using is “BurgYl.” In addition to palettes, the rcartocolor package offers a few convenience functions which make it easier to use their palettes. The carto_pal() function will return the HEX numbers associated with a given palette’s colors and the display_carto_pal() function will display the actual colors. carto_pal(7, &quot;BurgYl&quot;) ## [1] &quot;#fbe6c5&quot; &quot;#f5ba98&quot; &quot;#ee8a82&quot; &quot;#dc7176&quot; &quot;#c8586c&quot; &quot;#9c3f5d&quot; &quot;#70284a&quot; display_carto_pal(7, &quot;BurgYl&quot;) We’ll be using a diluted version of the third color for the panel background (i.e., theme(panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)))) and the darker purples for other plot elements. Here’s the plot. library(ggrepel) d %&gt;% ggplot(aes(x = mass, y = brain, label = species)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_text_repel(size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[7], family = &quot;Courier&quot;, seed = 438) + coord_cartesian(xlim = 30:65) + labs(x = &quot;body mass (kg)&quot;, y = &quot;brain volume (cc)&quot;, subtitle = &quot;Average brain volume by body\\nmass for six hominin species&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Let’s fit the first six models in bulk. First we’ll make a custom function, fit_lm(), into which we’ll feed the desired names and formulas of our models. We’ll make a tibble initially composed of those names (i.e., model) and formulas (i.e., formula). Via purrr::map2() within mutate(), we’ll then fit the models and save the model objects within the tibble. The broom package provides an array of convenience functions to convert statistical analysis summaries into tidy data objects. We’ll employ broom::tidy() and broom::glance() to extract information from the model fits. library(broom) fit_lm &lt;- function(model, formula){ model &lt;- lm(data = d, formula = formula) } fits &lt;- tibble(model = str_c(&quot;b6.&quot;, 1:6), formula = c(&quot;brain ~ mass&quot;, &quot;brain ~ mass + I(mass^2)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)&quot;)) %&gt;% mutate(fit = map2(model, formula, fit_lm)) %&gt;% mutate(tidy = map(fit, tidy), glance = map(fit, glance)) # what did we just do? print(fits) ## # A tibble: 6 x 5 ## model formula fit tidy glance ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 b6.1 brain ~ mass &lt;lm&gt; &lt;tibble [2 ×… &lt;tibble [1 × … ## 2 b6.2 brain ~ mass + I(mass^2) &lt;lm&gt; &lt;tibble [3 ×… &lt;tibble [1 × … ## 3 b6.3 brain ~ mass + I(mass^2) + I(mass^3) &lt;lm&gt; &lt;tibble [4 ×… &lt;tibble [1 × … ## 4 b6.4 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) &lt;lm&gt; &lt;tibble [5 ×… &lt;tibble [1 × … ## 5 b6.5 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(ma… &lt;lm&gt; &lt;tibble [6 ×… &lt;tibble [1 × … ## 6 b6.6 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(ma… &lt;lm&gt; &lt;tibble [7 ×… &lt;tibble [1 × … Our fits object is a nested tibble. To learn more about this bulk approach to fitting models, check out Hadley Wickham’s talk Managing many models with R. As you might learn in the talk, we can extract the \\(R^2\\) from each model with map_dbl(&quot;r.squared&quot;), which we’ll then display in a plot. fits &lt;- fits %&gt;% mutate(r2 = glance %&gt;% map_dbl(&quot;r.squared&quot;)) %&gt;% mutate(r2_text = round(r2, digits = 2) %&gt;% as.character() %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) fits %&gt;% ggplot(aes(x = r2, y = formula, label = r2_text)) + geom_text(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 3.5) + scale_x_continuous(expression(italic(R)^2), limits = 0:1, breaks = 0:1) + ylab(NULL) + theme_classic() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If we wanted to look at the model coefficients, we could unnest(tidy) and wrangle a bit. fits %&gt;% unnest(tidy) %&gt;% select(model, term:estimate) %&gt;% mutate_if(is.double, round, digits = 1) %&gt;% complete(term = distinct(., term), model) %&gt;% spread(key = term, value = estimate) %&gt;% select(model, `(Intercept)`, mass, everything()) ## # A tibble: 6 x 8 ## model `(Intercept)` mass `I(mass^2)` `I(mass^3)` `I(mass^4)` `I(mass^5)` `I(mass^6)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b6.1 -228. 20.7 NA NA NA NA NA ## 2 b6.2 -2618. 127. -1.1 NA NA NA NA ## 3 b6.3 21990. -1474. 32.8 -0.2 NA NA NA ## 4 b6.4 322887. -27946. 892. -12.4 0.1 NA NA ## 5 b6.5 -1535342. 180049 -8325. 190. -2.1 0 NA ## 6 b6.6 10849891. -1473228. 82777 -2463. 40.9 -0.4 0 For Figure 6.3, we’ll make each plot individually and them glue them together with gridExtra::grid.arrange(). Since they all share a common stucture, we’ll start by specifying a base plot which we’ll save as p. p &lt;- d %&gt;% ggplot(aes(x = mass, y = brain)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_x_continuous(&quot;body mass (kg)&quot;, limits = c(33, 62), expand = c(0, 0)) + coord_cartesian(ylim = c(300, 1500)) + ylab(&quot;brain volume (cc)&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Now for each subplot, we’ll tack the subplot-specific components onto p. The main action is in stat_smooth(). For each subplot, the first three lines in stat_smooth() are identical, with only the bottom formula line differing. Like McElreath did in the text, we also adjust the y-axis range for the last two plots. # linear p1 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, # note our rare use of 89% intervals color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ x) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .49&quot;))) # quadratic p2 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 2)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .54&quot;))) # cubic p3 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 3)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .68&quot;))) # fourth-order polynomial p4 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 4)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .81&quot;))) # fifth-order polynomial p5 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 5)) + # we&#39;re adjusting the y-axis range for this plot (and the next) coord_cartesian(ylim = c(150, 1900)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .99&quot;))) # sixth-order polynomial p6 &lt;- p + # mark off 0 on the y-axis geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 6)) + coord_cartesian(ylim = c(-300, 1500)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = 1&quot;))) Okay, now we’re ready to combine the six subplots and produce our version of Figure 6.3. library(gridExtra) grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2) 6.1.2 Too few parameters hurts, too. Fit the intercept only model, b6.7. b6.7 &lt;- lm(data = d, brain ~ 1) summary(b6.7) ## ## Call: ## lm(formula = brain ~ 1, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -275.71 -227.21 -101.71 97.79 636.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 713.7 121.8 5.86 0.00109 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 322.2 on 6 degrees of freedom With the intercept-only model, we didn’t even get an \\(R^2\\) value in the summary.broom::glance() offers a quick way to get one. glance(b6.7) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 0 322. NA NA 1 -49.8 104. 104. 623061. 6 Zero. Our intercept-only b6.7 explained exactly zero variance in brain. All it did was tell us what the unconditional mean and variance (i.e., ‘Residual standard error’) were. I hope that makes sense. They were the only things in the model: \\(\\text{brain}_i \\sim \\text{Normal}(\\mu = \\alpha, \\sigma)\\). To get the intercept-only model for Figure 6.4, we plug formula = y ~ 1 into the stat_smooth() function. p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ 1) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = 0&quot;))) 6.1.2.1 Overthinking: Dropping rows. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of d like this. d %&gt;% filter(row_number() != 2) ## # A tibble: 6 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 habilis 612 34.5 ## 3 boisei 521 41.5 ## 4 rudolfensis 752 55.5 ## 5 ergaster 871 61 ## 6 sapiens 1350 53.5 We can then extend that logic into a custom function, make_lines(), that will drop a row from d, fit the simple model brain ~ mass, and then use base R predict() to return the model-implied trajectory over new data values. # because these lines are straight, we only need new data over two points of `mass` nd &lt;- tibble(mass = c(30, 70)) make_lines &lt;- function(row){ my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass) predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } Here we’ll make a tibble, lines, which will specify rows 1 through 7 in the row column. We’ll then feed those row numbers into our custom make_lines() function, which will return the predicted values and their corresponding mass values, per model. ( lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) ) ## # A tibble: 14 x 3 ## row brain mass ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 436. 30 ## 2 1 1201. 70 ## 3 2 421. 30 ## 4 2 1205. 70 ## 5 3 323. 30 ## 6 3 1264. 70 ## 7 4 423. 30 ## 8 4 1221. 70 ## 9 5 376. 30 ## 10 5 1335. 70 ## 11 6 332. 30 ## 12 6 1433. 70 ## 13 7 412. 30 ## 14 7 964. 70 Now we’re ready to plot the left panel of Figure 6.5. p + scale_x_continuous(expand = c(0, 0)) + geom_line(data = lines, aes(x = mass, y = brain, group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) To make the right panel for Figure 6.5, we’ll need to increase the number of mass points in our nd data and redefine the make_lines() function to fit the sixth-order-polynomial model. # because these lines will be very curvy, we&#39;ll need new data over many points of `mass` nd &lt;- tibble(mass = seq(from = 30, to = 65, length.out = 200)) # redifine the function make_lines &lt;- function(row){ my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)) predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } # make our new tibble lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) # plot! p + geom_line(data = lines, aes(group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) + coord_cartesian(ylim = -300:2000) 6.2 Information theory and model performance Whether you end up using regularization or information criteria or both, the first thing you must do is pick a criterion of model performance. What do you want the model to do well at? We’ll call this criterion the target, and in this section you’ll see how information theory provides a common and useful target, the out-of-sample deviance. (p. 174, emphasis in the original) 6.2.1 Firing the weatherperson. If you let rain = 1 and sun = 0, here’s a way to make a plot of the first table of page 175, the weatherperson’s predictions. weatherperson &lt;- tibble(day = 1:10, prediction = rep(c(1, 0.6), times = c(3, 7)), observed = rep(c(1, 0), times = c(3, 7))) weatherperson %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;)) Here’s how the newcomer fared: newcomer &lt;- tibble(day = 1:10, prediction = 0, observed = rep(c(1, 0), times = c(3, 7))) newcomer %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;)) If we do the math entailed in the tibbles, we’ll see why the newcomer could boast “I’m the best person for the job” (p. 175). weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person) %&gt;% summarise(hit_rate = mean(hit)) ## # A tibble: 2 x 2 ## person hit_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer 0.7 ## 2 weatherperson 0.58 6.2.1.1 Costs and benefits. Our new points variable doesn’t fit into the nice color-based geom_tile() plots from above. But we can still do the math. weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), points = ifelse(observed == 1 &amp; prediction != 1, -5, ifelse(observed == 1 &amp; prediction == 1, -1, -1 * prediction))) %&gt;% group_by(person) %&gt;% summarise(happiness = sum(points)) ## # A tibble: 2 x 2 ## person happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer -15 ## 2 weatherperson -7.2 6.2.1.2 Measuring accuracy. weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n() / 2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person, hit) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(power = hit ^ n, term = rep(letters[1:2], times = 2)) %&gt;% select(person, term, power) %&gt;% spread(key = term, value = power) %&gt;% mutate(probability_correct_sequence = a * b) ## # A tibble: 2 x 4 ## person a b probability_correct_sequence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 newcomer 0 1 0 ## 2 weatherperson 0.00164 1 0.00164 6.2.2 Information and uncertainty. The formula for information entropy is: \\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i)\\] McElreath put it in words as “the uncertainty contained in a probability distribution is the average log-probability of the event” (p. 178). We’ll compute the information entropy for weather at the first unnamed location, which we’ll call McElreath's house, and Abu Dhabi at once. tibble(place = c(&quot;McElreath&#39;s house&quot;, &quot;Abu Dhabi&quot;), p_rain = c(.3, .01)) %&gt;% mutate(p_shine = 1 - p_rain) %&gt;% group_by(place) %&gt;% mutate(H_p = (p_rain * log(p_rain) + p_shine * log(p_shine)) %&gt;% mean() * -1) ## # A tibble: 2 x 4 ## # Groups: place [2] ## place p_rain p_shine H_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 McElreath&#39;s house 0.3 0.7 0.611 ## 2 Abu Dhabi 0.01 0.99 0.0560 The uncertainty is less in Abu Dhabi because it rarely rains, there. If you have sun, rain and snow, the entropy for weather is: p &lt;- c(.7, .15, .15) -sum(p * log(p)) ## [1] 0.8188085 6.2.3 From entropy to accuracy. The formula for the Kullback-Leibler divergence (i.e., K-L divergence) is \\[D_{\\text{KL}} (p, q) = \\sum_i p_i \\big ( \\text{log} (p_i) - \\text{log} (q_i) \\big ) = \\sum_i p_i \\text{log} \\Bigg ( \\frac{p_i}{q_i} \\Bigg )\\] which, in plainer language, is what McElreath described as “the average difference in log probability between the target (p) and model (q)” (p. 179). In McElreath’s example \\(p_1 = .3\\) \\(p_2 = .7\\) \\(q_1 = .25\\) \\(q_2 = .75\\) With those values, we can compute \\(D_{\\text{KL}} (p, q)\\) within a tibble like so: tibble(p_1 = .3, p_2 = .7, q_1 = .25, q_2 = .75) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.25 0.75 0.00640 Our systems in this section are binary (e.g., \\(q = \\lbrace q_i, q_2 \\rbrace\\)). Thus if you know \\(q_1 = .3\\) you know of a necessity \\(q_2 = 1 - q_1\\). Therefore we can code the tibble for the next example of when \\(p = q\\) like this: tibble(p_1 = .3) %&gt;% mutate(p_2 = 1 - p_1, q_1 = p_1) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.3 0.7 0 Building off of that, you can make the data required for Figure 6.6 like this. t &lt;- tibble(p_1 = .3, p_2 = .7, q_1 = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) head(t) ## # A tibble: 6 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.01 0.99 0.778 ## 2 0.3 0.7 0.02 0.98 0.577 ## 3 0.3 0.7 0.03 0.97 0.462 ## 4 0.3 0.7 0.04 0.96 0.383 ## 5 0.3 0.7 0.05 0.95 0.324 ## 6 0.3 0.7 0.06 0.94 0.276 Now we have the data, plotting Figure 6.6 is a just geom_line() with stylistic flourishes. t %&gt;% ggplot(aes(x = q_1, y = d_kl)) + geom_vline(xintercept = .3, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1.5) + annotate(geom = &quot;text&quot;, x = .4, y = 1.5, label = &quot;q = p&quot;, color = carto_pal(7, &quot;BurgYl&quot;)[5], family = &quot;Courier&quot;, size = 3.5) + labs(x = &quot;q[1]&quot;, y = &quot;Divergence of q from p&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.2.3.1 Rethinking: Divergence depends upon direction. Here we see \\(H(p, q) \\neq H(q, p)\\). That is, direction matters. tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 2 x 6 ## direction p_1 q_1 p_2 q_2 d_kl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 ## 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 The \\(D_{\\text{KL}}\\) was double when applying Martian estimates to Terran estimates. 6.2.4 From divergence to deviance. The point of all the preceding material about information theory and divergence is to establish both: How to measure the distance of a model from our target. Information theory gives us the distance measure we need, the K-L divergence. How to estimate the divergence. Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks. (p. 181) Now we’ll start working on item #2. We define deviance as: \\[D(q) = -2 \\sum_i \\text{log}(p_i)\\] In the formula, \\(i\\) indexes each case and \\(q_i\\) is the likelihood for each case. Here’s the deviance from model b6.1. lm(data = d, brain ~ mass) %&gt;% logLik() * -2 ## &#39;log Lik.&#39; 94.92499 (df=3) 6.2.4.1 Overthinking: Computing deviance. To follow along with the text, we’ll need to standardize mass before we compute deviance. d &lt;- d %&gt;% mutate(mass_s = (mass - mean(mass)) / sd(mass)) Open brms. library(brms) Now we’ll specify the initial values and fit the model. # Here we specify our starting values inits &lt;- list(Intercept = mean(d$brain), mass_s = 0, sigma = sd(d$brain)) inits_list &lt;- list(inits, inits, inits, inits) # The model b6.8 &lt;- brm(data = d, family = gaussian, brain ~ 1 + mass_s, prior = c(prior(normal(0, 1000), class = Intercept), prior(normal(0, 1000), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, # here we insert our start values seed = 6) print(b6.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: brain ~ 1 + mass_s ## Data: d (Number of observations: 7) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 706.81 105.11 483.15 915.58 2422 1.00 ## mass_s 220.85 111.88 -4.61 447.78 2257 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 264.36 95.97 149.69 501.95 1518 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Details about inits: You don’t have to specify your inits lists outside of the brm() function the way we did, here. This is just how I currently prefer. When you specify start values for the parameters in your Stan models, you need to do so with a list of lists. You need as many lists as HMC chains–four in this example. And then you put your–in this case–four lists inside a list. Lists within lists. Also, we were lazy and specified the same start values across all our chains. You can mix them up across chains if you want. Anyway, the brms function log_lik() returns a matrix. Each occasion gets a column and each HMC chain iteration gets a row. To make it easier to understand the output, we’ll name the columns by species using the .name_repair argument within the as_tibble() function. ll &lt;- b6.8 %&gt;% log_lik() %&gt;% as_tibble(.name_repair = ~ d$species) ll %&gt;% glimpse() ## Observations: 4,000 ## Variables: 7 ## $ afarensis &lt;dbl&gt; -6.398538, -6.398538, -6.474711, -6.644044, -6.547919, -6.656966, -6.482902, … ## $ africanus &lt;dbl&gt; -6.041552, -6.041552, -6.422862, -6.553371, -6.515666, -6.742793, -6.533691, … ## $ habilis &lt;dbl&gt; -5.883949, -5.883949, -6.579271, -6.400874, -6.752435, -7.317240, -7.047530, … ## $ boisei &lt;dbl&gt; -6.320301, -6.320301, -6.502303, -6.575147, -6.632262, -6.623189, -6.473505, … ## $ rudolfensis &lt;dbl&gt; -6.357385, -6.357385, -6.698074, -6.469439, -7.353899, -6.584505, -6.478985, … ## $ ergaster &lt;dbl&gt; -6.108233, -6.108233, -6.707478, -6.408949, -7.653877, -6.592203, -6.479748, … ## $ sapiens &lt;dbl&gt; -15.045611, -15.045611, -8.197374, -8.866857, -7.191201, -8.893135, -9.199886… Deviance is the sum of the occasion-level LLs multiplied by -2. Why by -2? “The -2 in front doesn’t do anything important. It’s there for historical reasons” (p. 182). If you follow footnote 93 at the end of that sentence in the text, you’ll learn “under somewhat general conditions, for many common model types, a difference between two deviances has a chi-squared distribution. The factor of 2 is there to scale it that way” (p. 451). ll &lt;- ll %&gt;% mutate(sums = rowSums(.), deviance = -2 * sums) Because we used HMC, deviance is a distribution rather than a single number. library(tidybayes) ll %&gt;% ggplot(aes(x = deviance, y = 0)) + geom_halfeyeh(fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7], point_interval = median_qi, .width = .95) + scale_x_continuous(breaks = quantile(ll$deviance, c(.025, .5, .975)), labels = quantile(ll$deviance, c(.025, .5, .975)) %&gt;% round(1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The deviance distribution&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) But notice our deviance distribution was centered right around the sole value McElreath reported in the text. 6.2.5 From deviance to out-of-sample. Deviance is a principled way to measure distance from the target. But deviance as computed in the previous section has the same flaw as \\(R^2\\): It always improves as the model gets more complex, at least for the types of models we have considered so far. Just like \\(R^2\\), deviance in-sample is a measure of retrodictive accuracy, not predictive accuracy. In the next subsection, we’ll see this in a simulation. 6.2.5.1 Overthinking: Simulated training and testing. I find the rethinking::sim.train.test() function opaque. If you’re curious, you can find McElreath’s code here. Let’s simulate and see what happens. library(rethinking) n &lt;- 20 kseq &lt;- 1:5 # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 n_cores &lt;- 4 # here&#39;s our dev object based on `N &lt;- 20` dev_20 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) # here&#39;s our dev object based on N &lt;- 100 n &lt;- 100 dev_100 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) If you didn’t quite catch it, the simulation yields dev_20 and dev_100. We’ll want to convert them to tibbles, bind them together, and wrangle extensively before we’re ready to plot. dev_tibble &lt;- dev_20 %&gt;% as_tibble() %&gt;% bind_rows( dev_100 %&gt;% as_tibble() ) %&gt;% mutate(n = rep(c(&quot;n = 20&quot;, &quot;n = 100&quot;), each = 4), statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 2)) %&gt;% gather(n_par, value, -n, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .075, n_par + .075)) head(dev_tibble) ## # A tibble: 6 x 5 ## n sample n_par mean sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 n = 100 in 0.925 283. 14.1 ## 2 n = 100 in 1.92 279. 13.9 ## 3 n = 100 in 2.92 263. 11.1 ## 4 n = 100 in 3.92 263. 11.2 ## 5 n = 100 in 4.92 262. 11.2 ## 6 n = 100 out 1.08 285. 14.4 Now we’re ready to make Figure 6.7. # this intermediary tibble will make `geom_text()` easier dev_text &lt;- dev_tibble %&gt;% filter(n_par &gt; 1.5, n_par &lt; 2.5) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .2, n_par + .28)) # the plot dev_tibble %&gt;% ggplot(aes(x = n_par, y = mean, ymin = mean - sd, ymax = mean + sd, group = sample, color = sample, fill = sample)) + geom_pointrange(shape = 21) + geom_text(data = dev_text, aes(label = sample)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[5], carto_pal(7, &quot;BurgYl&quot;)[7])) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~n, scale = &quot;free_y&quot;) Even with a substantially smaller \\(N\\), our simulation results matched up well with those in the text. 6.3 Regularization The root of overfitting is a model’s tendency to get overexcited by the training sample… One way to prevent a model from getting too excited by the training sample is to give it a skeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample. (p. 186) In case you were curious, here’s how you might do Figure 6.8 with ggplot2. All the action is in the geom_ribbon() portions. tibble(x = seq(from = - 3.5, to = 3.5, by = .01)) %&gt;% ggplot(aes(x = x, ymin = 0)) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 0.2)), fill = carto_pal(7, &quot;BurgYl&quot;)[7], alpha = 1/2) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 0.5)), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 1)), fill = carto_pal(7, &quot;BurgYl&quot;)[5], alpha = 1/2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;parameter value&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) In our version of the plot, darker purple = more regularizing. But to prepare for Figure 6.9, let’s simulate. This time we’ll wrap the basic simulation code we used before into a function we’ll call make_sim(). Our make_sim() function has two parameters, N and b_sigma, both of which come from McElreath’s simulation code. So you’ll note that instead of hard coding the values for N and b_sigma within the simulation, we’re leaving them adjustable (i.e., sim.train.test(N = n, k = k, b_sigma = b_sigma)). Also notice that instead of saving the simulation results as objects, like before, we’re just converting them to tibbles with the as_tibble() function at the bottom. Our goal is to use make_sim() within a purrr::map2() statement. The result will be a nested tibble into which we’ve saved the results of 6 simulations based off of two sample sizes (i.e., n = c(20, 100)) and three values of \\(\\sigma\\) for our Gaussian \\(\\beta\\) prior (i.e., b_sigma = c(1, .5, .2)). library(rethinking) # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 make_sim &lt;- function(n, b_sigma){ sapply(kseq, function(k) { print(k); # this is an augmented line of code r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k, b_sigma = b_sigma), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) %&gt;% # this is a new line of code as_tibble() } s &lt;- tibble(n = rep(c(20, 100), each = 3), b_sigma = rep(c(1, .5, .2), times = 2)) %&gt;% mutate(sim = map2(n, b_sigma, make_sim)) %&gt;% unnest() We’ll follow the same principles for wrangling these data as we did those from the previous simulation, dev_tibble. And after wrangling, we’ll feed the data directly into the code for our version of Figure 6.9. # wrangle the simulation data s %&gt;% mutate(statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 3 * 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 3 * 2)) %&gt;% gather(n_par, value, -n, -b_sigma, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(n = str_c(&quot;n = &quot;, n) %&gt;% factor(., levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% # now plot ggplot(aes(x = n_par, y = mean, group = interaction(sample, b_sigma))) + geom_line(aes(color = sample, size = b_sigma %&gt;% as.character())) + # this function contains the data from the previous simulation geom_point(data = dev_tibble, aes(group = sample, fill = sample), color = &quot;black&quot;, shape = 21, size = 2.5, stroke = .1) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_size_manual(values = c(1, .5, .2)) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~n, scale = &quot;free_y&quot;) Our results don’t perfectly align with those in the text. I suspect his is because we used 1e3 iterations, rather than the 1e4 of the text. If you’d like to wait all night long for the simulation to yield more stable results, be my guest. Regularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. So to use them effectively, you need some way to tune them. Tuning them isn’t always easy. (p. 187) For more on this how to choose your priors, consider Gelman, Simpson, and Betancourt’s The prior can generally only be understood in the context of the likelihood, a paper that will probably make more sense after Chapter 8. And if you’re feeling feisty, also check out Simpson’s related blog post (It’s never a) Total Eclipse of the Prior. 6.3.0.1 Rethinking: Ridge regression. Within the brms framework, you can do something like this with the horseshoe prior via the horseshoe() function. You can learn all about it from the horseshoe section of the brms reference manual (version 2.8.0). Here’s an extract from the section: The horseshoe prior is a special shrinkage prior initially proposed by Carvalho et al. (2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is non- zero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(&quot;horseshoe(1)&quot;). (p. 70) And to dive even deeper into the horseshoe prior, check out Michael Betancourt’s tutorial, Bayes Sparse Regression. 6.4 Information criteria The data from our initial simulation isn’t formatted well to plot Figure 6.10. We’ll have to wrangle a little. ( dev_tibble &lt;- dev_tibble %&gt;% select(-sd) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par + .075, n_par - .075)) %&gt;% spread(key = sample, value = mean) %&gt;% mutate(height = (out - `in`) %&gt;% round(digits = 1) %&gt;% as.character(), dash = `in` + 2 * n_par) ) ## # A tibble: 10 x 6 ## n n_par `in` out height dash ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 n = 20 1 55.6 57.4 1.8 57.6 ## 2 n = 20 2 54.4 58.1 3.7 58.4 ## 3 n = 20 3 50.6 55.9 5.3 56.6 ## 4 n = 20 4 49.5 57.7 8.1 57.5 ## 5 n = 20 5 49.0 58.8 9.8 59.0 ## 6 n = 100 1 283. 285. 2.6 285. ## 7 n = 100 2 279. 283. 3.6 283. ## 8 n = 100 3 263. 268. 5.2 269. ## 9 n = 100 4 263. 269. 6 271. ## 10 n = 100 5 262. 270. 8 272. Now we’re ready to plot. dev_tibble %&gt;% ggplot(aes(x = n_par)) + geom_line(aes(y = dash), linetype = 2, color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_point(aes(y = `in`), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 2) + geom_point(aes(y = out), color = carto_pal(7, &quot;BurgYl&quot;)[5], size = 2) + geom_errorbar(aes(x = n_par + .15, ymin = `in`, ymax = out), width = .1, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_text(aes(x = n_par + .4, y = (out + `in`) / 2, label = height), family = &quot;Courier&quot;, size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~n, scale = &quot;free_y&quot;) Again, our numbers aren’t the exact same as McElreath’s because a) this is a simulation and b) our number of simulations was an order of magnitude smaller than his. But the overall pattern is the same. More to the point, the distances between the in- and out-of-sample points are nearly the same, for each model, at both \\(N = 20\\) (left) and \\(N = 100\\) (right). Each distance is nearly twice the number of parameters, as labeled on the horizontal axis. The dashed lines show exactly the [dark purple] points plus twice the number of parameters, tracing closely along the average out-of-sample deviance for each model. This is the phenomenon behind information criteria. (p. 189) In the text, McElreach focused on the DIC and WAIC. As you’ll see, the LOO has increased in popularity since he published the text. Going forward, we’ll juggle the WAIC and the LOO in this project. But we will respect the text and work in a little DIC talk. 6.4.1 DIC. The DIC has been widely used for some time, now. For a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. If we define \\(D\\) as the deviance’s posterior distribution, \\(\\bar{D}\\) as its mean and \\(\\hat{D}\\) as the deviance when computed at the posterior mean, then we define the DIC as \\[\\text{DIC} = \\bar{D} + (\\bar{D} + \\hat{D}) + \\bar{D} + p_D\\] And \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, I’m not aware of a way to that brms or the loo package–to be introduced shortly–offer convenience functions that yield the DIC. 6.4.2 WAIC. It’s okay that the brms and loo packages don’t yield the DIC because even better than the DIC is the Widely Applicable Information Criterion (WAIC)… Define \\(\\text{Pr} (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters sampled from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, lppd: \\[\\text{lppd} = \\sum_{i = 1}^N \\text{log Pr} (y_i)\\] You might say this out loud as: The log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation. … The second piece of WAIC is the effect number of parameters \\(p_{\\text{WAIC}}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood for observation \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_{\\text{WAIC}}\\) is defined as: \\[p_{\\text{WAIC}} = \\sum_{i=1}^N V (y_i)\\] Now WAIC is defined as: \\[\\text{WAIC} = -2 (\\text{lppd} - p_{\\text{WAIC}})\\] And this value is yet another estimate of out-of-sample deviance. (pp. 191–192) You’ll see how to compute the WAIC in brms in just a bit. 6.4.2.1 Overthinking: WAIC calculation. Here is how to fit the pre-WAIC model in brms. data(cars) b &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 30), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6) Here’s the summary. print(b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: dist ~ 1 + speed ## Data: cars (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -17.38 6.85 -30.41 -3.76 2259 1.00 ## speed 3.92 0.42 3.12 4.74 2053 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 15.85 1.69 12.91 19.68 2206 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In brms, you return the loglikelihood with log_lik(). ll &lt;- b %&gt;% log_lik() %&gt;% as_tibble() Computing the lppd, the “Bayesian deviance”, takes a bit of leg work. dfmean &lt;- ll %&gt;% exp() %&gt;% summarise_all(mean) %&gt;% gather(key, means) %&gt;% select(means) %&gt;% log() ( lppd &lt;- dfmean %&gt;% sum() ) ## [1] -206.6801 Comupting the effective number of parameters, \\(p_{\\text{WAIC}}\\), isn’t much better. dfvar &lt;- ll %&gt;% summarise_all(var) %&gt;% gather(key, vars) %&gt;% select(vars) pwaic &lt;- dfvar %&gt;% sum() pwaic ## [1] 3.220411 Finally, here’s what we’ve been working so hard for: our hand calculated WAIC value. Compare it to the value returned by the brms waic() function. -2 * (lppd - pwaic) ## [1] 419.8009 waic(b) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -209.9 6.3 ## p_waic 3.2 1.2 ## waic 419.8 12.6 Before we move on, did you notice the elpd_waic row in the tibble waic() returned? That value is the lppd minus the pwaic, but without multiplying the result by -2. E.g., lppd - pwaic ## [1] -209.9005 That tidbit will come in handy a little bit later. But for now, here’s how we compute the WAIC standard error. dfmean %&gt;% mutate(waic_vec = -2 * (means - dfvar$vars)) %&gt;% summarise(waic_se = (var(waic_vec) * nrow(dfmean)) %&gt;% sqrt()) ## # A tibble: 1 x 1 ## waic_se ## &lt;dbl&gt; ## 1 12.6 6.4.3 DIC and WAIC as estimates of deviance. Once again, we’ll wrap McElreath’s sim.train.test()-based simulation code within a custom function, make_sim(). This time we’ve adjusted make_sim() to take one argument, b_sigma. We will then feed that value into the same-named argument within sim.train.test(). Also notice that within sim.train.test(), we’ve specified TRUE for the information criteria and deviance arguments. Be warned: it takes extra time to compute the WAIC. Because we do that for every model, this simulation takes longer than the previous ones. To get a taste, try running it with something like n_sim &lt;- 5 first. n_sim &lt;- 1e3 make_sim &lt;- function(b_sigma){ sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = 20, k = k, b_sigma = b_sigma, DIC = T, WAIC = T, devbar = T, devbarout = T), mc.cores = n_cores); c(dev_in = mean(r[1, ]), dev_out = mean(r[2, ]), DIC = mean(r[3, ]), WAIC = mean(r[4, ]), devbar = mean(r[5, ]), devbarout = mean(r[6, ])) } ) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% rename(statistic = rowname) } s &lt;- tibble(b_sigma = c(100, .5)) %&gt;% mutate(sim = purrr::map(b_sigma, make_sim)) %&gt;% unnest() Here we wrangle and plot. s %&gt;% gather(n_par, value, -b_sigma, -statistic) %&gt;% mutate(n_par = str_remove(n_par, &quot;X&quot;) %&gt;% as.double()) %&gt;% filter(statistic != &quot;devbar&quot; &amp; statistic != &quot;devbarout&quot;) %&gt;% spread(key = statistic, value = value) %&gt;% gather(ic, value, -b_sigma, -n_par, -dev_in, -dev_out) %&gt;% gather(sample, deviance, -b_sigma, -n_par, -ic, -value) %&gt;% filter(sample == &quot;dev_out&quot;) %&gt;% mutate(b_sigma = b_sigma %&gt;% as.character()) %&gt;% ggplot(aes(x = n_par)) + geom_point(aes(y = deviance, color = b_sigma), size = 2.5) + geom_line(aes(y = value, group = b_sigma, color = b_sigma)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + # scale_color_manual(values = c(&quot;steelblue&quot;, &quot;black&quot;)) + labs(subtitle = &quot;n = 20&quot;, x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), legend.position = &quot;none&quot;) + facet_wrap(~ic, ncol = 1) And again, our results don’t perfectly match those in the text because a) we’re simulating and b) we used fewer iterations than McElreath did. But the overall pattern remains. 6.5 Using information criteria In contrast to model selection, “this section provides a brief example of model comparison and averaging” (p. 195, emphasis in the original). 6.5.1 Model comparison. Load the milk data from earlier in the text. library(rethinking) data(milk) d &lt;- milk %&gt;% drop_na(ends_with(&quot;_s&quot;)) rm(milk) d &lt;- d %&gt;% mutate(neocortex = neocortex.perc / 100) The dimensions of d are: dim(d) ## [1] 17 9 Load brms. detach(package:rethinking, unload = T) library(brms) We’re ready to fit the competing kcal.per.g models. Note our use of update() in the last two models. inits &lt;- list(Intercept = mean(d$kcal.per.g), sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.11 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, seed = 6) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.12 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(-1000, 1000), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, seed = 6) inits &lt;- list(Intercept = mean(d$kcal.per.g), `log(mass)` = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.13 &lt;- update(b6.12, newdata = d, formula = kcal.per.g ~ 1 + log(mass), inits = inits_list) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, `log(mass)` = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.14 &lt;- update(b6.13, newdata = d, formula = kcal.per.g ~ 1 + neocortex + log(mass), inits = inits_list) 6.5.1.1 Comparing WAIC values. In brms, you can get a model’s WAIC value with the waic() function. waic(b6.14) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 8.3 2.6 ## p_waic 3.2 0.9 ## waic -16.6 5.2 ## Warning: 2 (11.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. Note the warning message. Statisticians have made notable advances in Bayesian information criteria since McElreath published Statistical Rethinking. I won’t go into detail here, but the “We recommend trying loo instead” part of the message is designed to prompt us to use a different information criteria, the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; aka, the LOO). In brms this is available with the loo() function, which you can learn more about in this vignette from the makers of the loo package. For now, back to the WAIC. There are a few ways to approach information criteria within the brms framework. If all you want are the quick results for a model, just plug the name of your brm() fit object into the waic() function. waic(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 4.4 1.9 ## p_waic 1.3 0.3 ## waic -8.8 3.7 The WAIC and its standard error are on the bottom row. The \\(p_\\text{WAIC}\\) and its SE are stacked atop that. And look there on the top row. Remember how we pointed out, above, that we get the WAIC by multiplying (lppd - pwaic) by -2? Well, if you just do the subtraction without multiplying the result by -2, you get the elpd_waic. File that away. It’ll become important in a bit. Following the version 2.8.0 update, part of the suggested workflow for using information criteria with brms (i.e., execute ?loo.brmsfit) is to add the estimates to the brm() fit object itself. You do that with the add_criterion() function. Here’s how we’d do so with b6.11. b6.11 &lt;- add_criterion(b6.11, &quot;waic&quot;) With that in place, here’s how you’d extract the WAIC information from the fit object. b6.11$waic ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 4.4 1.9 ## p_waic 1.3 0.3 ## waic -8.8 3.7 Why would I go through all that trouble?, you might ask. Well, two reasons. First, now your WAIC information is saved with all the rest of your fit output, which can be convenient. But second, it sets you up to use the loo_compare() function to compare models by their information criteria. To get a sense of that workflow, here we use add_criterion() for the next three models. Then we’ll use loo_compare(). # compute and save the WAIC information for the next three models b6.12 &lt;- add_criterion(b6.12, &quot;waic&quot;) b6.13 &lt;- add_criterion(b6.13, &quot;waic&quot;) b6.14 &lt;- add_criterion(b6.14, &quot;waic&quot;) # compare the WAIC estimates w &lt;- loo_compare(b6.11, b6.12, b6.13, b6.14, criterion = &quot;waic&quot;) print(w) ## elpd_diff se_diff ## b6.14 0.0 0.0 ## b6.13 -3.8 1.8 ## b6.11 -3.9 2.5 ## b6.12 -4.7 2.5 You don’t have to save those results as an object like we just did with w. But that’ll serve some pedagogical purposes in just a bit. So go with it. With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences. The models have been rank ordered from the lowest (i.e., b6.14) to the highest (i.e., b6.12). The scores listed are the differences of b6.14 minus the comparison model. Since b6.14 is the comparison model in the top row, the values are naturally 0 (i.e., \\(x - x = 0\\)). But now here’s another critical thing to understand: Since the brms version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(-2 * x\\) metric. Remember how we keep rehearsing that multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist chi-square test? We’ll, the makers of the loo package aren’t fans and they no longer support the conversion. So here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the proper metric, we’ll need to multiply the se_diff column by 2. You wouldn’t multiply by -2 because that would return a negative standard error, which would be silly. Here’s a quick way to do so. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b6.14 0.000000 0.000000 ## b6.13 7.552324 3.542171 ## b6.11 7.875847 4.942273 ## b6.12 9.354846 5.081992 One more thing. On page 198, and on many other pages to follow in the text, McElreath used the rethinking::compare() function to return a rich table of information about the WAIC information for several models. If we’re tricky, we can do something similar with loo_compare. To learn how, let’s peer further into the structure of our w object. str(w) ## &#39;compare.loo&#39; num [1:4, 1:8] 0 -3.78 -3.94 -4.68 0 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:4] &quot;b6.14&quot; &quot;b6.13&quot; &quot;b6.11&quot; &quot;b6.12&quot; ## ..$ : chr [1:8] &quot;elpd_diff&quot; &quot;se_diff&quot; &quot;elpd_waic&quot; &quot;se_elpd_waic&quot; ... When we used print(w), a few code blocks earlier, it only returned two columns. It appears we actually have eight. We can see the full output with the simplify = F argument. print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b6.14 0.0 0.0 8.3 2.6 3.2 0.9 -16.6 5.2 ## b6.13 -3.8 1.8 4.5 2.1 2.0 0.4 -9.1 4.2 ## b6.11 -3.9 2.5 4.4 1.9 1.3 0.3 -8.8 3.7 ## b6.12 -4.7 2.5 3.6 1.6 1.9 0.3 -7.3 3.2 The results are quite analogous to those from rethinking::compare(). Again, the difference estimates are in the metric of the \\(\\text{elpd}\\). But the interpretation is the same and we can convert them to the traditional information criteria metric with simple multiplication. As we’ll see later, this basic workflow applies to the LOO, too. If you want to get those WAIC weights, you can use the brms::model_weights() function like so: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b6.11 b6.12 b6.13 b6.14 ## 0.02 0.01 0.02 0.95 That last round() line was just to limit the decimal-place precision. If you really wanted to go through the trouble, you could make yourself a little table like this: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% as_tibble() %&gt;% rename(weight = value) %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), weight = weight %&gt;% round(digits = 2)) %&gt;% select(model, weight) %&gt;% arrange(desc(weight)) %&gt;% knitr::kable() model weight b6.14 0.95 b6.11 0.02 b6.13 0.02 b6.12 0.01 With a little [] subsetting and light wrangling, we can convert the contents of our w object to a format suitable for plotting the WAIC estimates. w[, 7:8] %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model_name&quot;) %&gt;% ggplot(aes(x = model_name, y = waic, ymin = waic - se_waic, ymax = waic + se_waic)) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;My custom WAIC plot&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) We briefly discussed the alternative information criteria, the LOO, above. Here’s how to use it in brms. loo(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_loo 4.4 1.9 ## p_loo 1.3 0.3 ## looic -8.7 3.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. The Pareto \\(k\\) values are a useful model fit diagnostic tool, which we’ll discuss later. But for now, realize that brms uses functions from the loo package to compute its WAIC and LOO values. In addition to the vignette, above, this vignette demonstrates the LOO with these very same examples from McElreath’s text. And if you’d like to dive a little deeper, check out Aki Vehtari’s GPSS2017 workshop or his talk from November 2018, Model assessment, selection and averaging. Let’s get back on track with the text. To put all this model comparison in perspective, in this analysis, the best model has more than 90% of the model weight. That’s pretty good. But with only 12 cases, the error on the WAIC estimate is substantial, and of course that uncertainty should propagate to the Akaike weights. So don’t get too excited. If we take the standard error of the difference from the [loo_compare()] table literally, you can think of the difference as a Gaussian distribution centered (for the difference between models [b6.14 and b6.11]) on [9.35] with a standard deviation of [5.08]. (p. 200) Here are those two values in the \\(\\text{elpd}\\) metric. w[4, 1:2] ## elpd_diff se_diff ## -4.677423 2.540996 And here we convert them to the WAIC metric. round(w[4, 1] * -2, 2) ## [1] 9.35 round(w[4, 2] * 2, 2) ## [1] 5.08 If it’s easier to see, here’s the same information in a tibble. tibble(value = c(&quot;difference&quot;, &quot;se&quot;), elpd = w[4, 1:2], conversion_factor = c(-2, 2)) %&gt;% mutate(waic = elpd * conversion_factor) ## # A tibble: 2 x 4 ## value elpd conversion_factor waic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 difference -4.68 -2 9.35 ## 2 se 2.54 2 5.08 Before we forget, McElreath gave some perspective difference between the models with the highest and lowest WAIC values (p. 200). But to the point, we can extract the two numerals and plug them into rnorm(). # how many draws would you like? n &lt;- 1e5 set.seed(6) # simulate diff &lt;- tibble(diff = rnorm(n, mean = w[4, 1] * -2, sd = w[4, 2] * 2)) diff %&gt;% summarise(the_probability_a_difference_is_negative = sum(diff &lt; 0) / n) ## # A tibble: 1 x 1 ## the_probability_a_difference_is_negative ## &lt;dbl&gt; ## 1 0.0329 In case you’re curious, this is a graphic version of what we just did. tibble(diff = -20:30) %&gt;% ggplot(aes(x = diff, ymin = 0)) + geom_ribbon(aes(ymax = dnorm(diff, w[4, 1] * -2, w[4, 2] * 2)), fill = carto_pal(7, &quot;BurgYl&quot;)[7]) + geom_ribbon(data = tibble(diff = -20:0), aes(ymax = dnorm(diff, w[4, 1] * -2, w[4, 2] * 2)), fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_vline(xintercept = 0, linetype = 3, color = carto_pal(7, &quot;BurgYl&quot;)[3]) + scale_y_continuous(NULL, breaks = NULL) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.5.1.2 Comparing estimates. The brms package doesn’t have anything like rethinking’s coeftab() function. However, one can get that information with a little ingenuity. Here we’ll employ the broom::tidy() function, which will save the summary statistics for our model parameters. For example, this is what it will produce for the full model, b6.14. tidy(b6.14) ## term estimate std.error lower upper ## 1 b_Intercept -1.07143269 0.59452616 -2.0314950 -0.12374226 ## 2 b_neocortex 2.77095114 0.92567188 1.2991209 4.25049529 ## 3 b_logmass -0.09588127 0.02841600 -0.1418985 -0.05050882 ## 4 sigma 0.13923140 0.03112164 0.1000525 0.19582557 ## 5 lp__ -19.19627019 1.67845099 -22.4100409 -17.31222656 Note, tidy() also grabs the log posterior (i.e., lp__), which we’ll exclude for our purposes. With a little purrr::map() code, you can save the brm() fits and their tidy() summaries into a nested tibble, and then unnest() the tibble for coeftab()-like use. my_coef_tab &lt;- tibble(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) head(my_coef_tab) ## # A tibble: 6 x 6 ## model term estimate std.error lower upper ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b6.11 b_Intercept 0.657 0.0454 0.583 0.730 ## 2 b6.11 sigma 0.188 0.0375 0.139 0.256 ## 3 b6.12 b_Intercept 0.352 0.558 -0.560 1.26 ## 4 b6.12 b_neocortex 0.449 0.822 -0.887 1.80 ## 5 b6.12 sigma 0.193 0.0397 0.140 0.266 ## 6 b6.13 b_Intercept 0.704 0.0574 0.608 0.797 Just a little more work and we’ll have a table analogous to the one McElreath produced with his coef_tab() function. my_coef_tab %&gt;% # learn more about `dplyr::complete()` here: https://rdrr.io/cran/tidyr/man/expand.html complete(term = distinct(., term), model) %&gt;% select(model, term, estimate) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) ## # A tibble: 4 x 5 ## term b6.11 b6.12 b6.13 b6.14 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 0.66 0.35 0.7 -1.07 ## 2 b_logmass NA NA -0.03 -0.1 ## 3 b_neocortex NA 0.45 NA 2.77 ## 4 sigma 0.19 0.19 0.18 0.14 I’m also not aware of an efficient way in brms to reproduce Figure 6.12 for which McElreath nested his coeftab() argument in a plot() argument. However, one can build something similar by hand with a little data wrangling. # data wrangling wrangled_my_coef_tab &lt;- my_coef_tab %&gt;% complete(term = distinct(., term), model) %&gt;% rbind( tibble( model = NA, term = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;), estimate = NA, std.error = NA, lower = NA, upper = NA)) %&gt;% mutate(axis = ifelse(is.na(model), term, model), model = factor(model, levels = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;)), term = factor(term, levels = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;, NA))) %&gt;% arrange(term, model) %&gt;% mutate(axis_order = letters[1:20], axis = ifelse(str_detect(axis, &quot;b6.&quot;), str_c(&quot; &quot;, axis), axis)) # plot ggplot(data = wrangled_my_coef_tab, aes(x = axis_order, y = estimate, ymin = lower, ymax = upper)) + theme_classic() + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + scale_x_discrete(NULL, labels = wrangled_my_coef_tab$axis) + ggtitle(&quot;My other coeftab() plot&quot;) + coord_flip() + theme(text = element_text(family = &quot;Courier&quot;), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) However, if you’re willing to deviate just a bit from the format of McElreath’s coeftab() plot, here’s a more elegant way to work with our my_coef_tab tibble. my_coef_tab %&gt;% ggplot(aes(x = model, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + labs(x = NULL, y = NULL) + coord_flip() + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), strip.background = element_rect(color = &quot;transparent&quot;)) + facet_wrap(~term, ncol = 1) 6.5.1.3 Rethinking: Barplots suck. Man, I agree. “The only problem with barplots is that they have bars” (p. 203). You can find alternatives here, here, here, here, and a whole bunch here. 6.5.2 Model averaging. Within the current brms framework, you can do model-averaged predictions with the pp_average() function. The default weighting scheme is with the LOO. Here we’ll use the weights = &quot;waic&quot; argument to match McElreath’s method in the text. Because pp_average() yields a matrix, we’ll want to convert it to a tibble before feeding it into ggplot2. # we need new data for both the `fitted()` and `pp_average()` functions nd &lt;- tibble(neocortex = seq(from = .5, to = .8, length.out = 30), mass = 4.5) # we&#39;ll get the `b6.14`-implied trajectory with `fitted()` f &lt;- fitted(b6.14, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # the model-average trajectory comes from `pp_average()` pp_average(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;, method = &quot;fitted&quot;, # for new data predictions, use `method = &quot;predict&quot;` newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% # plot Figure 6.13 ggplot(aes(x = neocortex, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/4) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_ribbon(data = f, aes(ymin = Q2.5, ymax = Q97.5), color = carto_pal(7, &quot;BurgYl&quot;)[5], fill = &quot;transparent&quot;, linetype = 2) + geom_line(data = f, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_point(data = d, aes(y = kcal.per.g), size = 2, color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(y = &quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(d$neocortex), ylim = range(d$kcal.per.g)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.6 Summary Bonus: \\(R^2\\) talk At the beginning of the chapter (pp. 167–168), McElreath briefly introduced \\(R^2\\) as a popular way to assess the variance explained in a model. He pooh-poohed it because of its tendency to overfit. It’s also limited in that it doesn’t generalize well outside of the single-level Gaussian framework. However, if you should find yourself in a situation where \\(R^2\\) suits your purposes, the brms bayes_R2() function might be of use. Simply feeding a model brm fit object into bayes_R2() will return the posterior mean, \\(SD\\), and 95% intervals. For example: bayes_R2(b6.14) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.496 0.133 0.167 0.665 With just a little data processing, you can get a tibble table of each of models’ \\(R^2\\) ‘Estimate’. rbind(bayes_R2(b6.11), bayes_R2(b6.12), bayes_R2(b6.13), bayes_R2(b6.14)) %&gt;% as_tibble() %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), r_square_posterior_mean = round(Estimate, digits = 2)) %&gt;% select(model, r_square_posterior_mean) ## # A tibble: 4 x 2 ## model r_square_posterior_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 b6.11 0 ## 2 b6.12 0.07 ## 3 b6.13 0.15 ## 4 b6.14 0.5 If you want the full distribution of the \\(R^2\\), you’ll need to add a summary = F argument. Note how this returns a numeric vector. r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) r2_b6.13 %&gt;% glimpse() ## num [1:4000, 1] 0.01759 0.05596 0.28723 0.00866 0.32772 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; If you want to use these in ggplot2, you’ll need to put them in tibbles or data frames. Here we do so for two of our model fits. # model `b6.13` r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_13 = R2) # model `b6.14` r2_b6.14 &lt;- bayes_R2(b6.14, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_14 = R2) # let&#39;s put them in the same data object r2_combined &lt;- bind_cols(r2_b6.13, r2_b6.14) %&gt;% mutate(dif = r2_14 - r2_13) # plot their densities r2_combined %&gt;% ggplot() + geom_density(aes(x = r2_13), fill = carto_pal(7, &quot;BurgYl&quot;)[4], alpha = 3/4, size = 0, ) + geom_density(aes(x = r2_14), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 3/4, size = 0, ) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + labs(x = NULL, title = expression(paste(italic(&quot;R&quot;)^{2}, &quot; distributions&quot;)), subtitle = &quot;Going from left to right, these are\\nfor models b6.13 and b6.14.&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If you do your work in a field where folks use \\(R^2\\) change, you might do that with a simple difference score, which we computed above with mutate(dif = R2.14 - R2.13). Here’s the \\(\\Delta R^2\\) (i.e., dif) plot: r2_combined %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7], point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(Delta, italic(&quot;R&quot;)^{2})), subtitle = &quot;This is how much more variance, in\\nterms of %, model b6.14 explained\\ncompared to model b6.13.&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) The brms package did not get these \\(R^2\\) values by traditional method used in, say, ordinary least squares estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out the paper by Gelman, Goodrich, Gabry, and Vehtari. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 tidybayes_1.1.0 brms_2.9.0 ## [6] Rcpp_1.0.1 gridExtra_2.3 broom_0.5.2 ggrepel_0.8.1 rcartocolor_2.0.0 ## [11] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 readr_1.3.1 ## [16] tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 bridgesampling_0.6-0 ## [16] codetools_0.2-16 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 ## [22] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 ## [25] backports_1.1.4 assertthat_0.2.1 Matrix_1.2-17 ## [28] lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [31] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [34] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 ## [37] glue_1.3.1 reshape2_1.4.3 V8_2.2 ## [40] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-144 ## [43] crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [46] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.5 ## [52] zoo_1.8-6 scales_1.1.1.9000 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 curl_3.3 ## [61] yaml_2.2.0 loo_2.1.0 stringi_1.4.3 ## [64] highr_0.8 dygraphs_1.1.1.6 boot_1.3-24 ## [67] pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [70] pkgconfig_2.0.2 matrixStats_0.54.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_1.5.1 htmlwidgets_1.3 ## [76] labeling_0.3 tidyselect_0.2.5 processx_3.3.1 ## [79] plyr_1.8.4 magrittr_1.5 bookdown_0.11 ## [82] R6_2.4.0 generics_0.0.2 pillar_1.4.1 ## [85] haven_2.1.0 withr_2.1.2 xts_0.11-2 ## [88] abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_1.13 ## [94] grid_3.6.3 readxl_1.3.1 callr_3.2.0 ## [97] threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [100] httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 ## [103] viridisLite_0.3.0 shinyjs_1.0 "],
["interactions.html", "7 Interactions 7.1 Building an interaction. 7.2 Symmetry of the linear interaction. 7.3 Continuous interactions 7.4 Interactions in design formulas 7.5 Summary Bonus: marginal_effects() Reference Session info", " 7 Interactions Every model so far in [McElreath’s text] has assumed that each predictor has an independent association with the mean of the outcome. What if we want to allow the association to be conditional?… To model deeper conditionality—where the importance of one predictor depends upon another predictor—we need interaction. Interaction is a kind of conditioning, a way of allowing parameters (really their posterior distributions) to be conditional on further aspects of the data. (p. 210) 7.1 Building an interaction. “Africa is special” (p. 211). Let’s load the rugged data to see one of the reasons why. library(rethinking) data(rugged) d &lt;- rugged And here we switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) rm(rugged) We’ll continue to use tidyverse-style syntax to wrangle the data. library(tidyverse) # make the log version of criterion d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) # extract countries with GDP data dd &lt;- d %&gt;% filter(complete.cases(rgdppc_2000)) # split the data into countries in Africa and not in Africa d.A1 &lt;- dd %&gt;% filter(cont_africa == 1) d.A0 &lt;- dd %&gt;% filter(cont_africa == 0) The first two models predicting log_gdp are univariable. b7.1 &lt;- brm(data = d.A1, family = gaussian, log_gdp ~ 1 + rugged, prior = c(prior(normal(8, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7) b7.2 &lt;- update(b7.1, newdata = d.A0) In the text, McElreath more or less dared us to figure out how to make Figure 7.2. Here’s the brms-relevant data wrangling. nd &lt;- tibble(rugged = seq(from = 0, to = 6.3, length.out = 30)) f_b7.1 &lt;- fitted(b7.1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b7.2 &lt;- fitted(b7.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # here we&#39;ll put both in a single data object, with `f_b7.1` stacked atop `f_b7.2` f &lt;- full_join(f_b7.1, f_b7.2) %&gt;% mutate(cont_africa = rep(c(&quot;Africa&quot;, &quot;not Africa&quot;), each = 30)) For this chapter, we’ll take our plot theme from the ggthemes package. # install.packages(&quot;ggthemes&quot;, dependencies = T) library(ggthemes) Here’s the plot code for our version of Figure 7.2. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = cont_africa, color = cont_africa), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp, color = cont_africa), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) 7.1.1 Adding a dummy variable doesn’t work. Here’s our model with all the countries, but without the cont_africa dummy. b7.3 &lt;- update(b7.1, newdata = dd) Now we’ll add the dummy. b7.4 &lt;- update(b7.3, newdata = dd, formula = log_gdp ~ 1 + rugged + cont_africa) Using the skills from Chapter 6, let’s compute the information criteria for the two models. Note how with the add_criterion() function, you can compute both the LOO and the WAIC at once. b7.3 &lt;- add_criterion(b7.3, c(&quot;loo&quot;, &quot;waic&quot;)) b7.4 &lt;- add_criterion(b7.4, c(&quot;loo&quot;, &quot;waic&quot;)) Here we’ll compare the models with the loo_compare() function, first by the WAIC and then by the LOO. loo_compare(b7.3, b7.4, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## b7.4 0.0 0.0 ## b7.3 -31.6 7.3 loo_compare(b7.3, b7.4, criterion = &quot;loo&quot;) ## elpd_diff se_diff ## b7.4 0.0 0.0 ## b7.3 -31.6 7.3 Happily, the WAIC and the LOO are in agreement. The model with the dummy, b7.4, fit the data much better. Here are the WAIC model weights. model_weights(b7.3, b7.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 ## 0 1 As in the text, almost all the weight went to the multivariable model, b7.4. Before we can plot that model, we need to wrangle a bit. nd &lt;- tibble(rugged = seq(from = 0, to = 6.3, length.out = 30) %&gt;% rep(., times = 2), cont_africa = rep(0:1, each = 30)) f &lt;- fitted(b7.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) Behold our Figure 7.3. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = cont_africa, color = cont_africa), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp, color = cont_africa), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = c(.69, .94), legend.title = element_blank(), legend.direction = &quot;horizontal&quot;) 7.1.2 Adding a linear interaction does work. Yes, it sure does. But before we fit, here’s the equation: \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = \\beta_1 + \\beta_3 \\text{cont_africa}_i \\end{align*}\\] Fit the model. b7.5 &lt;- update(b7.4, formula = log_gdp ~ 1 + rugged*cont_africa) For kicks, we’ll just use the LOO to compare the last three models. b7.5 &lt;- add_criterion(b7.5, c(&quot;loo&quot;, &quot;waic&quot;)) l &lt;- loo_compare(b7.3, b7.4, b7.5, criterion = &quot;loo&quot;) print(l, simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b7.5 0.0 0.0 -234.8 7.3 5.1 0.9 469.6 14.6 ## b7.4 -3.3 3.0 -238.1 7.4 4.2 0.8 476.2 14.9 ## b7.3 -34.9 7.3 -269.7 6.5 2.5 0.3 539.3 12.9 And recall, if we want those LOO difference scores in the traditional metric like McElreath displayed in the text, we can do a quick conversion with algebra and cbind(). cbind(loo_diff = l[, 1] * -2, se = l[, 2] * 2) ## loo_diff se ## b7.5 0.000000 0.000000 ## b7.4 6.540292 5.964698 ## b7.3 69.723681 14.670811 And we can weight the models based on the LOO rather than the WAIC, too. model_weights(b7.3, b7.4, b7.5, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 b7.5 ## 0.000 0.037 0.963 7.1.2.1 Overthinking: Conventional form of interaction. The conventional equation for the interaction model might look like: \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i \\end{align*}\\] Instead of the y ~ 1 + x1*x2 approach, which will work fine with brm(), you can use this more explicit syntax. b7.5b &lt;- update(b7.5, formula = log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa) From here on, I will default to this style of syntax for interactions. Since this is the same model, it yields the same information criteria estimates within simulation error. Here we’ll confirm that with the LOO. b7.5b &lt;- add_criterion(b7.5b, c(&quot;loo&quot;, &quot;waic&quot;)) b7.5$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.8 7.3 ## p_loo 5.1 0.9 ## looic 469.6 14.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 169 99.4% 955 ## (0.5, 0.7] (ok) 1 0.6% 1514 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. b7.5b$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.7 7.3 ## p_loo 5.1 0.9 ## looic 469.4 14.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. 7.1.3 Plotting the interaction. Here’s our prep work for the figure. f &lt;- fitted(b7.5, newdata = nd) %&gt;% # we can use the same `nd` data from last time as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) And here’s the code for our version of Figure 7.4. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged, color = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = cont_africa), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) 7.1.4 Interpreting an interaction estimate. Interpreting interaction estimates is tricky. It’s trickier than interpreting ordinary estimates. And for this reason, I usually advise against trying to understand an interaction from tables of numbers along. Plotting implied predictions does far more for both our own understanding and for our audience’s. (p. 219) 7.1.4.1 Parameters change meaning. In a simple linear regression with no interactions, each coefficient says how much the average outcome, \\(\\mu\\), changes when the predictor changes by one unit. And since all of the parameters have independent influences on the outcome, there’s no trouble in interpreting each parameter separately. Each slope parameter gives us a direct measure of each predictor variable’s influence. Interaction models ruin this paradise. (p. 220) Return the parameter estimates. posterior_summary(b7.5) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 9.1827479 0.13782719 8.90831234 9.45493458 ## b_rugged -0.1858397 0.07713900 -0.33236039 -0.03231415 ## b_cont_africa -1.8447253 0.22258129 -2.29193517 -1.40547418 ## b_rugged:cont_africa 0.3489124 0.13049489 0.08189891 0.60550947 ## sigma 0.9512600 0.05429148 0.85216666 1.06468924 ## lp__ -244.4782798 1.66066034 -248.77962567 -242.36652318 “Since \\(\\gamma\\) (gamma) doesn’t appear in this table—it wasn’t estimated—we have to compute it ourselves” (p. 221). Like in the text, we’ll do so first by working with the point estimates. # within Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 1 ## [1] 0.1630727 # outside Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 0 ## [1] -0.1858397 7.1.4.2 Incorporating uncertainty. To get some idea of the uncertainty around those \\(\\gamma\\) values, we’ll need to use the whole posterior. Since \\(\\gamma\\) depends upon parameters, and those parameters have a posterior distribution, \\(\\gamma\\) must also have a posterior distribution. Read the previous sentence again a few times. It’s one of the most important concepts in processing Bayesian model fits. Anything calculated using parameters has a distribution. (p. 212) Like McElreath, we’ll avoid integral calcus in favor of working with the posterior_samples(). post &lt;- posterior_samples(b7.5) post %&gt;% transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% gather(key, value) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) ## # A tibble: 2 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 gamma_Africa 0.163 ## 2 gamma_notAfrica -0.186 And here is our version of Figure 7.5. post %&gt;% transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% gather(key, value) %&gt;% ggplot(aes(x = value, group = key, color = key, fill = key)) + geom_density(alpha = 1/4) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(expression(gamma), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Terraine Ruggedness slopes&quot;, subtitle = &quot;Blue = African nations, Green = others&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) What proportion of these differences is below zero? post %&gt;% mutate(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% mutate(diff = gamma_Africa -gamma_notAfrica) %&gt;% summarise(Proportion_of_the_difference_below_0 = sum(diff &lt; 0) / length(diff)) ## Proportion_of_the_difference_below_0 ## 1 0.00325 7.2 Symmetry of the linear interaction. Consider for example the GDP and terrain ruggedness problem. The interaction there has two equally valid phrasings. How much does the influence of ruggedness (on GDP) depend upon whether the nation is in Africa? How much does the influence of being in Africa (on GDP) depend upon ruggedness? While these two possibilities sound different to most humans, your golem thinks they are identical. (p. 223) 7.2.1 Buridan’s interaction. Recall the original equation. \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = \\beta_1 + \\beta_3 \\text{cont_africa}_i \\end{align*}\\] Next McElreath replaced \\(\\gamma_i\\) with the expression for \\(\\mu_i\\). \\[\\begin{align*} \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_3 \\text{cont_africa}_i) \\times \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ &amp; = \\alpha + \\beta_1 \\text{rugged}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i + \\beta_2 \\text{cont_africa}_i \\end{align*}\\] And new we’ll factor together the terms containing \\(\\text{cont_africa}_i\\). \\[ \\mu_i = \\alpha + \\beta_1 \\text{rugged}_i + \\underbrace{(\\beta_2 + \\beta_3 \\text{rugged}_i)}_G \\times \\text{cont_africa}_i \\] And just as in the text, our \\(G\\) term looks a lot like the original \\(\\gamma_i\\) term. 7.2.2 Africa depends upon ruggedness. Here is our version of McElreath’s Figure 7.6. # new predictor data for `fitted()` nd &lt;- tibble(rugged = rep(range(dd$rugged), times = 2), cont_africa = rep(0:1, each = 2)) # `fitted()` f &lt;- fitted(b7.5, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(ox = rep(c(-0.05, 0.05), times = 2)) # augment the `dd` data a bit dd %&gt;% mutate(ox = ifelse(rugged &gt; median(rugged), 0.05, -0.05), cont_africa = cont_africa + ox) %&gt;% select(cont_africa, everything()) %&gt;% # plot ggplot(aes(x = cont_africa, color = factor(ox))) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = factor(ox), linetype = factor(ox)), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), alpha = 1/2, shape = 1) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Continent&quot;, breaks = 0:1, labels = c(&quot;other&quot;, &quot;Africa&quot;)) + coord_cartesian(xlim = c(-.2, 1.2)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) 7.3 Continuous interactions Though continuous interactions can be more challenging to interpret, they’re just as easy to fit as interactions including dummies. 7.3.1 The data. Look at the tulips. library(rethinking) data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 7.3.2 The un-centered models. The likelihoods for the next two models are \\[\\begin{align*} \\text{blooms}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water}_i + \\beta_2 \\text{shade}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim \\text{Uniform} (0, 100) \\end{align*}\\] and \\[\\begin{align*} \\text{blooms}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water} + \\beta_2 \\text{shade}_i + \\beta_3 \\text{water}_i \\times \\text{shade}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_3 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim \\text{Uniform} (0, 100) \\end{align*}\\] Load brms. detach(package:rethinking, unload = T) library(brms) rm(tulips) Here we continue with McElreath’s very-flat priors for the multivariable and interaction models. b7.6 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water + shade, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 7) ## Warning: There were 49 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: Examine the pairs() plot to diagnose sampling problems b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade) ## Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: Examine the pairs() plot to diagnose sampling problems Much like in the text, these models yielded divergent transitions. Here, we’ll try to combat them by following Stan’s advice and “[increase] adapt_delta above 0.8.” While we’re at it, we’ll put better priors on \\(\\sigma\\). b7.6 &lt;- update(b7.6, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), control = list(adapt_delta = 0.9), seed = 7) b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade) Increasing adapt_delta did the trick. Instead of coeftab(), we can also use posterior_summary(), which gets us most of the way there. posterior_summary(b7.6) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 60.67 42.67 -21.92 144.77 ## b_water 74.11 14.44 45.62 101.89 ## b_shade -40.86 14.45 -69.63 -11.82 ## sigma 61.33 8.79 47.30 81.84 ## lp__ -169.73 1.50 -173.48 -167.87 posterior_summary(b7.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -108.83 64.39 -232.19 18.76 ## b_water 160.59 29.65 100.91 218.79 ## b_shade 44.81 29.73 -16.06 102.22 ## b_water:shade -43.55 13.69 -69.48 -16.06 ## sigma 50.09 7.38 37.84 67.17 ## lp__ -170.66 1.70 -174.96 -168.40 This is an example where HMC yielded point estimates notably different from MAP. However, look at the size of those posterior standard deviations (i.e., ‘Est.Error’ column)! The MAP estimates are well within a fraction of those \\(SD\\)s. Anyway, let’s look at WAIC. b7.6 &lt;- add_criterion(b7.6, &quot;waic&quot;) b7.7 &lt;- add_criterion(b7.7, &quot;waic&quot;) w &lt;- loo_compare(b7.6, b7.7, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b7.7 0.0 0.0 -146.7 3.9 4.6 1.2 293.4 7.8 ## b7.6 -5.2 2.7 -151.9 3.8 4.0 1.0 303.9 7.6 Here we use our cbind() trick to convert the difference from the \\(\\text{elpd}\\) metric to the more traditional WAIC metric. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b7.7 0.00000 0.000000 ## b7.6 10.47338 5.307215 Why not compute the WAIC weights? model_weights(b7.6, b7.7, weights = &quot;waic&quot;) ## b7.6 b7.7 ## 0.005289698 0.994710302 As in the text, almost all the weight went to the interaction model, b7.7. 7.3.3 Center and re-estimate. To center a variable means to create a new variable that contains the same information as the original, but has a new mean of zero. For example, to make centered versions of shade and water, just subtract the mean of the original from each value. (p. 230, emphasis in the original) Here’s a tidyverse way to center the predictors. d &lt;- d %&gt;% mutate(shade_c = shade - mean(shade), water_c = water - mean(water)) Now refit the models with our shiny new centered predictors. b7.8 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water_c + shade_c, prior = c(prior(normal(130, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 7) b7.9 &lt;- update(b7.8, formula = blooms ~ 1 + water_c + shade_c + water_c:shade_c) Check out the results. posterior_summary(b7.8) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 128.99 11.36 106.27 150.68 ## b_water_c 73.92 14.13 45.80 101.82 ## b_shade_c -40.85 14.14 -68.35 -12.70 ## sigma 61.13 9.06 46.57 81.27 ## lp__ -168.87 1.46 -172.54 -167.07 posterior_summary(b7.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 128.85 9.67 109.47 147.70 ## b_water_c 74.70 11.54 51.49 96.84 ## b_shade_c -41.02 11.62 -64.15 -18.13 ## b_water_c:shade_c -51.43 15.03 -81.18 -22.41 ## sigma 49.68 7.77 37.39 67.93 ## lp__ -168.64 1.79 -173.11 -166.30 And okay fine, if you really want a coeftab()-like summary, here’s a way to do it. tibble(model = str_c(&quot;b7.&quot;, 8:9)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, broom::tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% select(term, estimate, model) %&gt;% spread(key = model, value = estimate) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 5 x 3 ## term b7.8 b7.9 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 129. 129. ## 2 b_shade_c -40.8 -41.0 ## 3 b_water_c 73.9 74.7 ## 4 b_water_c:shade_c NA -51.4 ## 5 sigma 61.1 49.7 Anyway, centering helped a lot. Now, not only do the results in the text match up better than those from Stan, but the ‘Est.Error’ values are uniformly smaller. 7.3.3.1 Estimation worked better. Nothing to add, here. 7.3.3.2 Estimates changed less across models. On page 231, we read: The interaction parameter always factors into generating a prediction. Consider for example a tulip at the average moisture and shade levels, 2 in each case. The expected blooms for such a tulip is: \\[\\mu_i | \\text{shade}_{i = 2}, \\text{water}_{i = 2} = \\alpha + \\beta_\\text{water} (2) + \\beta_\\text{shade} (2) + \\beta_{\\text{water} \\times \\text{shade}} (2 \\times 2)\\] So to figure out the effect of increasing water by 1 unit, you have to use all of the \\(\\beta\\) parameters. Plugging in the [HMC] values for the un-centered interaction model, [b7.7], we get: \\[\\mu_i | \\text{shade}_{i = 2}, \\text{water}_{i = 2} = -107.1 + 159.9 (2) + 44.0 (2) -43.2 (2 \\times 2)\\] With our brms workflow, we use fixef() to compute the predictions. k &lt;- fixef(b7.7) k[1] + k[2] * 2 + k[3] * 2 + k[4] * 2 * 2 ## [1] 127.7793 Even though or HMC parameters differ a bit from the MAP estimates McElreath reported in the text, the value they predicted matches quite closely with the one in the text. Same thing for the next one. k &lt;- fixef(b7.9) k[1] + k[2] * 0 + k[3] * 0 + k[4] * 0 * 0 ## [1] 128.8479 Here are the coefficient summaries for the centered model. print(b7.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: blooms ~ water_c + shade_c + water_c:shade_c ## Data: d (Number of observations: 27) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 128.85 9.67 109.47 147.70 4619 1.00 ## water_c 74.70 11.54 51.49 96.84 5133 1.00 ## shade_c -41.02 11.62 -64.15 -18.13 4671 1.00 ## water_c:shade_c -51.43 15.03 -81.18 -22.41 5056 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 49.68 7.77 37.39 67.93 3723 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 7.3.4 Plotting implied predictions. Now we’re ready for the bottom row of Figure 7.7. Here’s our variation on McElreath’s tryptych loop code, adjusted for brms and ggplot2. # loop over values of `water_c` and plot predictions shade_seq &lt;- -1:1 for(w in -1:1){ # define the subset of the original data dt &lt;- d[d$water_c == w, ] # defining our new data nd &lt;- tibble(water_c = w, shade_c = shade_seq) # use our sampling skills, like before f &lt;- fitted(b7.9, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # specify our custom plot fig &lt;- ggplot() + geom_smooth(data = f, aes(x = shade_c, y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#CC79A7&quot;, color = &quot;#CC79A7&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dt, aes(x = shade_c, y = blooms), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + labs(&quot;Blooms&quot;, title = paste(&quot;Water (centered) =&quot;, w)) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;)) # plot that joint plot(fig) } But we don’t necessarily need a loop. We can achieve all of McElreath’s Figure 7.7 with fitted(), some data wrangling, and a little help from ggplot2::facet_grid(). # `fitted()` for model b7.8 fitted(b7.8) %&gt;% as_tibble() %&gt;% # add `fitted()` for model b7.9 bind_rows( fitted(b7.9) %&gt;% as_tibble() ) %&gt;% # we&#39;ll want to index the models mutate(fit = rep(c(&quot;b7.8&quot;, &quot;b7.9&quot;), each = 27)) %&gt;% # here we add the data, `d` bind_cols(bind_rows(d, d)) %&gt;% # these will come in handy for `ggplot2::facet_grid()` mutate(x_grid = paste(&quot;water_c =&quot;, water_c), y_grid = paste(&quot;model: &quot;, fit)) %&gt;% # plot! ggplot(aes(x = shade_c)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#CC79A7&quot;, color = &quot;#CC79A7&quot;, alpha = 1/5, size = 1/2) + geom_point(aes(y = blooms, group = x_grid), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + ylab(&quot;Blooms&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), panel.background = element_rect(color = &quot;black&quot;)) + facet_grid(y_grid ~ x_grid) 7.4 Interactions in design formulas The brms syntax generally follows the design formulas typical of lm(). Hopefully this is all old hat. 7.5 Summary Bonus: marginal_effects() The brms package includes the marginal_effects() function as a convenient way to look at simple effects and two-way interactions. Recall the simple univariable model, b7.3: b7.3$formula ## log_gdp ~ 1 + rugged We can look at the regression line and its percentile-based intervals like so: marginal_effects(b7.3) If we nest marginal_effects() within plot() with a points = T argument, we can add the original data to the figure. plot(marginal_effects(b7.3), points = T) We can further customize the plot. For example, we can replace the intervals with a spaghetti plot. While we’re at it, we can use point_args to adjust the geom_jitter() parameters. plot(marginal_effects(b7.3, spaghetti = T, nsamples = 200), points = T, point_args = c(alpha = 1/2, size = 1)) With multiple predictors, things get more complicated. Consider our multivariable, non-interaction model, b7.4. b7.4$formula ## log_gdp ~ rugged + cont_africa marginal_effects(b7.4) We got one plot for each predictor, controlling the other predictor at zero. Note how the plot for cont_africa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set: b7.4$data %&gt;% glimpse() ## Observations: 170 ## Variables: 3 ## $ log_gdp &lt;dbl&gt; 7.492609, 8.216929, 9.933263, 9.407032, 7.792343, 9.212541, 10.143191, 10.274… ## $ rugged &lt;dbl&gt; 0.858, 3.427, 0.769, 0.775, 2.688, 0.006, 0.143, 3.513, 1.672, 1.780, 0.388, … ## $ cont_africa &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,… One way to fix that is to adjust the data set and refit the model. d_factor &lt;- b7.4$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.4_factor &lt;- update(b7.4, newdata = d_factor) Using the update() syntax often speeds up the re-fitting process. marginal_effects(b7.4_factor) Now our second marginal plot more clearly expresses the cont_africa predictor as categorical. Things get more complicated with the interaction model, b7.5. b7.5$formula ## log_gdp ~ rugged + cont_africa + rugged:cont_africa marginal_effects(b7.5) The marginal_effects() function defaults to expressing interactions such that the first variable in the term–in this case, rugged–is on the x axis and the second variable in the term–cont_africa, treated as an integer–is depicted in three lines corresponding its mean and its mean +/- one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is, you guessed it, to refit the model after adjusting the data. d_factor &lt;- b7.5$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.5_factor &lt;- update(b7.5, newdata = d_factor) Just for kicks, we’ll use probs = c(.25, .75) to return 50% intervals, rather than the conventional 95%. marginal_effects(b7.5_factor, probs = c(.25, .75)) With the effects argument, we can just return the interaction effect, which is where all the action’s at. While we’re at it, we’ll use plot() to change some of the settings. plot(marginal_effects(b7.5_factor, effects = &quot;rugged:cont_africa&quot;, spaghetti = T, nsamples = 150), points = T, point_args = c(alpha = 2/3, size = 1), mean = F) Note, the ordering of the variables matters for the interaction term. Consider our interaction model for the tulips data. b7.9$formula ## blooms ~ water_c + shade_c + water_c:shade_c The plot tells a slightly different story, depending on whether you specify effects = &quot;shade_c:water_c&quot; or effects = &quot;water_c:shade_c&quot;. plot(marginal_effects(b7.9, effects = &quot;shade_c:water_c&quot;), points = T) plot(marginal_effects(b7.9, effects = &quot;water_c:shade_c&quot;), points = T) One might want to evaluate the effects of the second term in the interaction–water_c, in this case–at values other than the mean and the mean +/- one standard deviation. When we reproduced the bottom row of Figure 7.7, we expressed the interaction based on values -1, 0, and 1 for water_c. We can do that, here, by using the int_conditions argument. It expects a list, so we’ll put our desired water_c values in just that. ic &lt;- list(water.c = c(-1, 0, 1)) plot(marginal_effects(b7.9, effects = &quot;shade_c:water_c&quot;, int_conditions = ic), points = T) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggthemes_4.2.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [6] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 tidyverse_1.2.1 brms_2.9.0 ## [11] Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 markdown_1.0 ## [5] base64enc_0.1-3 rstudioapi_0.10 farver_2.0.3 DT_0.7 ## [9] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 xml2_1.2.0 ## [13] bridgesampling_0.6-0 codetools_0.2-16 knitr_1.23 shinythemes_1.1.2 ## [17] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 broom_0.5.2 ## [21] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 backports_1.1.4 ## [25] assertthat_0.2.1 Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 ## [29] later_0.8.0 htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [33] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 glue_1.3.1 ## [37] reshape2_1.4.3 V8_2.2 cellranger_1.1.0 vctrs_0.1.0 ## [41] nlme_3.1-144 crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [45] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 lifecycle_0.1.0 ## [49] gtools_3.8.1 MASS_7.3-51.5 zoo_1.8-6 scales_1.1.1.9000 ## [53] colourpicker_1.0 hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [57] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 curl_3.3 ## [61] gridExtra_2.3 loo_2.1.0 stringi_1.4.3 dygraphs_1.1.1.6 ## [65] boot_1.3-24 pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [69] pkgconfig_2.0.2 matrixStats_0.54.0 evaluate_0.14 lattice_0.20-38 ## [73] rstantools_1.5.1 htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [77] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 bookdown_0.11 ## [81] R6_2.4.0 generics_0.0.2 pillar_1.4.1 haven_2.1.0 ## [85] withr_2.1.2 xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [89] crayon_1.3.4 utf8_1.1.4 rmarkdown_1.13 grid_3.6.3 ## [93] readxl_1.3.1 callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [97] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 ## [101] shinyjs_1.0 "],
["markov-chain-monte-carlo.html", "8 Markov Chain Monte Carlo 8.1 Good King Markov and His island kingdom 8.2 Markov chain Monte Carlo 8.3 Easy HMC: map2stan brm() 8.4 Care and feeding of your Markov chain. Reference Session info", " 8 Markov Chain Monte Carlo “This chapter introduces one of the more marvelous examples of how Fortuna and Minerva cooperate: the estimation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC) estimation” (p. 241). Though we’ve been using MCMC via the brms package for chapters, now, this chapter should clarify some of the details. 8.1 Good King Markov and His island kingdom In this version of the code, we’ve added set.seed(), which helps make the exact results reproducible. set.seed(8) num_weeks &lt;- 1e5 positions &lt;- rep(0, num_weeks) current &lt;- 10 for (i in 1:num_weeks) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around the archipelago if (proposal &lt; 1) proposal &lt;- 10 if (proposal &gt; 10) proposal &lt;- 1 # move? prob_move &lt;- proposal / current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } In this chapter, we’ll borrow a theme, theme_ipsum(), from the hrbrthemes package. # install.packages(&quot;hrbrthemes&quot;, dependencies = T) library(hrbrthemes) Figure 8.2.a. library(tidyverse) tibble(week = 1:1e5, island = positions) %&gt;% ggplot(aes(x = week, y = island)) + geom_point(shape = 1) + scale_x_continuous(breaks = seq(from = 0, to = 100, by = 20)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + coord_cartesian(xlim = 0:100, ylim = 1:10) + labs(title = &quot;Behold: The Metropolis algorithm in action!&quot;, subtitle = &quot;The dots show the king&#39;s path over the first 100 weeks.&quot;) + theme_ipsum() Figure 8.2.b. tibble(week = 1:1e5, island = positions) %&gt;% mutate(island = factor(island)) %&gt;% ggplot(aes(x = island)) + geom_bar() + labs(title = &quot;Old Metropolis shines in the long run.&quot;, subtitle = &quot;Sure enough, the time the king spent on each island was\\nproportional to its population size.&quot;) + theme_ipsum() 8.2 Markov chain Monte Carlo “The metropolis algorithm is the grandparent of several different strategies for getting samples from unknown posterior distributions” (p. 245). If you’re interested, Robert and Casells wrote a good historical overview of MCMC. 8.3 Easy HMC: map2stan brm() Here we load the rugged data. library(rethinking) data(rugged) d &lt;- rugged Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(rugged) It takes just a sec to do a little data manipulation. d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) dd &lt;- d %&gt;% drop_na(rgdppc_2000) In the context of this chapter, it doesn’t make sense to translate McElreath’s m8.1 map() code to brm() code. Below, we’ll just go directly to the brm() variant of his m8.1stan. 8.3.1 Preparation. When working with brms, you don’t need to do the data processing McElreath did on pages 248 and 249. If you wanted to, however, here’s how you might do it within the tidyverse. dd.trim &lt;- dd %&gt;% select(log_gdp, rugged, cont_africa) str(dd.trim) 8.3.2 Estimation. Finally, we get to work that sweet HMC. b8.1 &lt;- brm(data = dd, family = gaussian, log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), seed = 8) Now we have officially ditched the uniform distribution for \\(\\sigma\\). We’ll only see it again in special cases for pedagogical purposes. Here’s the posterior: print(b8.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.95 9.50 2589 1.00 ## rugged -0.20 0.08 -0.36 -0.05 2476 1.00 ## cont_africa -1.95 0.24 -2.39 -1.47 2281 1.00 ## rugged:cont_africa 0.39 0.14 0.12 0.66 2221 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.85 1.06 4125 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do note a couple things: If you look closely at the summary information at the top, you’ll see that the brms::brm() function defaults to chains = 4. If you check the manual, you’ll see it also defaults to cores = 1. You’ll also note it defaults to iter = 2000, warmup = 1000. Also of note, McElreath’s rethinking::precis() returns highest posterior density intervals (HPDIs) when summarizing map2stan() models. Not so with brms. If you want HPDIs, you’ll have to use the convenience functions from the tidybayes package. library(tidybayes) post &lt;- posterior_samples(b8.1) post %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) # note our rare use of 89% intervals ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_cont_africa -1.95 -2.31 -1.57 0.89 mean hdi ## 2 b_Intercept 9.22 9.00 9.46 0.89 mean hdi ## 3 b_rugged -0.202 -0.328 -0.0737 0.89 mean hdi ## 4 b_rugged:cont_africa 0.392 0.171 0.602 0.89 mean hdi ## 5 lp__ -249. -251. -246. 0.89 mean hdi ## 6 sigma 0.950 0.866 1.04 0.89 mean hdi 8.3.3 Sampling again, in parallel. Here we sample in parallel by adding cores = 4. b8.1_4chains_4cores &lt;- update(b8.1, cores = 4) This model sampled so fast that it really didn’t matter if we sampled in parallel or not. It will for others. print(b8.1_4chains_4cores) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.95 9.49 2942 1.00 ## rugged -0.20 0.08 -0.35 -0.05 2642 1.00 ## cont_africa -1.94 0.23 -2.40 -1.50 2376 1.00 ## rugged:cont_africa 0.39 0.13 0.14 0.65 2140 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.85 1.05 3639 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 8.3.4 Visualization. Unlike the way rethinking’s extract.samples() yields a list, brms’s posterior_samples() returns a data frame. post &lt;- posterior_samples(b8.1) str(post) ## &#39;data.frame&#39;: 4000 obs. of 6 variables: ## $ b_Intercept : num 9.25 9.23 9.29 9.13 9.07 ... ## $ b_rugged : num -0.193 -0.189 -0.21 -0.193 -0.117 ... ## $ b_cont_africa : num -1.9 -1.97 -2.32 -2.09 -1.88 ... ## $ b_rugged:cont_africa: num 0.369 0.373 0.501 0.578 0.445 ... ## $ sigma : num 0.951 1.046 0.864 0.846 1.06 ... ## $ lp__ : num -246 -248 -249 -250 -250 ... As with McElreath’s rethinking, brms allows users to put the post data frame or the brmsfit object directly in pairs(). pairs(b8.1, off_diag_args = list(size = 1/5, alpha = 1/5)) Another nice way to customize your pairs plot is with the GGally package. library(GGally) post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs() Since GGally returns a ggplot2 object, you can customize it as you please. my_diag &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;grey50&quot;) } my_lower &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_point(shape = 1, size = 1/2, alpha = 1/6) } post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs(diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + labs(subtitle = &quot;My custom pairs plot&quot;) + theme_ipsum() For more ideas on customizing a GGally pairs plot, go here. 8.3.5 Using the samples. Older versions of brms allowed users to include information criteria as a part of the model summary by adding loo = T and/or waic = T in the summary() function (e.g., summary(b8.1, loo = T, waic = T). However, this is no longer the case. E.g., summary(b8.1, loo = T, waic = T) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.95 9.50 2589 1.00 ## rugged -0.20 0.08 -0.36 -0.05 2476 1.00 ## cont_africa -1.95 0.24 -2.39 -1.47 2281 1.00 ## rugged:cont_africa 0.39 0.14 0.12 0.66 2221 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.85 1.06 4125 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Although R didn’t bark at us for adding loo = T, waic = T, they didn’t do anything. Nowadays, if you want that information, you’ll have to use the waic() and/or loo() functions. waic(b8.1) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_waic -234.6 7.4 ## p_waic 5.1 0.9 ## waic 469.3 14.9 ## Warning: 2 (1.2%) p_waic estimates greater than 0.4. We recommend trying ## loo instead. (l_b8.1 &lt;- loo(b8.1)) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.7 7.4 ## p_loo 5.2 0.9 ## looic 469.4 14.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 169 99.4% 1369 ## (0.5, 0.7] (ok) 1 0.6% 1343 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. And the recommended workflow since brms version 2.8.0 is to save the information criteria information with your brm() fit objects with the add_criterion() function. b8.1 &lt;- add_criterion(b8.1, c(&quot;waic&quot;, &quot;loo&quot;)) You retrieve that information like this: b8.1$waic ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_waic -234.6 7.4 ## p_waic 5.1 0.9 ## waic 469.3 14.9 ## Warning: 2 (1.2%) p_waic estimates greater than 0.4. We recommend trying ## loo instead. b8.1$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.7 7.4 ## p_loo 5.2 0.9 ## looic 469.4 14.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 169 99.4% 1369 ## (0.5, 0.7] (ok) 1 0.6% 1343 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. In response to the brms version 2.8.0 update, which itself accommodated updates to the loo package and both of which occurred years after McElreath published the first edition of his text, we’ve been bantering on about the \\(\\text{elpd}\\) and its relation to the WAIC and the LOO since Chapter 6. This is a fine place to go into some detail. The elpd values returned by loo() and waic() are the expected log pointwise predictive density for new data. It follows the formula \\[\\text{elpd} = \\sum_{i = 1}^n \\int p_t (\\tilde{y}_i) \\text{log} p (\\tilde{y}_i | y) d \\tilde{y}_i,\\] where \\(p_t (\\tilde{y}_i)\\) is the distribution representing the true data-generating process for \\(\\tilde{y}_i\\). The \\(p_t (\\tilde{y}_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distributions are also implicitly conditioned on any predictors in the model. (Vehtari, Gelman, &amp; Gabry, 2016, p. 2). Later in the paper, we learn the elpd_loo (i.e., the Bayesian LOO estimate of out-of-sample predictive fit) is defined as \\[\\text{elpd}_{\\text{loo}} = \\sum_{i = 1}^n \\text{log } p (y_i | y - _i),\\] where \\[p (y_i | y - _i) = \\int p (y_i | \\theta) p (\\theta | y - _i) d \\theta\\] “is the leave-one-out predictive density given the data without the \\(i\\)th data point” (p. 3). And recall, you can convert the \\(\\text{elpd}\\) to the conventional information criteria metric by multiplying it by -2. To learn more about the \\(\\text{elpd}\\), read the rest of the paper and the other works referenced by the loo package team. And if you prefer watching video lectures to reading technical papers, check out Vehtari’s Model assessment, selection and averaging. 8.3.6 Checking the chain. Using plot() for a brm() fit returns both density and trace lots for the parameters. plot(b8.1) The bayesplot package allows a little more control. Here, we use bayesplot’s mcmc_trace() to show only trace plots with our custom theme. Note that mcmc_trace() works with data frames, not brmfit objects. There’s a further complication. Recall how we made post (i.e., post &lt;- posterior_samples(b8.1)). Our post data frame carries no information on chains. To retain that information, we’ll need to add an add_chain = T argument to our posterior_samples() function. library(bayesplot) post &lt;- posterior_samples(b8.1, add_chain = T) mcmc_trace(post[, c(1:5, 7)], # we need to include column 7 because it contains the chain info facet_args = list(ncol = 3), size = .15) + labs(title = &quot;My custom trace plots&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.95, .2)) The bayesplot package offers a variety of diagnostic plots. Here we make autocorrelation plots for all model parameters, one for each HMC chain. mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;b_rugged&quot;, &quot;b_cont_africa&quot;, &quot;b_rugged:cont_africa&quot;, &quot;sigma&quot;), lags = 5) + scale_color_ipsum() + theme_ipsum() That’s just what we like to see–nice L-shaped autocorrelation plots. Those are the kinds of shapes you’d expect when you have reasonably large effective samples. Anyway… 8.3.6.1 Overthinking: Raw Stan model code. The stancode() function works in brms much like it does in rethinking. brms::stancode(b8.1) ## // generated with brms 2.9.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## int Kc = K - 1; ## matrix[N, Kc] Xc; // centered version of X ## vector[Kc] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // population-level effects ## real temp_Intercept; // temporary intercept ## real&lt;lower=0&gt; sigma; // residual SD ## } ## transformed parameters { ## } ## model { ## vector[N] mu = temp_Intercept + Xc * b; ## // priors including all constants ## target += normal_lpdf(b | 0, 10); ## target += normal_lpdf(temp_Intercept | 0, 100); ## target += cauchy_lpdf(sigma | 0, 2) ## - 1 * cauchy_lccdf(0 | 0, 2); ## // likelihood including all constants ## if (!prior_only) { ## target += normal_lpdf(Y | mu, sigma); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = temp_Intercept - dot_product(means_X, b); ## } You can also get that information with b8.1$model or b8.1$fit@stanmodel. 8.4 Care and feeding of your Markov chain. Markov chain Monte Carlo is a highly technical and usually automated procedure. Most people who use it don’t really understand what it is doing. That’s okay, up to a point. Science requires division of labor, and if every one of us had to write our own Markov chains from scratch, a lot less research would get done in the aggregate. (p. 255) But if you do want to learn more about HMC, McElreath has some nice introductory lectures on the topic (see here and here). To dive even deeper, Michael Betancourt from the Stan team has given many lectures on the topic (e.g., here and here). 8.4.1 How many samples do you need? The brms defaults for iter and warmup match those of McElreath’s rethinking. If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255) 8.4.2 How many chains do you need? “Using 3 or 4 chains is conventional, and quite often more than enough to reassure us that the sampling is working properly” (p. 257). 8.4.2.1 Convergence diagnostics. The default diagnostic output from Stan includes two metrics, n_eff and Rhat. The first is a measure of the effective number of samples. The second is the Gelman-Rubin convergence diagnostic, \\(\\hat{R}\\). When n_eff is much lower than the actual number of iterations (minus warmup) of your chains, it means the chains are inefficient, but possibly still okay. When Rhat is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples. If you draw more iterations, it could be fine, or it could never converge. See the Stan user manual for more details. It’s important however not to rely too much on these diagnostics. Like all heuristics, there are cases in which they provide poor advice. (p. 257) For more on n_eff and Rhat, you might also check out Gabry and Modrák’s vignette, Visual MCMC diagnostics using the bayesplot package. The \\(\\hat{R}\\) has been our friend for many years. But times are changing. As it turns out, the Stan team has found some deficiencies with the \\(\\hat{R}\\), for which they’ve made recommendations that will be implemented in the Stan ecosystem sometime soon. In the meantime, you can read all about it in their preprint and in one of Dan Simpson’s blogs. If you learn best by sassy twitter banter, click through this interchange among some of our Stan team all-stars. 8.4.3 Taming a wild chain. As with rethinking, brms can take data in the form of a list. Recall however, that in order to specify starting values, you need to specify a list of lists with an inits argument rather than with start. b8.2 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(uniform(-1e10, 1e10), class = Intercept), prior(uniform(0, 1e10), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8) Those were some silly flat priors. Check the damage. post &lt;- posterior_samples(b8.2, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.a.&quot;, subtitle = &quot;These trace plots do not look like the fuzzy caterpillars we usually hope for.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Let’s peek at the summary. print(b8.2) ## Warning: The model has not converged (some Rhats are &gt; 1.1). Do not analyse the results! ## We recommend running more iterations and/or setting stronger priors. ## Warning: There were 669 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept -96140259.73 264598844.49 -1141666616.81 29882399.77 15 ## Rhat ## Intercept 1.13 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 338093987.31 1051205791.44 27295.83 3520419658.02 33 1.05 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Holy smokes, those parameters are a mess! Plus we got a nasty warning message, too. Watch our reasonable priors save the day. b8.3 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8) print(b8.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.04 1.55 -3.15 3.08 1582 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.97 1.61 0.62 6.42 1614 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As in the text, no more warning signs and no more silly estimates. The trace plots look great, too. post &lt;- posterior_samples(b8.3, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.b&quot;, subtitle = &quot;Oh man. This looks so much better.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Now behold our version of Figure 8.6.a. post %&gt;% select(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = min(post$b_Intercept), to = max(post$b_Intercept), length.out = 50)), aes(x = x, y = dnorm(x = x, mean = 0, sd = 10)), color = ipsum_pal()(1), linetype = 2) + theme_ipsum() Here’s our version of Figure 8.6.b. post %&gt;% select(sigma) %&gt;% ggplot(aes(x = sigma)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = 0, to = max(post$sigma), length.out = 50)), aes(x = x, y = dcauchy(x = x, location = 0, scale = 1)*2), color = ipsum_pal()(2)[2], linetype = 2) + coord_cartesian(xlim = c(0, 10)) + theme_ipsum() 8.4.3.1 Overthinking: Cauchy distribution. Behold the beautiful Cauchy probability density: \\[p(x|x_0, \\gamma) = \\Bigg ( \\pi \\gamma \\Bigg [ 1 + \\Big ( \\frac{x - x_0}{\\gamma} \\Big ) ^2 \\Bigg ] \\Bigg ) ^{-1}\\] The Cauchy has no mean and variance, but \\(x_0\\) is the location and \\(\\gamma\\) is the scale. Here’s our version of the simulation. Note our use of the cummean() function. n &lt;- 1e4 set.seed(8) tibble(y = rcauchy(n, location = 0, scale = 5), mu = cummean(y), index = 1:n) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line() + theme_ipsum() The whole thing is quite remarkible. Just for kicks, here we do it again, this time with eight simulations. n &lt;- 1e4 set.seed(8) tibble(a = rcauchy(n, location = 0, scale = 5), b = rcauchy(n, location = 0, scale = 5), c = rcauchy(n, location = 0, scale = 5), d = rcauchy(n, location = 0, scale = 5), e = rcauchy(n, location = 0, scale = 5), f = rcauchy(n, location = 0, scale = 5), g = rcauchy(n, location = 0, scale = 5), h = rcauchy(n, location = 0, scale = 5)) %&gt;% gather() %&gt;% group_by(key) %&gt;% mutate(mu = cummean(value)) %&gt;% ungroup() %&gt;% mutate(index = rep(1:n, times = 8)) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line(aes(color = key)) + scale_color_manual(values = ipsum_pal()(8)) + scale_x_continuous(breaks = c(0, 5000, 10000)) + theme_ipsum() + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, ncol = 4, scales = &quot;free&quot;) 8.4.4 Non-identifiable parameters. It appears that the only way to get a brms version of McElreath’s m8.4 and m8.5 is to augment the data. In addition to the Gaussian y vector, we’ll add two constants to the data, intercept_1 = 1 and intercept_2 = 1. set.seed(8) y &lt;- rnorm(100, mean = 0, sd = 1) b8.4 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(uniform(-1e10, 1e10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8) Our model results don’t perfectly mirror McElreath’s, but they’re identical in spirit. print(b8.4) ## Warning: The model has not converged (some Rhats are &gt; 1.1). Do not analyse the results! ## We recommend running more iterations and/or setting stronger priors. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept_1 -947.52 1018.20 -2543.49 595.05 1 2.46 ## intercept_2 947.43 1018.20 -595.23 2543.43 1 2.46 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.06 0.06 0.95 1.17 29 1.08 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note the frightening warning message. Those results are a mess! Let’s try again. b8.5 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8) print(b8.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept_1 0.10 7.02 -14.10 13.33 1424 1.00 ## intercept_2 -0.19 7.03 -13.44 14.06 1424 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.09 0.08 0.94 1.25 2366 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Much better. Now we’ll do the preparatory work for Figure 8.7. Instead of showing the plots, here, we’ll save them as objects, left_column and right_column, in order to combine them below. post &lt;- posterior_samples(b8.4, add_chain = T) left_column &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) post &lt;- posterior_samples(b8.5, add_chain = T) right_column &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) library(gridExtra) grid.arrange(left_column, right_column, ncol = 2) The central message in the text, default to weakly-regularizing priors, holds for brms just as it does in rethinking. For more on the topic, see the recommendations from the Stan team. If you want to dive deeper, check out Dan Simpson’s post on Gelman’s blog and their corresponding paper with Michael Betancourt. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] gridExtra_2.3 bayesplot_1.7.0 GGally_1.4.0 ## [4] tidybayes_1.1.0 brms_2.9.0 Rcpp_1.0.1 ## [7] dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 ## [10] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [13] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 ## [16] tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## [19] extrafont_0.17 hrbrthemes_0.6.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.13 ggstance_0.3.1 ## [5] markdown_1.0 base64enc_0.1-3 ## [7] rethinking_2.01 rstudioapi_0.10 ## [9] farver_2.0.3 svUnit_0.7-12 ## [11] DT_0.7 fansi_0.4.0 ## [13] mvtnorm_1.0-10 lubridate_1.7.4 ## [15] xml2_1.2.0 codetools_0.2-16 ## [17] bridgesampling_0.6-0 knitr_1.23 ## [19] shinythemes_1.1.2 zeallot_0.1.0 ## [21] jsonlite_1.6 broom_0.5.2 ## [23] Rttf2pt1_1.3.7 shiny_1.3.2 ## [25] compiler_3.6.3 httr_1.4.0 ## [27] backports_1.1.4 assertthat_0.2.1 ## [29] Matrix_1.2-17 lazyeval_0.2.2 ## [31] cli_1.1.0 later_0.8.0 ## [33] htmltools_0.3.6 prettyunits_1.0.2 ## [35] tools_3.6.3 igraph_1.2.4.1 ## [37] coda_0.19-2 gtable_0.3.0 ## [39] glue_1.3.1 reshape2_1.4.3 ## [41] V8_2.2 cellranger_1.1.0 ## [43] vctrs_0.1.0 nlme_3.1-144 ## [45] extrafontdb_1.0 crosstalk_1.0.0 ## [47] xfun_0.7 ps_1.3.0 ## [49] rvest_0.3.4 miniUI_0.1.1.1 ## [51] mime_0.7 lifecycle_0.1.0 ## [53] gtools_3.8.1 MASS_7.3-51.5 ## [55] zoo_1.8-6 scales_1.1.1.9000 ## [57] colourpicker_1.0 hms_0.4.2 ## [59] promises_1.0.1 Brobdingnag_1.2-6 ## [61] inline_0.3.15 RColorBrewer_1.1-2 ## [63] shinystan_2.5.0 yaml_2.2.0 ## [65] curl_3.3 gdtools_0.1.8 ## [67] loo_2.1.0 reshape_0.8.8 ## [69] stringi_1.4.3 dygraphs_1.1.1.6 ## [71] boot_1.3-24 pkgbuild_1.0.3 ## [73] shape_1.4.4 rlang_0.4.0 ## [75] pkgconfig_2.0.2 matrixStats_0.54.0 ## [77] HDInterval_0.2.0 evaluate_0.14 ## [79] lattice_0.20-38 rstantools_1.5.1 ## [81] htmlwidgets_1.3 labeling_0.3 ## [83] processx_3.3.1 tidyselect_0.2.5 ## [85] plyr_1.8.4 magrittr_1.5 ## [87] bookdown_0.11 R6_2.4.0 ## [89] generics_0.0.2 pillar_1.4.1 ## [91] haven_2.1.0 withr_2.1.2 ## [93] xts_0.11-2 abind_1.4-5 ## [95] modelr_0.1.4 crayon_1.3.4 ## [97] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [99] rmarkdown_1.13 grid_3.6.3 ## [101] readxl_1.3.1 callr_3.2.0 ## [103] threejs_0.3.1 digest_0.6.19 ## [105] xtable_1.8-4 httpuv_1.5.1 ## [107] stats4_3.6.3 munsell_0.5.0 ## [109] shinyjs_1.0 "],
["big-entropy-and-the-generalized-linear-model.html", "9 Big Entropy and the Generalized Linear Model 9.1 Maximum entropy 9.2 Generalized linear models Reference Session info", " 9 Big Entropy and the Generalized Linear Model …Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention. To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice. When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. (p. 267) 9.1 Maximum entropy In Chapter 6, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution \\(p\\) with probabilities \\(p_i\\) for each possible event \\(i\\) turns out to be just the average log-probability: \\[H(p) = - \\sum_i p_i \\text{ log } p_i\\] This function is known as information entropy. (p. 268, emphasis in the original) Let’s execute the code for the pebbles-in-buckets example. library(tidyverse) d &lt;- tibble(a = c(0, 0, 10, 0, 0), b = c(0, 1, 8, 1, 0), c = c(0, 2, 6, 2, 0), d = c(1, 2, 4, 2, 1), e = 2) # this is our analogue to McElreath&#39;s `lapply()` code d %&gt;% mutate_all(~ . / sum(.)) %&gt;% # the next few lines constitute our analogue to his `sapply()` code gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) ## # A tibble: 5 x 2 ## key h ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0 ## 2 b 0.639 ## 3 c 0.950 ## 4 d 1.47 ## 5 e 1.61 For more on the formula syntax we used within mutate_all(), you might check out this or this. Anyway, we’re almost ready to plot. Which brings us to color. For the plots in this chapter, we’ll be taking our color palettes from the ghibli package, which provides palettes based on scenes from anime films by the Studio Ghibli. # install.packages(&quot;ghibli&quot;, dependencies = T) library(ghibli) The main function is ghibli_palette() which you can use to both preview the palettes before using them and also index in order to use specific colors. For example, we’ll play with “MarnieMedium1”, first. ghibli_palette(&quot;MarnieMedium1&quot;) ghibli_palette(&quot;MarnieMedium1&quot;)[1:7] ## [1] &quot;#7BA46C&quot; &quot;#602D31&quot; &quot;#008D91&quot; &quot;#0A789F&quot; &quot;#C6A28A&quot; &quot;#61B8D3&quot; &quot;#EACF9E&quot; Now we’re ready to plot five of the six panels of Figure 9.1. d %&gt;% mutate(bucket = 1:5) %&gt;% gather(letter, pebbles, - bucket) %&gt;% ggplot(aes(x = bucket, y = pebbles)) + geom_col(width = 1/5, fill = ghibli_palette(&quot;MarnieMedium1&quot;)[2]) + geom_text(aes(y = pebbles + 1, label = pebbles)) + geom_text(data = tibble( letter = letters[1:5], bucket = 5.5, pebbles = 10, label = str_c(c(1, 90, 1260, 37800, 113400), rep(c(&quot; way&quot;, &quot; ways&quot;), times = c(1, 4)))), aes(label = label), hjust = 1) + scale_y_continuous(breaks = c(0, 5, 10)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6]), strip.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[1])) + facet_wrap(~letter, ncol = 2) We might plot the final panel like so. d %&gt;% # the next four lines are the same from above mutate_all(~ . / sum(.)) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) %&gt;% # here&#39;s the R code 9.4 stuff mutate(n_ways = c(1, 90, 1260, 37800, 113400)) %&gt;% group_by(key) %&gt;% mutate(log_ways = log(n_ways) / 10, text_y = ifelse(key &lt; &quot;c&quot;, h + .15, h - .15)) %&gt;% # plot ggplot(aes(x = log_ways, y = h)) + geom_abline(intercept = 0, slope = 1.37, color = &quot;white&quot;) + geom_point(size = 2.5, color = ghibli_palette(&quot;MarnieMedium1&quot;)[7]) + geom_text(aes(y = text_y, label = key)) + labs(x = &quot;log(ways) per pebble&quot;, y = &quot;entropy&quot;) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6])) “The distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution” (p. 271). Among the pebbles, the maximum entropy distribution was e (i.e., the uniform). 9.1.1 Gaussian. Behold the probability distribution for the generalized normal distribution: \\[\\text{Pr} (y | \\mu, \\alpha, \\beta) = \\frac{\\beta}{2 \\alpha \\Gamma \\bigg (\\frac{1}{\\beta} \\bigg )} e ^ {- \\bigg (\\frac{|y - \\mu|}{\\alpha} \\bigg ) ^ {\\beta}}\\] In this formulation, \\(\\alpha =\\) the scale, \\(\\beta =\\) the shape, \\(\\mu =\\) the location, and \\(\\Gamma =\\) the gamma function. If you read closely in the text, you’ll discover that the densities in the right panel of Figure 9.2 were all created with the constraint \\(\\sigma^2 = 1\\). But \\(\\sigma^2 \\neq \\alpha\\) and there’s no \\(\\sigma\\) in the equations in the text. However, it appears the variance for the generalized normal distribution follows the form: \\[\\sigma^2 = \\frac{\\alpha^2 \\Gamma (3/\\beta)}{\\Gamma (1/\\beta)}\\] So if you do the algebra, you’ll see that you can compute \\(\\alpha\\) for a given \\(\\sigma^2\\) and \\(\\beta\\) like so: \\[\\alpha = \\sqrt{ \\frac{\\sigma^2 \\Gamma (1/\\beta)}{\\Gamma (3/\\beta)} }\\] I got the formula from Wikipedia.com. Don’t judge. We can wrap that formula in a custom function, alpha_per_beta(), use it to solve for the desired \\(\\beta\\) values, and plot. But one more thing: McElreath didn’t tell us exactly which \\(\\beta\\) values the left panel of Figure 9.2 was based on. So the plot below is my best guess. alpha_per_beta &lt;- function(variance, beta){ sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } tibble(mu = 0, variance = 1, # I arrived at these values by trial and error beta = c(1, 1.5, 2, 4)) %&gt;% mutate(alpha = map2(variance, beta, alpha_per_beta)) %&gt;% unnest() %&gt;% expand(nesting(mu, beta, alpha), value = seq(from = -5, to = 5, by = .1)) %&gt;% # behold the formula for the generalized normal distribution in code mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% # plot ggplot(aes(x = value, y = density, group = beta)) + geom_line(aes(color = beta == 2, size = beta == 2)) + scale_color_manual(values = c(ghibli_palette(&quot;MarnieMedium2&quot;)[2], ghibli_palette(&quot;MarnieMedium2&quot;)[4])) + scale_size_manual(values = c(1/4, 1.25)) + ggtitle(NULL, subtitle = &quot;Guess which color denotes the Gaussian.&quot;) + coord_cartesian(xlim = -4:4) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) Here’s Figure 9.2’s right panel. tibble(mu = 0, variance = 1, # this time we need a more densely-packed sequence of `beta` values beta = seq(from = 1, to = 4, length.out = 100)) %&gt;% mutate(alpha = map2(variance, beta, alpha_per_beta)) %&gt;% unnest() %&gt;% expand(nesting(mu, beta, alpha), value = -8:8) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% # this is just an abbreviated version of the formula we used in our first code block summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_vline(xintercept = 2, color = &quot;white&quot;) + geom_line(size = 2, color = ghibli_palette(&quot;MarnieMedium2&quot;)[6]) + coord_cartesian(ylim = c(1.34, 1.42)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) If you look closely, you’ll see our version doesn’t quite match up with McElreath’s. Over x-axis values of 2 to 4, they match up pretty well. But as you go from 2 to 1, you’ll see our line drops off more steeply than his did. [And no, coord_cartesian() isn’t the problem.] If you can figure out why our numbers diverged, please share the answer. But getting back on track: The take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian. (p. 274) 9.1.2 Binomial. The binomial likelihood entails counting the numbers of ways that a given observation could arise, according to assumptions… If only two things can happen (blue or white marble, for example), and there’s a constant chance \\(p\\) of each across \\(n\\) trials, then the probability of observing \\(y\\) events of type 1 and \\(n - y\\) events of type 2 is: \\[\\text{Pr} (y | n, p) = \\frac{n!}{y! (n - y)!} p^y (1 - p)^{n - y}\\] It may help to note that the fraction with the factorials is just saying how many different ordered sequences of \\(n\\) outcomes have a count of \\(y\\). (p. 275) For me, that last sentence made more sense when I walked it out in an example. To do so, lets wrap that fraction of factorials into a function. count_ways &lt;- function(n, y){ # n = the total number of trials (i.e., the number of rows in your vector) # y = the total number of 1s (i.e., successes) in your vector (factorial(n) / (factorial(y) * factorial(n - y))) } Now consider three sequences: 0, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 0\\)) 1, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 1\\)) 1, 1, 0, 0 (i.e., \\(n = 4\\) and \\(y = 2\\)) We can organize that information in a little tibble and then demo our count_ways() function. tibble(sequence = 1:3, n = 4, y = c(0, 1, 2)) %&gt;% mutate(n_ways = map2(n, y, count_ways)) %&gt;% unnest() ## # A tibble: 3 x 4 ## sequence n y n_ways ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 0 1 ## 2 2 4 1 4 ## 3 3 4 2 6 Here’s the pre-Figure 9.3 data McElreath presented at the bottom of page 275. ( d &lt;- tibble(distribution = letters[1:4], ww = c(1/4, 2/6, 1/6, 1/8), bw = c(1/4, 1/6, 2/6, 4/8), wb = c(1/4, 1/6, 2/6, 2/8), bb = c(1/4, 2/6, 1/6, 1/8)) ) ## # A tibble: 4 x 5 ## distribution ww bw wb bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.25 0.25 0.25 0.25 ## 2 b 0.333 0.167 0.167 0.333 ## 3 c 0.167 0.333 0.333 0.167 ## 4 d 0.125 0.5 0.25 0.125 Those data take just a tiny bit of wrangling before they’re ready to plot with. d %&gt;% gather(key, value, -distribution) %&gt;% mutate(key = factor(key, levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) %&gt;% ggplot(aes(x = key, y = value, group = 1)) + geom_point(size = 2, color = ghibli_palette(&quot;PonyoMedium&quot;)[4]) + geom_line(color = ghibli_palette(&quot;PonyoMedium&quot;)[5]) + coord_cartesian(ylim = 0:1) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[2]), strip.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[6])) + facet_wrap(~distribution) If we go step by step, we might count the expected value for each distribution like follows. d %&gt;% gather(sequence, probability, -distribution) %&gt;% # `str_count()` will count the number of times &quot;b&quot; occurs within a given row of `sequence` mutate(n_b = str_count(sequence, &quot;b&quot;)) %&gt;% mutate(product = probability * n_b) %&gt;% group_by(distribution) %&gt;% summarise(expected_value = sum(product)) ## # A tibble: 4 x 2 ## distribution expected_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 1 ## 3 c 1 ## 4 d 1 We can use the same gather() and group_by() strategies on the way to computing the entropies. d %&gt;% gather(sequence, probability, -distribution) %&gt;% group_by(distribution) %&gt;% summarise(entropy = -sum(probability * log(probability))) ## # A tibble: 4 x 2 ## distribution entropy ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.39 ## 2 b 1.33 ## 3 c 1.33 ## 4 d 1.21 Like in the text, distribution == &quot;a&quot; had the largest entropy of the four. In the next example, the \\(\\text{expected value} = 1.4\\) and \\(p = .7\\). p &lt;- 0.7 ( a &lt;- c((1 - p)^2, p * (1 - p), (1 - p) * p, p^2) ) ## [1] 0.09 0.21 0.21 0.49 Here’s the entropy for our distribution a. -sum(a * log(a)) ## [1] 1.221729 I’m going to alter McElreath’s simulation function from R code block 9.9 to take a seed argument. In addition, I altered the names of the objects within the function and changed the output to a tibble that will also include the conditions “ww”, “bw”, “wb”, and “bb”. sim_p &lt;- function(seed, g = 1.4) { set.seed(seed) x_123 &lt;- runif(3) x_4 &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g) z &lt;- sum(c(x_123, x_4)) p &lt;- c(x_123, x_4) / z tibble(h = -sum(p * log(p)), p = p, key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) } For a given seed and g value, our augmented sim_p() function returns a \\(4 \\times 3\\) tibble. sim_p(seed = 9.9, g = 1.4) ## # A tibble: 4 x 3 ## h p key ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1.02 0.197 ww ## 2 1.02 0.0216 bw ## 3 1.02 0.184 wb ## 4 1.02 0.597 bb So the next step is to determine how many replications we’d like, create a tibble with seed values ranging from 1 to that number, and then feed those seed values into sim_p() via purrr::map2(), which will return a nested tibble. We’ll then unnest() and take a peek. # how many replications would you like? n_rep &lt;- 1e5 d &lt;- tibble(seed = 1:n_rep) %&gt;% mutate(sim = map2(seed, 1.4, sim_p)) %&gt;% unnest() head(d) ## # A tibble: 6 x 4 ## seed h p key ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1.21 0.108 ww ## 2 1 1.21 0.151 bw ## 3 1 1.21 0.233 wb ## 4 1 1.21 0.508 bb ## 5 2 1.21 0.0674 ww ## 6 2 1.21 0.256 bw In order to intelligently choose which four replications we want to highlight in Figure 9.4, we’ll want to rank order them by entropy, h. ranked_d &lt;- d %&gt;% group_by(seed) %&gt;% arrange(desc(h)) %&gt;% ungroup() %&gt;% # here&#39;s the rank order step mutate(rank = rep(1:n_rep, each = 4)) head(ranked_d) ## # A tibble: 6 x 5 ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 ## 5 71132 1.22 0.0902 ww 2 ## 6 71132 1.22 0.210 bw 2 And we’ll also want a subset of the data to correspond to McElreath’s “A” through “D” distributions. subset_d &lt;- ranked_d %&gt;% # I arrived at these `rank` values by trial and error filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% # I arrived at the `height` values by trial and error, too mutate(height = rep(c(8, 2.25, .75, .5), each = 4), distribution = rep(letters[1:4], each = 4)) head(subset_d) ## # A tibble: 6 x 7 ## seed h p key rank height distribution ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 55665 1.22 0.0903 ww 1 8 a ## 2 55665 1.22 0.209 bw 1 8 a ## 3 55665 1.22 0.210 wb 1 8 a ## 4 55665 1.22 0.490 bb 1 8 a ## 5 50981 1.000 0.0459 ww 87373 2.25 b ## 6 50981 1.000 0.0459 bw 87373 2.25 b We’re finally ready to plot the left panel of Figure 9.4. d %&gt;% ggplot(aes(x = h)) + geom_density(size = 0, fill = ghibli_palette(&quot;LaputaMedium&quot;)[3], adjust = 1/4) + # note the data statements for the next two geoms geom_linerange(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(ymin = 0, ymax = height), color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_text(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(y = height + .5, label = distribution)) + scale_x_continuous(&quot;Entropy&quot;, breaks = seq(from = .7, to = 1.2, by = .1)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7])) Did you notice how our adjust = 1/4 with geom_density() served a similar function to the adj=0.1 in McElreath’s dens() code. Anyways, here’s the right panel. ranked_d %&gt;% filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% mutate(distribution = rep(letters[1:4], each = 4)) %&gt;% ggplot(aes(x = key, y = p, group = 1)) + geom_line(color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_point(size = 2, color = ghibli_palette(&quot;LaputaMedium&quot;)[4]) + coord_cartesian(ylim = 0:1) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7]), strip.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[6])) + facet_wrap(~distribution) Because we were simulating, our values won’t match up identically with those in the text. But we’re pretty close, eh? Since we saved our sim_p() output in a nested tibble, which we then unnested(), there’s no need to separate the entropy values from the distributional values the way McElreath did in R code 9.11. If we wanted to determine our highest entropy value–and the corresponding seed and p values, while we’re at it–, we might use max(h) within slice(). ranked_d %&gt;% group_by(key) %&gt;% slice(max(h)) ## # A tibble: 4 x 5 ## # Groups: key [4] ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 That maximum h value matched up nicely with the one in the text. If you look at the p column, you’ll see our values approximated McElreath’s distribution values, too. In both cases, they’re real close to the a values we computed, above. a ## [1] 0.09 0.21 0.21 0.49 9.2 Generalized linear models For an outcome variable that is continuous and far from any theoretical maximum or minimum, [a simple] Gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. (p. 280) I winged the values for our Figure 9.5. tibble(x = seq(from = -1, to = 3, by = .01)) %&gt;% mutate(probability = .35 + x * .5) %&gt;% ggplot(aes(x = x, y = probability)) + geom_rect(xmin = -1, xmax = 3, ymin = 0, ymax = 1, fill = ghibli_palette(&quot;MononokeMedium&quot;)[5]) + geom_hline(yintercept = 0:1, linetype = 2, color = ghibli_palette(&quot;MononokeMedium&quot;)[7]) + geom_line(aes(linetype = probability &gt; 1, color = probability &gt; 1), size = 1) + geom_segment(x = 1.3, xend = 3, y = 1, yend = 1, size = 2/3, color = ghibli_palette(&quot;MononokeMedium&quot;)[3]) + scale_color_manual(values = c(ghibli_palette(&quot;MononokeMedium&quot;)[3], ghibli_palette(&quot;MononokeMedium&quot;)[7])) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(xlim = 0:2, ylim = c(0, 1.2)) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MononokeMedium&quot;)[1])) For a count outcome \\(y\\) for which each observation arises from \\(n\\) trials and with constant expected value \\(np\\), the binomial distribution has maximum entropy. So it’s the least informative distribution that satisfies our prior knowledge of the outcomes \\(y\\). (p. 281) The binomial model follows the basic form \\[\\begin{align*} y_i &amp; \\sim \\text{Binomial} (n, p_i) \\\\ f(p_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] The \\(f()\\) portion of the second line represents the link function. We need the link function because, though the shape of the Binomial distribution is determined by two parameters–\\(n\\) and \\(p\\)–, neither is equivalent to the Gaussian mean \\(\\mu\\). The mean outcome, rather, is \\(np\\)–a function of both. The link function also ensures the model doesn’t make probability predictions outside of the boundary \\([0, 1]\\). Let’s get more general. 9.2.1 Meet the family. Here are the Gamma and Exponential panels for Figure 9.6. length_out &lt;- 100 tibble(x = seq(from = 0, to = 5, length.out = length_out)) %&gt;% mutate(Gamma = dgamma(x, 2, 2), Exponential = dexp(x)) %&gt;% gather(key, density, -x) %&gt;% mutate(label = rep(c(&quot;y %~% Gamma(lambda, kappa)&quot;, &quot;y %~% Exponential(lambda)&quot;), each = n()/2)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~label, scales = &quot;free_y&quot;, labeller = label_parsed) The Gaussian: length_out &lt;- 100 tibble(x = seq(from = -5, to = 5, length.out = length_out)) %&gt;% mutate(density = dnorm(x), strip = &quot;y %~% Normal(mu, sigma)&quot;) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -4:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Here is the Poisson. length_out &lt;- 100 tibble(x = 0:20) %&gt;% mutate(density = dpois(x, lambda = 2.5), strip = &quot;y %~% Poisson(lambda)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Finally, the Binomial: length_out &lt;- 100 tibble(x = 0:10) %&gt;% mutate(density = dbinom(x, size = 10, prob = .85), strip = &quot;y %~% Binomial(n, p)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) 9.2.1.1 Rethinking: A likelihood is a prior. In traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” However, likelihoods are themselves prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon context. (p. 284) For a little more in this, check out McElreath’s great lecture, Bayesian Statistics without Frequentist Language. This subsection also reminds me of the title of one of Gelman’s blog posts, “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”. The title, which itself is a quote, comes from one of his papers, which he linked to in the blog, along with several related papers. It’s taken some time for the weight of that quote to sink in with me, and indeed it’s still sinking. Perhaps you’ll benefit from it, too. 9.2.2 Linking linear models to distributions. To build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a link function to prevent mathematical accidents like negative distances or probability masses that exceed 1. (p. 284) These models generally follow the form \\[\\begin{align*} y_i &amp; \\sim \\text{Some distribution} (\\theta_i, \\phi) \\\\ f(\\theta_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] where \\(\\theta_i\\) is a parameter of central interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not of primary substantive interest (e.g., \\(\\sigma\\) in work-a-day Gaussian models). And as stated earlier, \\(f()\\) is the link function. Speaking of, the logit link maps a parameter that is defined as a probability mass and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this: \\[\\begin{align*} y_i &amp; \\sim \\text{Binomial}(n, p_i)\\\\ \\text{logit}(p_i) &amp; = \\alpha+\\beta x_i \\end{align*}\\] And the logit function itself is defined as the log-odds: \\[\\text{logit} (p_i) = \\text{log} \\frac{p_i}{1 - p_i}\\] The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is: \\[\\text{log} \\frac{p_i}{1 - p_i} = \\alpha + \\beta x_i\\] If we do the final algebraic manipulation on page 285, we can solve for \\(p_i\\) in terms of the linear model: \\[p_i = \\frac{\\text{exp} (\\alpha + \\beta x_i)}{1 + \\text{exp} (\\alpha + \\beta x_i)}\\] As we’ll see later, we will make great use of this formula via the brms::inv_logit_scaled() when making sense of logistic regression models. Now we have that last formula in hand, we can make the data necessary for Figure 9.7. # first, we&#39;ll make data for the horizontal lines alpha &lt;- 0 beta &lt;- 4 lines &lt;- tibble(x = seq(from = -1, to = 1, by = .25)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we&#39;re ready to make the primary data beta &lt;- 2 d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-odds`)) + geom_hline(data = lines, aes(yintercept = `log-odds`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = probability)) + geom_hline(data = lines, aes(yintercept = probability), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory library(gridExtra) grid.arrange(p1, p2, ncol = 2) The key lesson for now is just that no regression coefficient, such as \\(\\beta\\), from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 7) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change… The second very common link function is the log link. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation of \\(\\sigma\\) of a Gaussian distribution so it is a function of a predictor variable \\(x\\). The parameter \\(\\sigma\\) must be positive, because a standard deviation cannot be negative no can it be zero. The model might look like: \\[\\begin{align*} y_i &amp; \\sim \\text{Normal} (\\mu, \\sigma_i) \\\\ \\text{log} (\\sigma_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] In this model, the mean \\(\\mu\\) is constant, but the standard deviation scales with the value \\(x_i\\). (p. 268) This kind of model is trivial in the brms framework, which you can learn more about in Bürkner’s vignette Estimating Distributional Models with brms. Before moving on with the text, let’s detour and see how we might fit such a model. First, let’s simulate some continuous data y for which the \\(SD\\) is effected by a dummy variable x. set.seed(9) ( d &lt;- tibble(x = rep(0:1, each = 100)) %&gt;% mutate(y = rnorm(n = n(), mean = 100, sd = 10 + x * 10)) ) ## # A tibble: 200 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 0 92.3 ## 2 0 91.8 ## 3 0 98.6 ## 4 0 97.2 ## 5 0 104. ## 6 0 88.1 ## 7 0 112. ## 8 0 99.8 ## 9 0 97.5 ## 10 0 96.4 ## # … with 190 more rows We can view what data like these look like with aid from tidybayes::geom_halfeyeh(). library(tidybayes) d %&gt;% mutate(x = x %&gt;% as.character()) %&gt;% ggplot(aes(x = y, y = x, fill = x)) + geom_halfeyeh(color = ghibli_palette(&quot;KikiMedium&quot;)[2], point_interval = mean_qi, .width = .68) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[4], ghibli_palette(&quot;KikiMedium&quot;)[6])) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) Even though the means of y are the same for both levels of the x dummy, the variance for x == 1 is substantially larger than that for x == 0. Let’s open brms. library(brms) For such a model, we have two formulas: one for \\(\\mu\\) and one for \\(\\sigma\\). We wrap both within the bf() function. b9.1 &lt;- brm(data = d, family = gaussian, bf(y ~ 1, sigma ~ 1 + x), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = Intercept, dpar = sigma), prior(normal(0, 10), class = b, dpar = sigma)), seed = 9) Do note our use of the dpar arguments in the prior statements. Here’s the summary. print(b9.1) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 1 ## sigma ~ 1 + x ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 99.06 0.86 97.42 100.76 3714 1.00 ## sigma_Intercept 2.27 0.07 2.13 2.41 3455 1.00 ## sigma_x 0.72 0.10 0.53 0.92 3389 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we get an intercept for both \\(\\mu\\) and \\(\\sigma\\), with the intercept for sigma identified as sigma_Intercept. And note the coefficient for \\(\\sigma\\) was named sigma_x. Also notice the scale the sigma_ coefficients are on. These are not in the original metric, but rather based on log(). You can confirm that by the second line of the print() output: Links: mu = identity; sigma = log. So if you want to get a sense of the effects of x on the \\(\\sigma\\) for y, you have to exponentiate the formula. Here we’ll do so with the posterior_samples(). post &lt;- posterior_samples(b9.1) head(post) ## b_Intercept b_sigma_Intercept b_sigma_x lp__ ## 1 98.92777 2.210513 0.7128191 -817.9633 ## 2 97.20487 2.224788 0.7664619 -820.0208 ## 3 98.41696 2.209590 0.7179606 -818.2172 ## 4 99.81492 2.236576 0.8748752 -819.2368 ## 5 100.28893 2.309400 0.7424112 -819.0053 ## 6 98.39117 2.536367 0.4429144 -824.0089 With the samples in hand, we’ll use the model formula to compute the model-implied standard deviations of y based on the x dummy and then examine them in a plot. post %&gt;% transmute(`x == 0` = exp(b_sigma_Intercept + b_sigma_x * 0), `x == 1` = exp(b_sigma_Intercept + b_sigma_x * 1)) %&gt;% gather(key, sd) %&gt;% ggplot(aes(x = sd, y = key, fill = key)) + geom_halfeyeh(color = ghibli_palette(&quot;KikiMedium&quot;)[2], point_interval = median_qi, .width = .95) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[4], ghibli_palette(&quot;KikiMedium&quot;)[6])) + labs(x = expression(sigma[x]), y = NULL, subtitle = expression(paste(&quot;Model-implied &quot;, italic(SD), &quot;s by group x&quot;))) + coord_cartesian(ylim = c(1.5, 2)) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) And if we looked back at the data, those \\(SD\\) estimates are just what we’d expect. d %&gt;% group_by(x) %&gt;% summarise(sd = sd(y) %&gt;% round(digits = 1)) ## # A tibble: 2 x 2 ## x sd ## &lt;int&gt; &lt;dbl&gt; ## 1 0 9.6 ## 2 1 19.8 For more on models like this, check out Christakis’ 2014: What scientific idea is ready for retirement? or The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright. Kruschke also covered modeling \\(\\sigma\\) a bit in his Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan. Finally, this is foreshadowing a bit because it requires the multilevel model (see Chapters 12 and 13), but you might also check out the preprint by Williams, Liu, Martin, and Rast, Bayesian Multivariate Mixed-Effects Location Scale Modeling of Longitudinal Relations among Affective Traits, States, and Physical Activity. But getting back to the text, what the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving \\(\\text{log} (\\sigma_i) = \\alpha + \\beta x_i\\) for \\(\\sigma_i\\) yields the inverse link: \\[\\sigma_i = \\text{exp} (\\alpha + \\beta x_i)\\] The impact of this assumption can be seen in [our version of] Figure 9.8. (pp. 286—287) # first, we&#39;ll make data that&#39;ll be make the horizontal lines alpha &lt;- 0 beta &lt;- 2 lines &lt;- tibble(`log-measurement` = -3:3) %&gt;% mutate(`original measurement` = exp(`log-measurement`)) # now we&#39;re ready to make the primary data d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-measurement` = alpha + x * beta, `original measurement` = exp(alpha + x * beta)) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-measurement`)) + geom_hline(data = lines, aes(yintercept = `log-measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = `original measurement`)) + geom_hline(data = lines, aes(yintercept = `original measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1, ylim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory grid.arrange(p1, p2, ncol = 2) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 tidybayes_1.1.0 gridExtra_2.3 ## [5] ghibli_0.2.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [9] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ## [13] ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.13 ggstance_0.3.1 ## [5] markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 ## [9] rstan_2.18.2 svUnit_0.7-12 ## [11] DT_0.7 fansi_0.4.0 ## [13] mvtnorm_1.0-10 lubridate_1.7.4 ## [15] xml2_1.2.0 codetools_0.2-16 ## [17] bridgesampling_0.6-0 knitr_1.23 ## [19] shinythemes_1.1.2 zeallot_0.1.0 ## [21] bayesplot_1.7.0 jsonlite_1.6 ## [23] broom_0.5.2 shiny_1.3.2 ## [25] compiler_3.6.3 httr_1.4.0 ## [27] backports_1.1.4 assertthat_0.2.1 ## [29] Matrix_1.2-17 lazyeval_0.2.2 ## [31] cli_1.1.0 later_0.8.0 ## [33] htmltools_0.3.6 prettyunits_1.0.2 ## [35] tools_3.6.3 igraph_1.2.4.1 ## [37] coda_0.19-2 gtable_0.3.0 ## [39] glue_1.3.1 reshape2_1.4.3 ## [41] cellranger_1.1.0 vctrs_0.1.0 ## [43] nlme_3.1-144 crosstalk_1.0.0 ## [45] xfun_0.7 ps_1.3.0 ## [47] rvest_0.3.4 mime_0.7 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 ## [51] gtools_3.8.1 zoo_1.8-6 ## [53] scales_1.1.1.9000 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 ## [57] Brobdingnag_1.2-6 parallel_3.6.3 ## [59] inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.0 StanHeaders_2.18.1 ## [63] loo_2.1.0 stringi_1.4.3 ## [65] dygraphs_1.1.1.6 pkgbuild_1.0.3 ## [67] rlang_0.4.0 pkgconfig_2.0.2 ## [69] matrixStats_0.54.0 evaluate_0.14 ## [71] lattice_0.20-38 rstantools_1.5.1 ## [73] htmlwidgets_1.3 labeling_0.3 ## [75] tidyselect_0.2.5 processx_3.3.1 ## [77] plyr_1.8.4 magrittr_1.5 ## [79] bookdown_0.11 R6_2.4.0 ## [81] generics_0.0.2 pillar_1.4.1 ## [83] haven_2.1.0 withr_2.1.2 ## [85] xts_0.11-2 abind_1.4-5 ## [87] modelr_0.1.4 crayon_1.3.4 ## [89] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_1.13 grid_3.6.3 ## [93] readxl_1.3.1 callr_3.2.0 ## [95] threejs_0.3.1 digest_0.6.19 ## [97] xtable_1.8-4 httpuv_1.5.1 ## [99] stats4_3.6.3 munsell_0.5.0 ## [101] shinyjs_1.0 "],
["counting-and-classification.html", "10 Counting and Classification 10.1 Binomial regression 10.2 Poisson regression 10.3 Other count regressions Reference Session info", " 10 Counting and Classification All over the world, every day, scientists throw away information. Sometimes this is through the removal of “outliers,” cases in the data that offend the model and are exiled. More routinely, counted things are converted to proportions before analysis. Why does analysis of proportions throw away information? Because 10/20 and ½ are the same proportion, one-half, but have very different sample sizes. Once converted to proportions, and treated as outcomes in a linear regression, the information about sample size has been destroyed. It’s easy to retain the information about sample size. All that is needed is to model what has actually been observed, the counts instead of the proportions. (p. 291) In this chapter, we focus on the two most common types of count models: the binomial and the Poisson. Side note: For a nice Bayesian way to accommodate outliers in your Gaussian models, check out my blog Robust Linear Regression with Student’s \\(t\\)-Distribution. 10.1 Binomial regression The basic binomial model follows the form \\[y \\sim \\text{Binomial} (n, p)\\] where \\(y\\) is some count variable, \\(n\\) is the number of trials, and \\(p\\) it the probability a given trial was a 1, which is sometimes termed a success. When \\(n = 1\\), then \\(y\\) is a vector of 0s and 1s. Presuming the logit link, models of this type are commonly termed logistic regression. When \\(n &gt; 1\\), and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model. 10.1.1 Logistic regression: Prosocial chimpanzees. Load the chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(chimpanzees) We start with the simple intercept-only logistic regression model, which follows the statistical formula \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] In the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above. b10.1 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 10), class = Intercept), seed = 10) You might use fixef() to get a focused summary of the intercept. library(tidyverse) fixef(b10.1) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.32 0.09 0.14 0.5 The brms::inv_logit_scaled() function will be our alternative to the logistic() function in rethinking. c(.18, .46) %&gt;% inv_logit_scaled() ## [1] 0.5448789 0.6130142 fixef(b10.1) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.578806 0.5231408 0.5349467 0.6229265 With the next two chimp models, we add predictors in the usual way. b10.2 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), seed = 10) b10.3 &lt;- update(b10.2, newdata = d, formula = pulled_left | trials(1) ~ 1 + prosoc_left + condition:prosoc_left) Compute the WAIC for each model and save the results within the brmfit objects. b10.1 &lt;- add_criterion(b10.1, &quot;waic&quot;) b10.2 &lt;- add_criterion(b10.2, &quot;waic&quot;) b10.3 &lt;- add_criterion(b10.3, &quot;waic&quot;) Compare them with the loo_compare() and make sure to add the criterion = &quot;waic&quot; argument. w &lt;- loo_compare(b10.1, b10.2, b10.3, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b10.2 0.0 0.0 -340.3 4.7 2.1 0.0 680.6 9.4 ## b10.3 -1.0 0.4 -341.3 4.7 3.1 0.1 682.6 9.5 ## b10.1 -3.7 3.1 -344.0 3.5 1.1 0.0 688.1 7.1 Recall our cbind() trick to convert the differences from the \\(\\text{elpd}\\) metric to the WAIC metric. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) %&gt;% round(digits = 2) ## waic_diff se ## b10.2 0.00 0.00 ## b10.3 1.98 0.81 ## b10.1 7.42 6.21 For this chapter, we’ll take our color scheme from the wesanderson package’s Moonrise2 palette. # install.packages(&quot;wesanderson&quot;, dependencies = T) library(wesanderson) wes_palette(&quot;Moonrise2&quot;) wes_palette(&quot;Moonrise2&quot;)[1:4] ## [1] &quot;#798E87&quot; &quot;#C27D38&quot; &quot;#CCC591&quot; &quot;#29211F&quot; We’ll also take a few formatting cues from Edward Tufte, curtesy of the ggthemes package. The theme_tufte() function will change the default font and remove some chart junk. The theme_set() function, below, will make these adjustments the default for all subsequent ggplot2 plots. To undo this, just execute theme_set(theme_default()). library(ggthemes) library(bayesplot) theme_set(theme_default() + theme_tufte() + theme(plot.background = element_rect(fill = wes_palette(&quot;Moonrise2&quot;)[3], color = wes_palette(&quot;Moonrise2&quot;)[3]))) Finally, here’s our WAIC plot. w %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model&quot;) %&gt;% ggplot() + geom_pointrange(aes(x = reorder(model, -waic), y = waic, ymin = waic - se_waic, ymax = waic + se_waic, color = model), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1:2, 4)]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;WAIC&quot;) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) The full model, b10.3, did not have the lowest WAIC value. Though note how wide those standard error bars are relative to the point estimates. There’s a lot of model uncertainty there. Here are the WAIC weights. model_weights(b10.1, b10.2, b10.3, weights = &quot;waic&quot;) ## b10.1 b10.2 b10.3 ## 0.01752164 0.71641630 0.26606206 Let’s look at the parameter summaries for the theory-based model. print(b10.3) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ prosoc_left + prosoc_left:condition ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.05 0.13 -0.21 0.31 3031 1.00 ## prosoc_left 0.61 0.23 0.17 1.05 2607 1.00 ## prosoc_left:condition -0.10 0.27 -0.63 0.43 2575 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the odds are multiplied by: fixef(b10.3)[2] %&gt;% exp() ## [1] 1.847029 Given an estimated value of 4, the probability of a pull, all else equal, would be close to 1. inv_logit_scaled(4) ## [1] 0.9820138 Adding the coefficient, fixef(b10.3)[2], would yield an even higher estimate. (4 + fixef(b10.3)[2]) %&gt;% inv_logit_scaled() ## [1] 0.9901811 For our variant of Figure 10.2, we use brms::pp_average() in place of rethinking::ensemble(). # the combined `fitted()` results of the three models weighted by their WAICs ppa &lt;- pp_average(b10.1, b10.2, b10.3, weights = &quot;waic&quot;, method = &quot;fitted&quot;) %&gt;% as_tibble() %&gt;% bind_cols(b10.3$data) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # the empirically-based summaries d_plot &lt;- d %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(pulled_left = mean(pulled_left)) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # the plot ppa %&gt;% ggplot(aes(x = x_axis)) + geom_smooth(aes(y = pulled_left, ymin = Q2.5, ymax = Q97.5, group = 0), stat = &quot;identity&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[2], color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_line(data = d_plot, aes(y = pulled_left, group = actor), color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/3) + scale_x_discrete(expand = c(.03, .03)) + coord_cartesian(ylim = 0:1) + labs(x = &quot;prosoc_left/condition&quot;, y = &quot;proportion pulled left&quot;) + theme(axis.ticks.x = element_blank()) McElreath didn’t show the actual pairs plot in the text. Here’s ours using mcmc_pairs(). # this helps us set our custom color scheme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[3], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) # the actual plot mcmc_pairs(x = posterior_samples(b10.3), pars = c(&quot;b_Intercept&quot;, &quot;b_prosoc_left&quot;, &quot;b_prosoc_left:condition&quot;), off_diag_args = list(size = 1/10, alpha = 1/6), diag_fun = &quot;dens&quot;) As McElreath observed, the posterior looks multivariate Gaussian. In equations, the next model follows the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{actor}} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] Enclosing the actor variable within factor() will produce the indexing we need to get actor-specific intercepts. Also notice we’re using the 0 + factor(actor) part of the model formula to suppress the brms default intercept. As such, the priors for all parameters in the model will be of class = b. And since we’re using the same Gaussian prior for each, we only need one line for the prior argument. b10.4 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left , prior(normal(0, 10), class = b), iter = 2500, warmup = 500, chains = 2, cores = 2, control = list(adapt_delta = 0.9), seed = 10) Within the tidyverse, distinct() yields the information you’d otherwise get from unique(). d %&gt;% distinct(actor) ## actor ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 We have no need to use something like depth=2 for our posterior summary. print(b10.4) ## Warning: There were 2 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left ## Data: d (Number of observations: 504) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## factoractor1 -0.74 0.27 -1.28 -0.21 3322 1.00 ## factoractor2 10.69 5.27 4.07 23.93 1589 1.00 ## factoractor3 -1.06 0.28 -1.60 -0.51 3587 1.00 ## factoractor4 -1.05 0.28 -1.64 -0.51 3340 1.00 ## factoractor5 -0.74 0.27 -1.29 -0.23 3683 1.00 ## factoractor6 0.22 0.27 -0.30 0.74 3578 1.00 ## factoractor7 1.81 0.40 1.08 2.63 3724 1.00 ## prosoc_left 0.83 0.26 0.33 1.35 2283 1.00 ## prosoc_left:condition -0.13 0.30 -0.72 0.44 3284 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Correspondingly, brms::posterior_samples() returns an object for b10.4 that doesn’t quite follow the same structure as from rethinking::extract.samples(). We just have a typical 2-dimensional data frame. post &lt;- posterior_samples(b10.4) post %&gt;% glimpse() ## Observations: 4,000 ## Variables: 10 ## $ b_factoractor1 &lt;dbl&gt; -0.6249345, -0.7478057, -0.7415190, -0.7592423, -0.9709409, -0.6540401, -… ## $ b_factoractor2 &lt;dbl&gt; 13.383148, 7.497157, 6.433093, 6.500553, 6.823564, 5.992749, 8.000636, 10… ## $ b_factoractor3 &lt;dbl&gt; -0.6887826, -1.4565022, -1.3877612, -1.5074418, -1.4247839, -0.9429962, -… ## $ b_factoractor4 &lt;dbl&gt; -0.7747454, -1.2061288, -1.1664359, -1.1308661, -1.0699072, -1.1439220, -… ## $ b_factoractor5 &lt;dbl&gt; -0.1975562, -1.2977602, -1.3162522, -1.2186394, -0.9225723, -0.7272301, -… ## $ b_factoractor6 &lt;dbl&gt; -0.17577770, 0.68844247, 0.76287267, 0.88287084, 0.63865462, -0.39060396,… ## $ b_factoractor7 &lt;dbl&gt; 1.425294, 2.094121, 2.194396, 1.638241, 1.425664, 1.973291, 1.567078, 1.9… ## $ b_prosoc_left &lt;dbl&gt; 0.4627765, 0.8767337, 0.8756102, 1.1859353, 1.0564925, 0.9258368, 0.54429… ## $ `b_prosoc_left:condition` &lt;dbl&gt; -0.033540094, -0.152316937, -0.215346334, -0.107831118, -0.244684340, -0.… ## $ lp__ &lt;dbl&gt; -292.2600, -291.9762, -292.6513, -293.7977, -289.3733, -289.3551, -287.26… Our variant of Figure 10.3: post %&gt;% ggplot(aes(x = b_factoractor2)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + scale_y_continuous(NULL, breaks = NULL) + labs(x = NULL, title = &quot;Actor 2&#39;s large and uncertain intercept&quot;, subtitle = &quot;Once your log-odds are above, like, 4, it&#39;s all\\npretty much a probability of 1.&quot;) Figure 10.4. shows the idiographic trajectories for four of our chimps. # subset the `d_plot` data d_plot_4 &lt;- d_plot %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor)) # compute the model-implied estimates with `fitted()` and wrangle f &lt;- fitted(b10.4) %&gt;% as_tibble() %&gt;% bind_cols(b10.4$data) %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left, actor) %&gt;% select(actor, everything()) %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor), x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # plot f %&gt;% ggplot(aes(x = x_axis, y = pulled_left, group = actor)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[2], color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_line(data = d_plot_4, color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1.25) + scale_x_discrete(expand = c(.03, .03)) + coord_cartesian(ylim = 0:1) + labs(x = &quot;prosoc_left/condition&quot;, y = &quot;proportion pulled left&quot;) + theme(axis.ticks.x = element_blank(), # color came from: http://www.color-hex.com/color/ccc591 panel.background = element_rect(fill = &quot;#d1ca9c&quot;, color = &quot;transparent&quot;)) + facet_wrap(~actor) 10.1.1.1 Overthinking: Using the by group_by() function. Let’s work within the tidyverse, instead. If you wanted to compute the proportion of trials pulled_left == 1 for each combination of prosoc_left, condition, and chimp actor, you’d put those last three variables within group_by() and then compute the mean() of pulled_left within summarise(). d %&gt;% group_by(prosoc_left, condition, actor) %&gt;% summarise(`proportion pulled_left` = mean(pulled_left)) ## # A tibble: 28 x 4 ## # Groups: prosoc_left, condition [4] ## prosoc_left condition actor `proportion pulled_left` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0 1 0.333 ## 2 0 0 2 1 ## 3 0 0 3 0.278 ## 4 0 0 4 0.333 ## 5 0 0 5 0.333 ## 6 0 0 6 0.778 ## 7 0 0 7 0.778 ## 8 0 1 1 0.278 ## 9 0 1 2 1 ## 10 0 1 3 0.167 ## # … with 18 more rows And since we’re working within the tidyverse, that operation returns a tibble rather than a list. 10.1.2 Aggregated binomial: Chimpanzees again, condensed. With the tidyverse, we use group_by() and summarise() to achieve what McElreath did with aggregate(). d_aggregated &lt;- d %&gt;% select(-recipient, -block, -trial, -chose_prosoc) %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(x = sum(pulled_left)) d_aggregated %&gt;% filter(actor %in% c(1, 2)) ## # A tibble: 8 x 4 ## # Groups: actor, condition [4] ## actor condition prosoc_left x ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 6 ## 2 1 0 1 9 ## 3 1 1 0 5 ## 4 1 1 1 10 ## 5 2 0 0 18 ## 6 2 0 1 18 ## 7 2 1 0 18 ## 8 2 1 1 18 To fit an aggregated binomial model in brms, we augment the &lt;criterion&gt; | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing \\(n\\). Either way, at least some of those trials will have an \\(n &gt; 1\\). b10.5 &lt;- brm(data = d_aggregated, family = binomial, x | trials(18) ~ 1 + prosoc_left + condition:prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) We might compare b10.3 with b10.5 like this. fixef(b10.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.13 -0.21 0.31 ## prosoc_left 0.61 0.23 0.17 1.05 ## prosoc_left:condition -0.10 0.27 -0.63 0.43 fixef(b10.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.13 -0.20 0.31 ## prosoc_left 0.60 0.23 0.14 1.07 ## prosoc_left:condition -0.09 0.27 -0.60 0.46 A coefficient plot can offer a complimentary perspective. library(broom) # wrangle tibble(model = str_c(&quot;b10.&quot;, c(3, 5))) %&gt;% mutate(fit = map(model, get)) %&gt;% mutate(tidy = map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% # plot ggplot() + geom_pointrange(aes(x = model, y = estimate, ymin = lower, ymax = upper, color = term), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1:2, 4)]) + coord_flip() + labs(x = NULL, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~term, ncol = 1) The two are close within simulation error. 10.1.3 Aggregated binomial: Graduate school admissions. Load the infamous UCBadmit data. # detach(package:brms) library(rethinking) data(UCBadmit) d &lt;- UCBadmit Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(UCBadmit) d ## dept applicant.gender admit reject applications ## 1 A male 512 313 825 ## 2 A female 89 19 108 ## 3 B male 353 207 560 ## 4 B female 17 8 25 ## 5 C male 120 205 325 ## 6 C female 202 391 593 ## 7 D male 138 279 417 ## 8 D female 131 244 375 ## 9 E male 53 138 191 ## 10 E female 94 299 393 ## 11 F male 22 351 373 ## 12 F female 24 317 341 Now compute our newly-constructed dummy variable, male. d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0)) The univariable logistic model with male as the sole predictor of admit follows the form \\[\\begin{align*} n_{\\text{admit}_i} &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\beta \\text{male}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] The second model omits the male predictor. b10.6 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male , prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) b10.7 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 10), class = Intercept), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) Compute the information criteria for each model and save the results within the brmfit objects. b10.6 &lt;- add_criterion(b10.6, &quot;waic&quot;) b10.7 &lt;- add_criterion(b10.7, &quot;waic&quot;) Here’s the WAIC comparison. w &lt;- loo_compare(b10.6, b10.7, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b10.6 0.0 0.0 -498.0 164.3 116.4 41.7 996.0 328.5 ## b10.7 -28.6 82.6 -526.6 165.2 88.1 38.4 1053.2 330.4 If you prefer the difference in the WAIC metric, use our cbind() conversion method from above. Bonus: Information criteria digression. Let’s see what happens if we switch to the LOO. b10.6 &lt;- add_criterion(b10.6, &quot;loo&quot;) ## Warning: Found 7 observations with a pareto_k &gt; 0.7 in model &#39;b10.6&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 7 times to compute the ELPDs for the problematic observations directly. b10.7 &lt;- add_criterion(b10.7, &quot;loo&quot;) ## Warning: Found 5 observations with a pareto_k &gt; 0.7 in model &#39;b10.7&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 5 times to compute the ELPDs for the problematic observations directly. If you just ape the text and use the WAIC, everything appears fine. But holy smokes look at those nasty warning messages from the loo()! One of the frightening but ultimately handy things about working with the PSIS-LOO is that it requires we estimate a Pareto \\(k\\) parameter, which you can learn all about in the loo-package section of the loo reference manual. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)s are low. The makers of the loo package get worried when those \\(k\\)s exceed 0.7 and as a result, loo() spits out a warning message when they do. First things first, if you explicitly open the loo package, you’ll have access to some handy diagnostic functions. library(loo) We’ll be leveraging those \\(k\\) values with the pareto_k_table() and pareto_k_ids() functions. Both functions take objects created by the loo() or psis() functions. So, before we can get busy, we’ll first make two objects with the loo(). l_b10.6 &lt;- loo(b10.6) ## Warning: Found 7 observations with a pareto_k &gt; 0.7 in model &#39;b10.6&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 7 times to compute the ELPDs for the problematic observations directly. l_b10.7 &lt;- loo(b10.7) ## Warning: Found 5 observations with a pareto_k &gt; 0.7 in model &#39;b10.7&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 5 times to compute the ELPDs for the problematic observations directly. There are those warning messages, again. Using the loo-object for model b10.6, which we’ve named l_b10.6, let’s take a look at the pareto_k_table() function. pareto_k_table(l_b10.6) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 4 33.3% 462 ## (0.5, 0.7] (ok) 1 8.3% 132 ## (0.7, 1] (bad) 2 16.7% 68 ## (1, Inf) (very bad) 5 41.7% 2 You may have noticed that this same table pops out when you just do something like loo(b10.6). Recall that this data set has 12 observations (i.e., execute count(d)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like nice and low \\(k\\)s. In this example, our observations are all over the place, with 5 in the “bad” \\(k\\) range We can take a closer look like this: plot(l_b10.6) So when you plot() a loo object, you get a nice diagnostic plot for those \\(k\\) values, ordered by observation number. Our plot indicates cases 1, 2, 3, 11, and 12 had “very bad” \\(k\\) values for this model. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function. pareto_k_ids(l_b10.6, threshold = 1) ## [1] 1 2 3 11 12 Note our use of the threshold argument. Play around with it to see how it works. If you want an explicit look at those \\(k\\) values, you do: l_b10.6$diagnostics ## $pareto_k ## [1] 2.67957810 1.11451857 1.88623531 0.09922047 0.39156088 0.68199122 0.72578896 0.47271647 0.43565612 ## [10] 0.71024673 1.95868789 1.78759806 ## ## $n_eff ## [1] 2.342374 13.293075 3.608359 1867.198063 839.806222 131.652439 68.013983 548.286594 ## [9] 461.687490 154.225368 2.546589 6.994980 The pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_{i}\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in this paper and in this presentation by Aki Vehtari. Anyway, the implication of all this is these values suggest model b10.6 isn’t a great fit for these data. Part of the warning message for model b10.6 read: It is recommended to set ‘reloo = TRUE’ in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model [\\(n\\)] times to compute the ELPDs for the problematic observations directly. Let’s do that. l_b10.6_reloo &lt;- loo(b10.6, reloo = T) Check the results. l_b10.6_reloo ## ## Computed from 4000 by 12 log-likelihood matrix ## ## Estimate SE ## elpd_loo -512.1 167.1 ## p_loo 130.6 48.3 ## looic 1024.2 334.2 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 11 91.7% 2 ## (0.5, 0.7] (ok) 1 8.3% 132 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Now that looks better. We’ll do the same thing for model b10.7. l_b10.7_reloo &lt;- loo(b10.7, reloo = T) Okay, let’s compare models with formal \\(\\text{elpd}_{\\text{loo}}\\) differences before and after adjusting with reloo = T. loo_compare(l_b10.6, l_b10.7) ## elpd_diff se_diff ## b10.6 0.0 0.0 ## b10.7 -29.5 77.8 loo_compare(l_b10.6_reloo, l_b10.7_reloo) ## elpd_diff se_diff ## b10.6 0.0 0.0 ## b10.7 -20.1 79.3 In this case, the results are kinda similar. The standard errors for the differences are huge compared to the point estimates, suggesting large uncertainty. Watch out for this in your real-world data. But this has all been a tangent from the central thrust of this section. Back from our information criteria digression. Let’s get back on track with the text. Here’s a look at b10.6, the unavailable model: print(b10.6) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 1 + male ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.83 0.05 -0.93 -0.73 2522 1.00 ## male 0.61 0.06 0.48 0.74 3059 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the relative difference in admission odds. fixef(b10.6)[2] %&gt;% exp() %&gt;% round(digits = 2) ## [1] 1.84 And now we’ll compute difference in admission probabilities. post &lt;- posterior_samples(b10.6) post %&gt;% mutate(p_admit_male = inv_logit_scaled(b_Intercept + b_male), p_admit_female = inv_logit_scaled(b_Intercept), diff_admit = p_admit_male - p_admit_female) %&gt;% summarise(`2.5%` = quantile(diff_admit, probs = .025), `50%` = median(diff_admit), `97.5%` = quantile(diff_admit, probs = .975)) ## 2.5% 50% 97.5% ## 1 0.1139081 0.1415119 0.1699301 Instead of the summarise() code, we could have also used tidybayes::median_qi(diff_admit). It’s good to have options. Here’s our version of Figure 10.5. d &lt;- d %&gt;% mutate(case = factor(1:12)) p &lt;- predict(b10.6) %&gt;% as_tibble() %&gt;% bind_cols(d) d_text &lt;- d %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) ggplot(data = d, aes(x = case, y = admit / applications)) + geom_pointrange(data = p, aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = d_text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + coord_cartesian(ylim = 0:1) + labs(y = &quot;Proportion admitted&quot;, title = &quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) As alluded to in all that LOO/pareto_k talk, above, this is not a great fit. So we’ll ditch the last model paradigm for one that answers the new question “What is the average difference in probability of admission between females and males within departments?” (p. 307). The statistical formula for the full model follows the form \\[\\begin{align*} n_{\\text{admit}_i} &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept}_i} + \\beta \\text{male}_i \\\\ \\alpha_{\\text{dept}} &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] We don’t need to coerce an index like McElreath did in the text. But here are the models. b10.8 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 0 + dept, prior(normal(0, 10), class = b), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) b10.9 &lt;- update(b10.8, newdata = d, formula = admit | trials(applications) ~ 0 + dept + male) Let’s make two more loo() objects using reloo = T. l_b10.8_reloo &lt;- loo(b10.8, reloo = T) l_b10.9_reloo &lt;- loo(b10.9, reloo = T) Now compare them. loo_compare(l_b10.6_reloo, l_b10.7_reloo, l_b10.8_reloo, l_b10.9_reloo) ## elpd_diff se_diff ## b10.8 0.0 0.0 ## b10.9 -5.3 2.6 ## b10.6 -446.1 162.7 ## b10.7 -466.2 160.7 Here are the LOO weights. model_weights(b10.6, b10.7, b10.8, b10.9, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b10.6 b10.7 b10.8 b10.9 ## 0.000 0.000 0.928 0.072 The parameters summaries for our multivariable model, b10.9, look like this: fixef(b10.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## deptA 0.68 0.10 0.50 0.88 ## deptB 0.64 0.12 0.41 0.86 ## deptC -0.58 0.07 -0.73 -0.44 ## deptD -0.62 0.09 -0.78 -0.45 ## deptE -1.06 0.10 -1.27 -0.86 ## deptF -2.64 0.16 -2.96 -2.34 ## male -0.10 0.08 -0.26 0.06 And on the proportional odds scale, the posterior mean for b_male is: fixef(b10.9)[7, 1] %&gt;% exp() ## [1] 0.9066073 Since we’ve been using brms, there’s no need to fit our version of McElreath’s m10.9stan. We already have that in our b10.9. But just for kicks and giggles, here’s another way to get the model summary. b10.9$fit ## Inference for Stan model: 11ff351173d29f96c679ef5534162471. ## 2 chains, each with iter=2500; warmup=500; thin=1; ## post-warmup draws per chain=2000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_deptA 0.68 0.00 0.10 0.50 0.62 0.68 0.75 0.88 1602 1 ## b_deptB 0.64 0.00 0.12 0.41 0.56 0.64 0.72 0.86 1681 1 ## b_deptC -0.58 0.00 0.07 -0.73 -0.63 -0.58 -0.53 -0.44 3444 1 ## b_deptD -0.62 0.00 0.09 -0.78 -0.67 -0.61 -0.56 -0.45 2299 1 ## b_deptE -1.06 0.00 0.10 -1.27 -1.13 -1.06 -0.99 -0.86 3622 1 ## b_deptF -2.64 0.00 0.16 -2.96 -2.75 -2.64 -2.53 -2.34 3138 1 ## b_male -0.10 0.00 0.08 -0.26 -0.15 -0.10 -0.04 0.06 1274 1 ## lp__ -70.70 0.04 1.89 -75.22 -71.71 -70.39 -69.33 -68.00 1922 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Jul 12 15:49:48 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Here’s our version of Figure 10.6, the posterior validation check. predict(b10.9) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = d_text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + coord_cartesian(ylim = 0:1) + labs(y = &quot;Proportion admitted&quot;, title = &quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) The model precisions are imperfect, but way more valid than before. The posterior looks reasonably multivariate Gaussian. pairs(b10.9, off_diag_args = list(size = 1/10, alpha = 1/6)) 10.1.3.1 Overthinking: WAIC and aggregated binomial models. McElreath wrote: The WAIC function in rethinking detects aggregated binomial models and automatically splits them apart into 0/1 Bernoulli trials, for the purpose of calculating WAIC. It does this, because WAIC is computed point by point (see Chapter 6). So what you define as a “point” affects WAIC’s value. In an aggregated binomial each “point” is a bunch of independent trials that happen to share the same predictor values. In order for the disaggregated and aggregated models to agree, it makes sense to use the disaggregated representation. (p. 309) To my knowledge, brms::waic() and brms::loo() do not do this, which might well be why some of our values didn’t match up with those in the text. If you have additional insight on this, please share with the rest of the class. 10.1.4 Fitting binomial regressions with glm(). We’re not here to learn frequentist code, so we’re going to skip most of this section. But model b.good is worth fitting. Here are the data. # outcome and predictor almost perfectly associated y &lt;- c(rep(0, 10), rep(1, 10)) x &lt;- c(rep(-1, 9), rep(1, 11)) Fit the b.good model. b.good &lt;- brm(data = list(y = y, x = x), family = binomial, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), seed = 10) Our model summary will differ a bit from the one in the text. It seems this is because of the MAP/HMC contrast and our choice of priors. print(b.good) ## Family: binomial ## Links: mu = logit ## Formula: y ~ 1 + x ## Data: list(y = y, x = x) (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -5.22 4.18 -15.39 0.40 563 1.01 ## x 7.98 4.17 2.36 18.03 567 1.01 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You might experiment with different prior \\(SD\\)s to see how they influence the posterior \\(SD\\)s. Anyways, here’s the pairs() plot McElreath excluded from the text. pairs(b.good, off_diag_args = list(size = 1/10, alpha = 1/6)) That posterior, my friends, is not multivariate Gaussian. The plot deserves and extensive quote from McElreath. Inspecting the pairs plot (not shown) demonstrates just how subtle even simple models can be, once we start working with GLMs. I don’t say this to scare the reader. But it’s true that even simple models can behave in complicated ways. How you fit the model is part of the model, and in principle no GLM is safe for MAP estimation. (p. 311) 10.2 Poisson regression We’ll simulate our sweet count data. set.seed(10) # make the results reproducible tibble(y = rbinom(1e5, 1000, 1/1000)) %&gt;% summarise(y_mean = mean(y), y_variance = var(y)) ## # A tibble: 1 x 2 ## y_mean y_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.994 0.995 Yes, those statistics are virtually the same. When dealing with Poisson data, \\(\\mu = \\sigma^2\\). When you have a number of trials for which \\(n\\) is unknown or much larger than seen in the data, the Poisson likelihood is a useful tool. We define it like this \\[y \\sim \\text{Poisson} (\\lambda)\\] As \\(\\lambda\\) expresses both mean and variance because, within this model, the variance scales right along with the mean. Since \\(\\lambda\\) is constrained to be positive, we typically use the log link. Thus the basic Poisson regression model is \\[\\begin{align*} y_i &amp; \\sim \\text{Poisson} (\\lambda_i) \\\\ \\text{log} (\\lambda_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] 10.2.1 Example: Oceanic tool complexity. Load the Kline data. library(rethinking) data(Kline) d &lt;- Kline Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(Kline) d ## culture population contact total_tools mean_TU ## 1 Malekula 1100 low 13 3.2 ## 2 Tikopia 1500 low 22 4.7 ## 3 Santa Cruz 3600 low 24 4.0 ## 4 Yap 4791 high 43 5.0 ## 5 Lau Fiji 7400 high 33 5.0 ## 6 Trobriand 8000 high 19 4.0 ## 7 Chuuk 9200 high 40 3.8 ## 8 Manus 13000 low 28 6.6 ## 9 Tonga 17500 high 55 5.4 ## 10 Hawaii 275000 low 71 6.6 Here are our new columns. d &lt;- d %&gt;% mutate(log_pop = log(population), contact_high = ifelse(contact == &quot;high&quot;, 1, 0)) Our statistical model will follow the form \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\text{Poisson} (\\lambda_i) \\\\ \\text{log} (\\lambda_i) &amp; = \\alpha + \\beta_1 \\text{log_pop}_i + \\beta_2 \\text{contact_high}_i + \\beta_3 \\text{contact_high}_i \\times \\text{log_pop}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_3 &amp; \\sim \\text{Normal} (0, 1) \\end{align*}\\] The only new thing in our model code is family = poisson. brms defaults to the log() link. b10.10 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 10) print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.94 0.37 0.20 1.64 4075 1.00 ## log_pop 0.26 0.04 0.19 0.33 4317 1.00 ## contact_high -0.09 0.82 -1.70 1.49 2406 1.00 ## log_pop:contact_high 0.04 0.09 -0.13 0.22 2397 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the lower triangle of the correlation matrix for the parameters. post &lt;- posterior_samples(b10.10) post %&gt;% select(-lp__) %&gt;% rename(b_interaction = `b_log_pop:contact_high`) %&gt;% psych::lowerCor() ## b_Int b_lg_ b_cn_ b_ntr ## b_Intercept 1.00 ## b_log_pop -0.98 1.00 ## b_contact_high -0.16 0.16 1.00 ## b_interaction 0.10 -0.12 -0.99 1.00 And here’s the coefficient plot via bayesplot::mcmc_intervals(): # we&#39;ll set a renewed color theme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[4], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) post %&gt;% select(-lp__) %&gt;% rename(b_interaction = `b_log_pop:contact_high`) %&gt;% mcmc_intervals(prob = .5, prob_outer = .95) + theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) How plausible is it a high-contact island will have more tools than a low-contact island? post &lt;- post %&gt;% mutate(lambda_high = exp(b_Intercept + b_contact_high + (b_log_pop + `b_log_pop:contact_high`) * 8), lambda_low = exp(b_Intercept + b_log_pop * 8)) %&gt;% mutate(diff = lambda_high - lambda_low) post %&gt;% summarise(sum = sum(diff &gt; 0) / length(diff)) ## sum ## 1 0.9585 Quite, it turns out. Behold the corresponding Figure 10.8.a. post %&gt;% ggplot(aes(x = diff)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + geom_vline(xintercept = 0, linetype = 2, color = wes_palette(&quot;Moonrise2&quot;)[2]) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;lambda_high - lambda_low&quot;) I’m not happy with how clunky this solution is, but one way to get those marginal dot and line plots for the axes is to make intermediary tibbles. Anyway, here’s a version of Figure 10.8.b. # intermediary tibbles for our the dot and line portoin of the plot point_tibble &lt;- tibble(x = c(median(post$b_contact_high), min(post$b_contact_high)), y = c(min(post$`b_log_pop:contact_high`), median(post$`b_log_pop:contact_high`))) line_tibble &lt;- tibble(parameter = rep(c(&quot;b_contact_high&quot;, &quot;b_log_pop:contact_high&quot;), each = 2), x = c(quantile(post$b_contact_high, probs = c(.025, .975)), rep(min(post$b_contact_high), times = 2)), y = c(rep(min(post$`b_log_pop:contact_high`), times = 2), quantile(post$`b_log_pop:contact_high`, probs = c(.025, .975)))) # the plot post %&gt;% ggplot(aes(x = b_contact_high, y = `b_log_pop:contact_high`)) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) + geom_point(data = point_tibble, aes(x = x, y = y)) + geom_line(data = line_tibble, aes(x = x, y = y, group = parameter)) Here we deconstruct model b10.10, bit by bit. # no interaction b10.11 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop + contact_high) # no contact rate b10.12 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop) # no log-population b10.13 &lt;- update(b10.10, formula = total_tools ~ 1 + contact_high) # intercept only b10.14 &lt;- update(b10.10, formula = total_tools ~ 1, seed = 10) I know we got all excited with the LOO, above. Let’s just be lazy and go WAIC. [Though beware, the LOO opens up a similar can of worms, here.] b10.10 &lt;- add_criterion(b10.10, criterion = &quot;waic&quot;) b10.11 &lt;- add_criterion(b10.11, criterion = &quot;waic&quot;) b10.12 &lt;- add_criterion(b10.12, criterion = &quot;waic&quot;) b10.13 &lt;- add_criterion(b10.13, criterion = &quot;waic&quot;) b10.14 &lt;- add_criterion(b10.14, criterion = &quot;waic&quot;) Now compare them. w &lt;- loo_compare(b10.10, b10.11, b10.12, b10.13, b10.14, criterion = &quot;waic&quot;) cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) %&gt;% round(digits = 2) ## waic_diff se ## b10.11 0.00 0.00 ## b10.10 0.65 1.23 ## b10.12 5.09 8.43 ## b10.14 62.13 34.54 ## b10.13 70.77 46.41 Let’s get those WAIC weights, too. model_weights(b10.10, b10.11, b10.12, b10.13, b10.14, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b10.10 b10.11 b10.12 b10.13 b10.14 ## 0.40 0.55 0.04 0.00 0.00 Now wrangle w a little and make the WAIC plot. w %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model&quot;) %&gt;% ggplot(aes(x = reorder(model, -waic), y = waic, ymin = waic - se_waic, ymax = waic + se_waic, color = model)) + geom_pointrange(shape = 16, show.legend = F) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1, 2, 1, 1, 1)]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;WAIC&quot;) + theme(axis.ticks.y = element_blank()) Here’s our version of Figure 10.9. Recall, to do an “ensemble” posterior prediction in brms, one uses the pp_average() function. I know we were just lazy and focused on the WAIC. But let’s play around, a bit. Here we’ll weight the models based on the LOO by adding a weights = &quot;loo&quot; argument to the pp_average() function. If you check the corresponding section of the brms reference manual, you’ll find several weighting schemes. nd &lt;- tibble(contact_high = 0:1) %&gt;% expand(contact_high, log_pop = seq(from = 6.5, to = 13, length.out = 50)) ppa &lt;- pp_average(b10.10, b10.11, b10.12, weights = &quot;loo&quot;, method = &quot;fitted&quot;, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ppa %&gt;% ggplot(aes(x = log_pop, group = contact_high)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = contact_high, color = contact_high), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_text(data = d, aes(y = total_tools, label = total_tools, color = contact_high), size = 3.5) + coord_cartesian(xlim = c(7.1, 12.4), ylim = c(12, 70)) + labs(x = &quot;log population&quot;, y = &quot;total tools&quot;, subtitle = &quot;Blue is the high contact rate; black is the low.&quot;) + theme(legend.position = &quot;none&quot;, panel.border = element_blank()) In case you were curious, here are those LOO weights: model_weights(b10.10, b10.11, b10.12, weights = &quot;loo&quot;) ## b10.10 b10.11 b10.12 ## 0.35654696 0.60198589 0.04146715 10.2.2 MCMC islands. We fit our analogue to m10.10stan, b10.10, some time ago. print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.94 0.37 0.20 1.64 4075 1.00 ## log_pop 0.26 0.04 0.19 0.33 4317 1.00 ## contact_high -0.09 0.82 -1.70 1.49 2406 1.00 ## log_pop:contact_high 0.04 0.09 -0.13 0.22 2397 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Center log_pop. d &lt;- d %&gt;% mutate(log_pop_c = log_pop - mean(log_pop)) Now fit the log_pop-centered model. b10.10_c &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 10) print(b10.10_c) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.31 0.09 3.14 3.48 6029 1.00 ## log_pop_c 0.26 0.03 0.19 0.33 6174 1.00 ## contact_high 0.28 0.12 0.05 0.51 6622 1.00 ## log_pop_c:contact_high 0.07 0.17 -0.26 0.39 7179 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll use mcmc_pairs(), again, for Figure 10.10.a. # this helps us set our custom color scheme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[3], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) # the actual plot mcmc_pairs(x = posterior_samples(b10.10), pars = c(&quot;b_Intercept&quot;, &quot;b_log_pop&quot;, &quot;b_contact_high&quot;, &quot;b_log_pop:contact_high&quot;), off_diag_args = list(size = 1/10, alpha = 1/10), diag_fun = &quot;dens&quot;) And now behold Figure 10.10.b. mcmc_pairs(x = posterior_samples(b10.10_c), pars = c(&quot;b_Intercept&quot;, &quot;b_log_pop_c&quot;, &quot;b_contact_high&quot;, &quot;b_log_pop_c:contact_high&quot;), off_diag_args = list(size = 1/10, alpha = 1/10), diag_fun = &quot;dens&quot;) If you really want the correlation point estimates, use psych::lowerCorr(). psych::lowerCor(posterior_samples(b10.10)[, 1:4]) ## b_Int b_lg_ b_cn_ b__:_ ## b_Intercept 1.00 ## b_log_pop -0.98 1.00 ## b_contact_high -0.16 0.16 1.00 ## b_log_pop:contact_high 0.10 -0.12 -0.99 1.00 psych::lowerCor(posterior_samples(b10.10_c)[, 1:4]) ## b_Int b_l__ b_cn_ b___: ## b_Intercept 1.00 ## b_log_pop_c -0.45 1.00 ## b_contact_high -0.76 0.33 1.00 ## b_log_pop_c:contact_high 0.11 -0.20 -0.27 1.00 10.2.3 Example: Exposure and the offset. For the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained on page 312 [of the text], is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset. (p. 321, emphasis in the original) Here we simulate our data. set.seed(10) num_days &lt;- 30 y &lt;- rpois(num_days, 1.5) num_weeks &lt;- 4 y_new &lt;- rpois(num_weeks, 0.5 * 7) Let’s make them tidy and add log_days. ( d &lt;- tibble(y = c(y, y_new), days = c(rep(1, num_days), rep(7, num_weeks)), monastery = c(rep(0, num_days), rep(1, num_weeks))) %&gt;% mutate(log_days = log(days)) ) ## # A tibble: 34 x 4 ## y days monastery log_days ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 ## 2 1 1 0 0 ## 3 1 1 0 0 ## 4 2 1 0 0 ## 5 0 1 0 0 ## 6 1 1 0 0 ## 7 1 1 0 0 ## 8 1 1 0 0 ## 9 2 1 0 0 ## 10 1 1 0 0 ## # … with 24 more rows With the brms package, you use the offset() syntax, in which you put a pre-processed variable like log_days or the log of a variable, such as log(days). b10.15 &lt;- brm(data = d, family = poisson, y ~ 1 + offset(log_days) + monastery, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) The model summary: print(b10.15) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + offset(log_days) + monastery ## Data: d (Number of observations: 34) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.30 0.16 -0.01 0.59 3083 1.00 ## monastery -1.10 0.31 -1.72 -0.50 3179 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The model summary helps clarify that when you use offset(), brm() fixes the value. Thus there is no parameter estimate for the offset(). It’s a fixed part of the model not unlike the \\(\\nu\\) parameter of the Student-\\(t\\) distribution gets fixed to infinity when you use the Gaussian likelihood. Here we’ll compute the posterior means and 89% HDIs with tidybayes::mean_hdi(). library(tidybayes) posterior_samples(b10.15) %&gt;% transmute(lambda_old = exp(b_Intercept), lambda_new = exp(b_Intercept + b_monastery)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;lambda_old&quot;, &quot;lambda_new&quot;))) %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lambda_old 1.37 1.02 1.7 0.89 mean hdi ## 2 lambda_new 0.47 0.25 0.65 0.89 mean hdi As McElreath pointed out in the text, “Your estimates will be slightly different, because you got different randomly simulated data” (p. 322). 10.3 Other count regressions The next two of the remaining four models are maximum entropy distributions for certain problem types. The last two are mixtures, of which we’ll see more in the next chapter. 10.3.1 Multinomial. When more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. [We] already met the multinomial, implicitly, in Chapter 9 when we tossed pebbles into buckets as an introduction to maximum entropy. The binomial is really a special case of this distribution. And so its distribution formula resembles the binomial, just extrapolated out to three or more types of events. If there are \\(K\\) types of events with probabilities \\(p_1, …, p_K\\), then the probability of observing \\(y_1, …, y_K\\) events of each type out of \\(n\\) trials is (p. 323): \\[\\text{Pr} (y_1, ..., y_K | n, p_1, ..., p_K) = \\frac{n!}{\\prod_i y_i!} \\prod_{i = 1}^K p_i^{y_i}\\] Compare that equation with the simpler version in section 2.3.1 (page 33 in the text). 10.3.1.1 Explicit multinomial models. “The conventional and natural link is this context is the multinomial logit. This link function takes a vector of scores, one for each \\(K\\) event types, and computed the probability of a particular type of event \\(K\\) as” (p. 323, emphasis in the original) \\[\\text{Pr} (k |s_1, s_2, ..., s_K) = \\frac{\\text{exp} (s_k)}{\\sum_{i = 1}^K \\text{exp} (s_i)}\\] Let’s simulate the data. library(rethinking) # simulate career choices among 500 individuals n &lt;- 500 # number of individuals income &lt;- 1:3 # expected income of each career score &lt;- 0.5 * income # scores for each career, based on income # next line converts scores to probabilities p &lt;- softmax(score[1], score[2], score[3]) # now simulate choice # outcome career holds event type values, not counts career &lt;- rep(NA, n) # empty vector of choices for each individual set.seed(10) # sample chosen career for each individual for(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p) Here’s what the data look like. career %&gt;% as_tibble() %&gt;% ggplot(aes(x = value %&gt;% as.factor())) + geom_bar(size = 0, fill = wes_palette(&quot;Moonrise2&quot;)[2]) Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Here’s my naïve attempt to fit the model in brms. b10.16 &lt;- brm(data = list(career = career), family = categorical(link = logit), career ~ 1, prior(normal(0, 5), class = Intercept), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) This differs from McElreath’s m10.16. Most obviously, this has two parameters. McElreath’s m10.16 only has one. If you have experience with these models and know how to reproduce McElreath’s results in brms, please share your code. print(b10.16) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 ## Data: list(career = career) (Number of observations: 500) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## mu2_Intercept 0.49 0.13 0.23 0.76 1171 1.00 ## mu3_Intercept 1.01 0.12 0.77 1.25 1241 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the second data simulation, this time based on McElreath’s R code 10.58. library(rethinking) n &lt;- 100 set.seed(10) # simulate family incomes for each individual family_income &lt;- runif(n) # assign a unique coefficient for each type of event b &lt;- (1:-1) career &lt;- rep(NA, n) # empty vector of choices for each individual for (i in 1:n) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Here’s the brms version of McElreath’s m10.17. b10.17 &lt;- brm(data = list(career = career, # note how we used a list instead of a tibble family_income = family_income), family = categorical(link = logit), career ~ 1 + family_income, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10) Happily, these results cohere with the rethinking model. print(b10.17) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 + family_income ## Data: list(career = career, family_income = family_incom (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## mu2_Intercept 1.04 0.54 -0.02 2.14 2530 1.00 ## mu3_Intercept 1.04 0.56 -0.06 2.16 2425 1.00 ## mu2_family_income -1.81 1.00 -3.87 0.12 2528 1.00 ## mu3_family_income -1.75 1.03 -3.78 0.22 2566 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). McElreath described the parameters as “on a scale that is very hard to interpret” (p. 325). Indeed. 10.3.1.2 Multinomial in disguise as Poisson. Here we fit a multinomial likelihood by refactoring it to a series of Poissons. Let’s retrieve the Berkeley data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit rm(UCBadmit) detach(package:rethinking, unload = T) library(brms) Fit the models. # binomial model of overall admission probability b_binom &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 10) # Poisson model of overall admission rate and rejection rate b_pois &lt;- brm(data = d %&gt;% mutate(rej = reject), # &#39;reject&#39; is a reserved word family = poisson, mvbind(admit, rej) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 10) Note, the mvbind() syntax made b_pois a multivariate Poisson model. Starting with version 2.0.0, brms supports a variety of multivariate models. Anyway, here are the implications of b_pois. # extract the samples post &lt;- posterior_samples(b_pois) # wrangle post %&gt;% transmute(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% gather() %&gt;% # plot ggplot(aes(x = value, y = key, fill = key)) + geom_halfeyeh(point_interval = median_qi, .width = .95, color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = c(wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2])) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank()) The model summaries: print(b_binom) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 1 ## Data: d (Number of observations: 12) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.46 0.03 -0.52 -0.40 1057 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b_pois) ## Family: MV(poisson, poisson) ## Links: mu = log ## mu = log ## Formula: admit ~ 1 ## rej ~ 1 ## Data: d %&gt;% mutate(rej = reject) (Number of observations: 12) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## admit_Intercept 4.99 0.02 4.94 5.03 2002 1.00 ## rej_Intercept 5.44 0.02 5.41 5.48 2346 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the posterior mean for the probability of admission, based on b_binom. fixef(b_binom)[ ,&quot;Estimate&quot;] %&gt;% inv_logit_scaled() ## [1] 0.3875688 Happily, we get the same value within simulation error from model b_pois. k &lt;- fixef(b_pois) %&gt;% as.numeric() exp(k[1]) / (exp(k[1]) + exp(k[2])) ## [1] 0.387805 The formula for what we just did in code is \\[p_{\\text{admit}} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{\\text{exp} (\\alpha_1)}{\\text{exp} (\\alpha_1) + \\text{exp} (\\alpha_2)}\\] 10.3.2 Geometric. Sometimes a count variable is a number of events up until something happened. Call this “something” the terminating event. Often we want to model the probability of that event, a kind of analysis known as event history analysis or survival analysis. When the probability of the terminating event is constant through time (or distance), and the units of time (or distance) are discrete, a common likelihood function is the geometric distribution. This distribution has the form: \\[\\text{Pr} (y | p) = p (1 - p) ^{y - 1}\\] where \\(y\\) is the number of time steps (events) until the terminating event occurred and \\(p\\) is the probability of that event in each time step. This distribution has maximum entropy for unbounded counts with constant expected value. (pp. 327–328) Here we simulate exemplar data. # simulate n &lt;- 100 set.seed(10) x &lt;- runif(n) set.seed(10) y &lt;- rgeom(n, prob = inv_logit_scaled(-1 + 2 * x)) In case you’re curious, here are the data. list(y = y, x = x) %&gt;% as_tibble() %&gt;% ggplot(aes(x = x, y = y)) + geom_point(size = 3/5, alpha = 2/3) We fit the geometric model using family = geometric(link = log). b10.18 &lt;- brm(data = list(y = y, x = x), family = geometric(link = log), y ~ 0 + intercept + x, prior = c(prior(normal(0, 10), class = b, coef = intercept), prior(normal(0, 1), class = b)), iter = 2500, warmup = 500, chains = 2, cores = 2, seed = 10) The results: print(b10.18, digits = 2) ## Family: geometric ## Links: mu = log ## Formula: y ~ 0 + intercept + x ## Data: list(y = y, x = x) (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 0.75 0.24 0.27 1.20 1248 1.00 ## x -1.63 0.51 -2.66 -0.63 1125 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It turns out brms uses a different parameterization for the geometric distribution than rethinking does. It follows the form \\[f(y_i) = {y_i \\choose y_i} \\bigg (\\frac{\\mu_i}{\\mu_i + 1} \\bigg )^{y_i} \\bigg (\\frac{1}{\\mu_i + 1} \\bigg )\\] Even though the parameters brms yielded look different from those in the text, their predictions describe the data well. Here’s the marginal_effects() plot: plot(marginal_effects(b10.18), points = T, point_args = c(size = 3/5, alpha = 2/3), line_args = c(color = wes_palette(&quot;Moonrise2&quot;)[1], fill = wes_palette(&quot;Moonrise2&quot;)[1])) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 loo_2.1.0 broom_0.5.2 bayesplot_1.7.0 ggthemes_4.2.0 ## [6] wesanderson_0.3.6 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [11] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 tidyverse_1.2.1 brms_2.9.0 ## [16] Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ggstance_0.3.1 ## [5] markdown_1.0 base64enc_0.1-3 rstudioapi_0.10 listenv_0.7.0 ## [9] farver_2.0.3 svUnit_0.7-12 DT_0.7 fansi_0.4.0 ## [13] mvtnorm_1.0-10 lubridate_1.7.4 xml2_1.2.0 bridgesampling_0.6-0 ## [17] codetools_0.2-16 mnormt_1.5-5 knitr_1.23 shinythemes_1.1.2 ## [21] zeallot_0.1.0 jsonlite_1.6 shiny_1.3.2 compiler_3.6.3 ## [25] httr_1.4.0 backports_1.1.4 assertthat_0.2.1 Matrix_1.2-17 ## [29] lazyeval_0.2.2 cli_1.1.0 later_0.8.0 htmltools_0.3.6 ## [33] prettyunits_1.0.2 tools_3.6.3 igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 V8_2.2 ## [41] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-144 crosstalk_1.0.0 ## [45] psych_1.8.12 xfun_0.7 globals_0.12.4 ps_1.3.0 ## [49] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 lifecycle_0.1.0 ## [53] gtools_3.8.1 future_1.13.0 MASS_7.3-51.5 zoo_1.8-6 ## [57] scales_1.1.1.9000 colourpicker_1.0 hms_0.4.2 promises_1.0.1 ## [61] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [65] curl_3.3 gridExtra_2.3 stringi_1.4.3 dygraphs_1.1.1.6 ## [69] boot_1.3-24 pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [73] pkgconfig_2.0.2 matrixStats_0.54.0 HDInterval_0.2.0 evaluate_0.14 ## [77] lattice_0.20-38 labeling_0.3 rstantools_1.5.1 htmlwidgets_1.3 ## [81] processx_3.3.1 tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [85] bookdown_0.11 R6_2.4.0 generics_0.0.2 foreign_0.8-75 ## [89] pillar_1.4.1 haven_2.1.0 withr_2.1.2 xts_0.11-2 ## [93] abind_1.4-5 modelr_0.1.4 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [97] utf8_1.1.4 rmarkdown_1.13 grid_3.6.3 readxl_1.3.1 ## [101] callr_3.2.0 threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [105] httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 shinyjs_1.0 "],
["monsters-and-mixtures.html", "11 Monsters and Mixtures 11.1 Ordered categorical outcomes 11.2 Zero-inflated outcomes 11.3 Over-dispersed outcomes Reference Session info", " 11 Monsters and Mixtures [Of these majestic creatures], we’ll consider two common and useful examples. The first type is the ordered categorical model, useful for categorical outcomes with a fixed ordering. This model is built by merging a categorical likelihood function with a special kind of link function, usually a cumulative link. The second type is a family of zero-inflated and zero-augmented models, each of which mixes a binary event within an ordinary GLM likelihood like a Poisson or binomial. Both types of models help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models. (p. 331) 11.1 Ordered categorical outcomes It is very common in the social sciences, and occasional in the natural sciences, to have an outcome variable that is discrete, like a count, but in which the values merely indicate different ordered levels along some dimension. For example, if I were to ask you how much you like to eat fish, on a scale from 1 to 7, you might say 5. If I were to ask 100 people the same question, I’d end up with 100 values between 1 and 7. In modeling each outcome value, I’d have to keep in mind that these values are ordered because 7 is greater than 6, which is greater than 5, and so on. But unlike a count, the differences in values are not necessarily equal. In principle, an ordered categorical variable is just a multinomial prediction problem (page 323). But the constraint that the categories be ordered demands special treatment… The conventional solution is to use a cumulative link function. The cumulative probability of a value is the probability of that value or any smaller value. (pp. 331–332, emphasis in the original) 11.1.1 Example: Moral intuition. Let’s get the Trolley data from rethinking. library(rethinking) data(Trolley) d &lt;- Trolley Unload rethinking and load brms. rm(Trolley) detach(package:rethinking, unload = T) library(brms) Use the tidyverse to get a sense of the dimensions of the data. library(tidyverse) glimpse(d) ## Observations: 9,930 ## Variables: 12 ## $ case &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa… ## $ response &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, … ## $ order &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18… ## $ id &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434… ## $ age &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14… ## $ male &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ edu &lt;fct&gt; Middle School, Middle School, Middle School, Middle Sc… ## $ action &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, … ## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ contact &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ story &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car,… ## $ action2 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, … Though we have 9,930 rows, we only have 331 unique individuals. d %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 331 11.1.2 Describing an ordered distribution with intercepts. Before we get to plotting, in this chapter we’ll use theme settings and a color palette from the ggthemes package. library(ggthemes) We’ll take our basic theme settings from the theme_hc() function. We’ll use the Green fields color palette, which we can inspect with the canva_pal() function and a little help from scales::show_col(). scales::show_col(canva_pal(&quot;Green fields&quot;)(4)) canva_pal(&quot;Green fields&quot;)(4) ## [1] &quot;#919636&quot; &quot;#524a3a&quot; &quot;#fffae1&quot; &quot;#5a5f37&quot; canva_pal(&quot;Green fields&quot;)(4)[3] ## [1] &quot;#fffae1&quot; Now we’re ready to make our ggplot2 version of the simple histogram, Figure 11.1.a. ggplot(data = d, aes(x = response, fill = ..x..)) + geom_histogram(binwidth = 1/4, size = 0) + scale_x_continuous(breaks = 1:7) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) Our cumulative proportion plot, Figure 11.1.b, will require some pre-plot wrangling. d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) %&gt;% ggplot(aes(x = response, y = cum_pr_k, fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1)) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = c(0, 1)) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) In order to make the next plot, we’ll need McElreath’s logit() function. Here it is, the logarithm of cumulative odds plot, Figure 11.1.c. # McElreath&#39;s convenience function from page 335 logit &lt;- function(x) log(x / (1 - x)) d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) %&gt;% filter(response &lt; 7) %&gt;% # we can do the `logit()` conversion right in ggplot2 ggplot(aes(x = response, y = logit(cum_pr_k), fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(xlim = c(1, 7)) + ylab(&quot;log-cumulative-odds&quot;) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) The code for Figure 11.2 is itself something of a monster. d_plot &lt;- d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) ggplot(data = d_plot, aes(x = response, y = cum_pr_k, color = cum_pr_k, fill = cum_pr_k)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + geom_linerange(aes(ymin = 0, ymax = cum_pr_k), alpha = 1/2, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + # there must be more elegant ways to do this part geom_linerange(data = . %&gt;% mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k)), aes(x = response + .025, ymin = ifelse(response == 1, 0, discrete_probability), ymax = cum_pr_k), color = &quot;black&quot;) + geom_text(data = tibble(text = 1:7, response = seq(from = 1.25, to = 7.25, by = 1), cum_pr_k = d_plot$cum_pr_k - .065), aes(label = text), size = 4) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1)) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = c(0, 1)) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) McElreath’s convention for this first type of statistical model is \\[\\begin{align*} R_i &amp; \\sim \\text{Ordered} (\\mathbf p) \\\\ \\text{logit} (p_k) &amp; = \\alpha_k \\\\ \\alpha_k &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] The Ordered distribution is really just a categorical distribution that takes a vector \\(\\mathbf p = {p_1, p_2, p_3, p_4, p_5, p_6}\\) of probabilities of each response value below the maximum response (7 in this example). Each response value \\(k\\) in this vector is defined by its link to an intercept parameter, \\(\\alpha_k\\). Finally, some weakly regularizing priors are placed on these intercepts. (p. 335) Whereas in rethinking::map() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(phi , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s the intercepts-only model: # define the start values inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits) b11.1 &lt;- brm(data = d, family = cumulative, response ~ 1, prior(normal(0, 10), class = Intercept), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = inits_list, # here we add our start values seed = 11) McElreath needed to include the depth=2 argument in the rethinking::precis() function to show the threshold parameters from his m11.1stan model. With a brm() fit, we just use print() or summary() as usual. print(b11.1) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept[1] -1.92 0.03 -1.98 -1.86 1435 1.00 ## Intercept[2] -1.27 0.02 -1.32 -1.22 2135 1.00 ## Intercept[3] -0.72 0.02 -0.76 -0.68 2424 1.00 ## Intercept[4] 0.25 0.02 0.21 0.29 2414 1.00 ## Intercept[5] 0.89 0.02 0.85 0.93 2473 1.00 ## Intercept[6] 1.77 0.03 1.71 1.82 2381 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). What McElreath’s m11.1stan summary termed cutpoints[k], ours termed Intercept[k]. In both cases, these are the \\(\\alpha_k\\) parameters from the equations, above. The summaries look like those in the text, number of effective samples are high, and the \\(\\hat{R}\\) values are great. The model looks good. Recall we use the brms::inv_logit_scaled() function in place of McElreath’s logistic() function to get these into the probability metric. b11.1 %&gt;% fixef() %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept[1] 0.1280697 0.5075402 0.1214363 0.1345826 ## Intercept[2] 0.2196981 0.5059642 0.2115772 0.2276978 ## Intercept[3] 0.3275305 0.5052724 0.3184447 0.3367581 ## Intercept[4] 0.5616229 0.5049055 0.5518075 0.5712807 ## Intercept[5] 0.7089041 0.5054578 0.6999861 0.7176025 ## Intercept[6] 0.8544495 0.5070451 0.8474534 0.8609982 But recall that the posterior \\(SD\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the posterior_samples(). posterior_samples(b11.1) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) ## # A tibble: 6 x 5 ## key mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] 0.128 0.00337 0.121 0.135 ## 2 b_Intercept[2] 0.220 0.00409 0.212 0.228 ## 3 b_Intercept[3] 0.328 0.00465 0.318 0.337 ## 4 b_Intercept[4] 0.562 0.00483 0.552 0.571 ## 5 b_Intercept[5] 0.709 0.00451 0.700 0.718 ## 6 b_Intercept[6] 0.854 0.00350 0.847 0.861 11.1.3 Adding predictor variables. Now we define the linear model as \\(\\phi_i = \\beta x_i\\). Accordingly, the formula for our cumulative logit model becomes \\[\\begin{align*} \\text{log} \\frac{\\text{Pr} (y_i \\leq k)}{1 - \\text{Pr} (y_i \\leq k)} &amp; = \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta x_i \\end{align*}\\] I’m not aware that brms has an equivalent to the rethinking::dordlogit() function. So here we’ll make it by hand. The code comes from McElreath’s GitHub page. # first, we needed to specify the `logistic()` function, which is apart of the `dordlogit()` function logistic &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) p &lt;- ifelse(x == Inf, 1, p) p } # now we get down to it dordlogit &lt;- function(x, phi, a, log = FALSE) { a &lt;- c(as.numeric(a), Inf) p &lt;- logistic(a[x] - phi) na &lt;- c(-Inf, a) np &lt;- logistic(na[x] - phi) p &lt;- p - np if (log == TRUE) p &lt;- log(p) p } The dordlogit() function works like this: (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1])) ## [1] 0.12806973 0.09162833 0.10783241 0.23409242 0.14728125 0.14554539 ## [7] 0.14555047 Note the slight difference in how we used dordlogit() with a brm() fit summarized by fixef() than the way McElreath did with a map2stan() fit summarized by coef(). McElreath just put coef(m11.1) into dordlogit(). We, however, more specifically placed fixef(b11.1)[, 1] into the function. With the [, 1] part, we specified that we were working with the posterior means (i.e., Estimate) and neglecting the other summaries (i.e., the posterior SDs and 95% intervals). If you forget to subset, chaos ensues. Next, as McElreath further noted in the text, “these probabilities imply an average outcome of:” sum(pk * (1:7)) ## [1] 4.199725 I found that a bit abstract. Here’s the thing in a more elaborate tibble format. ( explicit_example &lt;- tibble(probability_of_a_response = pk) %&gt;% mutate(the_response = 1:7) %&gt;% mutate(their_product = probability_of_a_response * the_response) ) ## # A tibble: 7 x 3 ## probability_of_a_response the_response their_product ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.128 1 0.128 ## 2 0.0916 2 0.183 ## 3 0.108 3 0.323 ## 4 0.234 4 0.936 ## 5 0.147 5 0.736 ## 6 0.146 6 0.873 ## 7 0.146 7 1.02 explicit_example %&gt;% summarise(average_outcome_value = sum(their_product)) ## # A tibble: 1 x 1 ## average_outcome_value ## &lt;dbl&gt; ## 1 4.20 Aside This made me wonder how this would compare if we were lazy and ignored the categorical nature of the response. Here we refit the model with the typical Gaussian likelihood. brm(data = d, family = gaussian, response ~ 1, # in this case, 4 (i.e., the middle response) seems to be the conservative place to put the mean prior = c(prior(normal(4, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11) %&gt;% print() ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 4.20 0.02 4.16 4.24 2859 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.90 0.01 1.88 1.93 3599 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Happily, this yielded a mean estimate of 4.2, much like our average_outcome_value, above. End aside Now we’ll try it by subtracting .5 from each. # the probabilities of a given response (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1] - .5)) ## [1] 0.08180026 0.06406237 0.08218371 0.20922371 0.15902895 0.18443256 ## [7] 0.21926845 # the average rating sum(pk * (1:7)) ## [1] 4.72999 So the rule is we subtract the linear model from each interecept. Let’s fit our multivariable models. # start values for b11.2 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0) b11.2 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + intention + contact, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = list(inits, inits), seed = 11) # start values for b11.3 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0, `action:intention` = 0, `contact:intention` = 0) b11.3 &lt;- update(b11.2, formula = response ~ 1 + action + intention + contact + action:intention + contact:intention, inits = list(inits, inits)) We don’t have a coeftab() function in brms like for rethinking. But as we did for Chapter 6, we can reproduce it with help from the broom package and a bit of data wrangling. library(broom) tibble(model = str_c(&quot;b11.&quot;, 1:3)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% select(model, term, estimate) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% complete(term = distinct(., term), model) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) %&gt;% # this last step isn&#39;t necessary, but it orders the rows to match the text slice(c(6:11, 1, 4, 3, 2, 5)) ## # A tibble: 11 x 4 ## term b11.1 b11.2 b11.3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] -1.92 -2.84 -2.64 ## 2 b_Intercept[2] -1.27 -2.16 -1.94 ## 3 b_Intercept[3] -0.72 -1.57 -1.34 ## 4 b_Intercept[4] 0.25 -0.55 -0.31 ## 5 b_Intercept[5] 0.89 0.12 0.36 ## 6 b_Intercept[6] 1.77 1.02 1.27 ## 7 b_action NA -0.71 -0.47 ## 8 b_intention NA -0.72 -0.28 ## 9 b_contact NA -0.96 -0.33 ## 10 b_action:intention NA NA -0.45 ## 11 b_intention:contact NA NA -1.28 If you really wanted that last nobs row at the bottom, you could elaborate on this code: b11.1$data %&gt;% count(). Also, if you want a proper coeftab() function for brms, McElreath’s code lives here. Give it a whirl. Here we compute the WAIC. Caution: This took some time to compute. b11.1 &lt;- add_criterion(b11.1, &quot;waic&quot;) b11.2 &lt;- add_criterion(b11.2, &quot;waic&quot;) b11.3 &lt;- add_criterion(b11.3, &quot;waic&quot;) Now compare the models. loo_compare(b11.1, b11.2, b11.3, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic ## b11.3 0.0 0.0 -18464.6 40.6 11.1 0.1 ## b11.2 -80.4 12.9 -18545.1 38.1 9.1 0.0 ## b11.1 -462.4 31.4 -18927.0 28.9 5.8 0.0 ## waic se_waic ## b11.3 36929.2 81.3 ## b11.2 37090.1 76.3 ## b11.1 37854.0 57.7 Here are the WAIC weights. model_weights(b11.1, b11.2, b11.3, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b11.1 b11.2 b11.3 ## 0 0 1 McElreath made Figure 11.3 by extracting the samples of his m11.3, saving them as post, and working some hairy base R plot() code. We’ll take a different route and use brms::fitted(). This will take substantial data wrangling, but hopefully it’ll be instructive. Let’s first take a look at the initial fitted() output for the beginnings of Figure 11.3.a. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% glimpse() ## Observations: 100 ## Variables: 14 ## $ `1.1` &lt;dbl&gt; 0.06506068, 0.06605582, 0.06783574, 0.06928439, 0.07047269… ## $ `2.1` &lt;dbl&gt; 0.08371328, 0.08438677, 0.08103099, 0.08650392, 0.09086568… ## $ `1.2` &lt;dbl&gt; 0.05594517, 0.05772093, 0.06213183, 0.06024808, 0.06058214… ## $ `2.2` &lt;dbl&gt; 0.06935840, 0.07107262, 0.07223049, 0.07266108, 0.07498511… ## $ `1.3` &lt;dbl&gt; 0.08204317, 0.08397825, 0.08113123, 0.08277670, 0.08288483… ## $ `2.3` &lt;dbl&gt; 0.09758441, 0.09922588, 0.09158018, 0.09615874, 0.09820399… ## $ `1.4` &lt;dbl&gt; 0.2196358, 0.2229184, 0.2135595, 0.2225014, 0.2147448, 0.2… ## $ `2.4` &lt;dbl&gt; 0.2394622, 0.2417221, 0.2272673, 0.2392751, 0.2332252, 0.2… ## $ `1.5` &lt;dbl&gt; 0.1672843, 0.1613754, 0.1701957, 0.1649318, 0.1669384, 0.1… ## $ `2.5` &lt;dbl&gt; 0.1637456, 0.1577072, 0.1680532, 0.1612923, 0.1627887, 0.1… ## $ `1.6` &lt;dbl&gt; 0.1858144, 0.1853695, 0.1943867, 0.1837582, 0.1942797, 0.1… ## $ `2.6` &lt;dbl&gt; 0.1657141, 0.1657481, 0.1792477, 0.1656508, 0.1720446, 0.1… ## $ `1.7` &lt;dbl&gt; 0.2242164, 0.2225818, 0.2107593, 0.2164994, 0.2100975, 0.2… ## $ `2.7` &lt;dbl&gt; 0.1804219, 0.1801374, 0.1805902, 0.1784581, 0.1678868, 0.1… Hopefully by now it’s clear why we needed the nd tibble, which we made use of in the newdata = nd argument. Because we set summary = F, we get draws from the posterior instead of summaries. With max_iter, we controlled how many of those posterior draws we wanted. McElreath used 100, which he indicated at the top of page 341, so we followed suit. It took me a minute to wrap my head around the meaning of the 14 vectors, which were named by brms::fitted() default. Notice how each column is named by two numerals, separated by a period. That first numeral indicates which if the two intention values the draw is based on (i.e., 1 stands for intention == 0, 2, stands for intention == 1). The numbers on the right of the decimals are the seven response options for response. For each posterior draw, you get one of those for each value of intention. Finally, it might not be immediately apparent, but the values are in the probability scale, just like pk on page 338. Now we know what we have in hand, it’s just a matter of careful wrangling to get those probabilities into a more useful format to feed into ggplot2. I’ve extensively annotated the code, below. If you lose track of happens in a given step, just run the code up till that point. Go step by step. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% # we convert the data to the long format gather() %&gt;% # we need an variable to index which posterior iteration we&#39;re working with mutate(iter = rep(1:max_iter, times = 14)) %&gt;% # this step isn’t technically necessary, but I prefer my `iter` index at the far left. select(iter, everything()) %&gt;% # here we extract the `intention` and `response` information out of the `key` vector and # spread it into two vectors. separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% # that step produced two character vectors. they’ll be more useful as numbers mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% # here we convert `intention` into its proper 0:1 metric mutate(intention = intention -1) %&gt;% # this isn&#39;t necessary, but it helps me understand exactly what metric the values are currently in rename(pk = value) %&gt;% # this step is based on McElreath&#39;s R code 11.10 on page 338 mutate(`pk:rating` = pk * rating) %&gt;% # I’m not sure how to succinctly explain this. you’re just going to have to trust me group_by(iter, intention) %&gt;% # this is very important for the next step. arrange(iter, intention, rating) %&gt;% # here we take our `pk` values and make culmulative sums. why? take a long hard look at Figure 11.2. mutate(probability = cumsum(pk)) %&gt;% # `rating == 7` is unnecessary. these `probability` values are by definition 1 filter(rating &lt; 7) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + # note how we made a new data object for `geom_text()` geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;, x = &quot;intention&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) Boom! Okay, that pile of code is a bit of a mess and you’re not going to want to repeatedly cut and paste all that. Let’s condense it into a homemade function, make_Figure_11.3_data(). make_Figure_11.3_data &lt;- function(action, contact, max_iter){ nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(iter = rep(1:max_iter, times = 14)) %&gt;% select(iter, everything()) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% rename(pk = value) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% arrange(iter, intention, rating) %&gt;% mutate(probability = cumsum(pk)) %&gt;% filter(rating &lt; 7) } Now we’ll use our sweet homemade function to make our plots. # Figure 11.3.a p1 &lt;- make_Figure_11.3_data(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;, x = &quot;intention&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Figure 11.3.b p2 &lt;- make_Figure_11.3_data(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.12, .24, .35, .50, .68, .80, .92)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 1,\\ncontact = 0&quot;, x = &quot;intention&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Figure 11.3.c p3 &lt;- make_Figure_11.3_data(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.15, .34, .44, .56, .695, .8, .92)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 0,\\ncontact = 1&quot;, x = &quot;intention&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # here we stitch them together with `grid.arrange()` library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) If you’d like to learn more about these kinds of models and how to fit them in brms, check out Bürkner and Vuorre’s Ordinal Regression Models in Psychology: A Tutorial. 11.1.4 Bonus: Figure 11.3 alternative. I have a lot of respect for McElreath. But man, Figure 11.3 is the worst. I’m in clinical psychology and there’s no way a working therapist is going to look at a figure like that and have any sense of what’s going on. Nobody’s got time for that. We’ve have clients to serve! Happily, we can go further. Look back at McElreath’s R code 11.10 on page 338. See how he multiplied the elements of pk by their respective response values and then just summed them up to get an average outcome value? With just a little amendment to our custom make_Figure_11.3_data() function, we can wrangle our fitted() output to express average response values for each of our conditions of interest. Here’s the adjusted function: make_data_for_an_alternative_fiture &lt;- function(action, contact, max_iter){ nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(iter = rep(1:max_iter, times = 14)) %&gt;% select(iter, everything()) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% rename(pk = value) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% # everything above this point is identical to the previous custom function. # all we do is replace the last few lines with this one line of code. summarise(mean_rating = sum(`pk:rating`)) } Our handy homemade but monstrously-named make_data_for_an_alternative_fiture() function works very much like its predecessor. You’ll see. # alternative to Figure 11.3.a p1 &lt;- make_data_for_an_alternative_fiture(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(&quot;response&quot;, breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # alternative to Figure 11.3.b p2 &lt;- make_data_for_an_alternative_fiture(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(&quot;response&quot;, breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 1,\\ncontact = 0&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # alternative to Figure 11.3.c p3 &lt;- make_data_for_an_alternative_fiture(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(&quot;response&quot;, breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 0,\\ncontact = 1&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) grid.arrange(p1, p2, p3, ncol = 3) Finally; now those are plots I can sell in a clinical psychology journal! 11.2 Zero-inflated outcomes Very often, the things we can measure are not emissions from any pure process. Instead, they are mixtures of multiple processes. Whenever there are different causes for the same observation, then a mixture model may be useful. A mixture model uses more than one simple probability distribution to model a mixture of causes. In effect, these models use more than one likelihood for the same outcome variable. Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A “zero” means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started. (p. 342, emphasis in the original) In his Rethinking: Breaking the law box, McElreath discussed how advances in computing have made it possible for working scientists to define their own data generating models. If you’d like to dive deeper into the topic, check out Bürkner’s vignette, Define Custom Response Distributions with brms. We’ll even make use of it a little further down. 11.2.1 Example: Zero-inflated Poisson. Here we simulate our drunk monk data. # define parameters prob_drink &lt;- 0.2 # 20% of days rate_work &lt;- 1 # average 1 manuscript per day # sample one year of production n &lt;- 365 # simulate days monks drink set.seed(11) drink &lt;- rbinom(n, 1, prob_drink) # simulate manuscripts completed y &lt;- (1 - drink) * rpois(n, rate_work) We’ll put those data in a tidy tibble before plotting. d &lt;- tibble(Y = y) %&gt;% arrange(Y) %&gt;% mutate(zeros = c(rep(&quot;zeros_drink&quot;, times = sum(drink)), rep(&quot;zeros_work&quot;, times = sum(y == 0 &amp; drink == 0)), rep(&quot;nope&quot;, times = n - sum(y == 0)) )) ggplot(data = d, aes(x = Y)) + geom_histogram(aes(fill = zeros), binwidth = 1, size = 1/10, color = &quot;grey92&quot;) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[1], canva_pal(&quot;Green fields&quot;)(4)[2], canva_pal(&quot;Green fields&quot;)(4)[1])) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) With these data, the likelihood of observing zero on y, (i.e., the likelihood zero manuscripts were completed on a given occasion) is \\[\\begin{align*} \\text{Pr} (0 | p, \\lambda) &amp; = \\text{Pr} (\\text{drink} | p) + \\text{Pr} (\\text{work} | p) \\times \\text{Pr} (0 | \\lambda) \\\\ &amp; = p + (1 - p) \\text{ exp} (- \\lambda) \\end{align*}\\] And since the Poisson likelihood of \\(y\\) is \\(\\text{Pr} (y | \\lambda) = \\lambda^y \\text{exp} (- \\lambda) / y!\\), the likelihood of \\(y = 0\\) is just \\(\\text{exp} (- \\lambda)\\). The above is just the mathematics for: The probability of observing a zero is the probability that the monks didn’t drink OR (\\(+\\)) the probability that the monks worked AND (\\(\\times\\)) failed to finish anything. And the likelihood of a non-zero value \\(y\\) is: \\[\\begin{align*} \\text{Pr} (y | p, \\lambda) &amp; = \\text{Pr} (\\text{drink} | p) (0) + \\text{Pr} (\\text{work} | p) \\text{Pr} (y | \\lambda) \\\\ &amp; = (1 - p) \\frac {\\lambda^y \\text{ exp} (- \\lambda)}{y!} \\end{align*}\\] Since drinking monks never produce \\(y &gt; 0\\), the expression above is just the chance the monks both work \\(1 - p\\), and finish \\(y\\) manuscripts. (p. 344, emphasis in the original) So letting \\(p\\) be the probability \\(y\\) is zero and \\(\\lambda\\) be the shape of the distribution, the zero-inflated Poisson (ZIPoisson) regression model takes the basic form \\[\\begin{align*} y_i &amp; \\sim \\text{ZIPoisson} (p_i, \\lambda_i)\\\\ \\text{logit} (p_i) &amp; = \\alpha_p + \\beta_p x_i \\\\ \\text{log} (\\lambda_i) &amp; = \\alpha_\\lambda + \\beta_\\lambda x_i \\end{align*}\\] One last thing to note is that in brms, \\(p_i\\) is denoted zi. So the intercept [and zi] only zero-inflated Poisson model in brms looks like this. b11.4 &lt;- brm(data = d, family = zero_inflated_poisson, Y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(beta(2, 2), class = zi)), # the brms default is beta(1, 1) cores = 4, seed = 11) print(b11.4) ## Family: zero_inflated_poisson ## Links: mu = log; zi = identity ## Formula: Y ~ 1 ## Data: d (Number of observations: 365) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.09 0.08 -0.07 0.25 1406 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## zi 0.23 0.05 0.13 0.33 1367 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The zero-inflated Poisson is parameterized in brms a little differently than it is in rethinking. The different parameterization did not influence the estimate for the Intercept, \\(\\lambda\\). In both here and in the text, \\(\\lambda\\) was about zero. However, it did influence the summary of zi. Note how McElreath’s logistic(-1.39) yielded 0.1994078. Seems rather close to our zi estimate of 0.234. First off, because he didn’t set his seed in the text before simulating, we couldn’t exactly reproduce his simulated drunk monk data. So our results will vary a little due to that alone. But after accounting for simulation variance, hopefully it’s clear that zi in brms is already in the probability metric. There’s no need to convert it. In the prior argument, we used beta(2, 2) for zi and also mentioned in the margin that the brms default is beta(1, 1). To give you a sense of the priors, let’s plot them. tibble(`zi prior`= seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(`beta(1, 1)` = dbeta(`zi prior`, 1, 1), `beta(2, 2)` = dbeta(`zi prior`, 2, 2)) %&gt;% gather(prior, density, -`zi prior`) %&gt;% ggplot(aes(x = `zi prior`, ymin = 0, ymax = density)) + geom_ribbon(aes(fill = prior)) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[4], canva_pal(&quot;Green fields&quot;)(4)[2])) + scale_x_continuous(&quot;prior for zi&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) + facet_wrap(~prior) Hopefully this clarifies that the brms default is flat, whereas our prior regularized a bit toward .5. Anyway, here’s that exponentiated \\(\\lambda\\). fixef(b11.4)[1, ] %&gt;% exp() ## Estimate Est.Error Q2.5 Q97.5 ## 1.0963571 1.0835717 0.9340192 1.2786830 11.2.1.1 Overthinking: Zero-inflated Poisson distribution function. dzip &lt;- function(x, p, lambda, log = TRUE) { ll &lt;- ifelse( x == 0, p + (1 - p) * exp(-lambda), (1 - p) * dpois(x, lambda, log = FALSE) ) if (log == TRUE) ll &lt;- log(ll) return(ll) } We can use McElreath’s dzip() to do a posterior predictive check for our model. To work with our estimates for \\(p\\) and \\(\\lambda\\) directly, we’ll set log = F. p_b11.4 &lt;- posterior_summary(b11.4)[2, 1] lambda_b11.4 &lt;- posterior_summary(b11.4)[1, 1] %&gt;% exp() tibble(x = 0:4) %&gt;% mutate(density = dzip(x = x, p = p_b11.4, lambda = lambda_b11.4, log = F)) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) If you look up to the histogram we made at the beginning of this section, you’ll see this isn’t a terrible approximation. 11.3 Over-dispersed outcomes All statistical models omit something. The question is only whether that something is necessary for making useful inferences. One symptom that something important has been omitted from a count model is over-dispersion. The variance of a variable is sometimes called its dispersion. For a counting process like a binomial, the variance is a function of the same parameters as the expected value. For example, the expected value of a binomial is \\(np\\) and its variance is \\(np (1 - p)\\). When the observed variance exceeds this amount—after conditioning on all the predictor variables—this implies that some omitted variable is producing additional dispersion in the observed counts. What could go wrong, if we ignore the over-dispersion? Ignoring it can lead to all of the same problems as ignoring any predictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or producing spurious inferences. (p, 346, emphasis in the original) In this chapter we’ll cope with the problem using continuous mixture models—first the beta-binomial and then the gamma-Poisson (a.k.a. negative binomial). 11.3.1 Beta-binomial. A beta-binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success. And predictor variables change the shape of this distribution, instead of directly determining the probability of each success. (p, 347, emphasis in the original) Unfortunately, we need to digress. As it turns out, there are multiple ways to parameterize the beta distribution and we’ve run square into two. In the text, McElreath wrote the beta distribution has two parameters, an average probability \\(\\overline{p}\\) and a shape parameter \\(\\theta\\). In his R code 11.24, which we’ll reproduce in a bit, he demonstrated that parameterization with the rethinking::dbeta2() function. The nice thing about this parameterization is how intuitive the pbar parameter is. If you want a beta with an average of .2, you set pbar &lt;- .2. If you want the distribution to be more or less certain, make the theta argument more or less large, respectively. However, the beta density is typically defined in terms of \\(\\alpha\\) and \\(\\beta\\). If you denote the data as \\(y\\), this follows the form \\[\\text{Beta} (y | \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{\\text B (\\alpha, \\beta)}\\] which you can verify in the Continuous Distributions on [0, 1] section of the Stan Functions Reference. In the formula, \\(\\text B\\) stands for the Beta function, which computes a normalizing constant, which you can learn about in the Mathematical Functions of the Stan reference manual. This is all important to be aware of because when we defined that beta prior for zi in the last model, it was using this parameterization. Also, if you look at the base R dbeta() function, you’ll learn it takes two parameters, shape1 and shape2. Those uncreatively-named parameters are the same \\(\\alpha\\) and \\(\\beta\\) from the density, above. They do not correspond to the pbar and theta parameters of McEreath’s rethinking::dbeta2(). McElreath had good reason for using dbeta2(). Beta’s typical \\(\\alpha\\) and \\(\\beta\\) parameters aren’t the most intuitive to use; the parameters in McElreath’s dbeta2() are much nicer. But if you’re willing to dive deeper, it turns out you can find the mean of a beta distribution in terms of \\(\\alpha\\) and \\(\\beta\\) like this \\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\] We can talk about the spread of the distribution, sometimes called \\(\\kappa\\), in terms \\(\\alpha\\) and \\(\\beta\\) like this \\[\\kappa = \\alpha + \\beta\\] With \\(\\mu\\) and \\(\\kappa\\) in hand, we can even find the \\(SD\\) of a beta distribution with \\[\\sigma = \\sqrt{\\mu (1 - \\mu) / (\\kappa + 1)}\\] I explicate all this because McElreath’s pbar is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) and his theta is \\(\\kappa = \\alpha + \\beta\\). This is great news because it means that we can understand what McElreath did with his beta2() function in terms of base R’s dbeta() function. Which also means that we can understand the distribution of the beta parameters used in brms::brm(). To demonstrate, let’s walk through McElreath’s R code 11.25. pbar &lt;- 0.5 theta &lt;- 5 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01))) + geom_ribbon(aes(x = x, ymin = 0, ymax = rethinking::dbeta2(x, pbar, theta)), fill = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(paste(&quot;The &quot;, beta, &quot; distribution&quot;)), subtitle = expression(paste(&quot;Defined in terms of &quot;, mu, &quot; (i.e., pbar) and &quot;, kappa, &quot; (i.e., theta)&quot;))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) In his 2014 text, Doing Bayesian Data Analysis, Kruschke provided code for a convenience function that will take pbar and theta as inputs and return the corresponding \\(\\alpha\\) and \\(\\beta\\) values. Here’s the function: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now we can use Kruschke’s betaABfromMeanKappa() to find the \\(\\alpha\\) and \\(\\beta\\) values corresponding to pbar and theta. betaABfromMeanKappa(mean = pbar, kappa = theta) ## $a ## [1] 2.5 ## ## $b ## [1] 2.5 And finally, we can double check that all of this works. Here’s the same distribution but defined in terms of \\(\\alpha\\) and \\(\\beta\\). ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01))) + geom_ribbon(aes(x = x, ymin = 0, ymax = dbeta(x, 2.5, 2.5)), fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(paste(&quot;The &quot;, beta, &quot; distribution&quot;)), subtitle = expression(paste(&quot;This time defined in terms of &quot;, alpha, &quot; and &quot;, beta))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) McElreath encouraged us to “explore different values for pbar and theta” (p. 348). Here’s a grid of plots with pbar = c(.25, .5, .75) and theta = c(5, 10, 15) # data tibble(pbar = c(.25, .5, .75)) %&gt;% expand(pbar, theta = c(5, 15, 30)) %&gt;% expand(nesting(pbar, theta), x = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(density = rethinking::dbeta2(x, pbar, theta), mu = str_c(&quot;mu == &quot;, pbar %&gt;% str_remove(., &quot;0&quot;)), kappa = str_c(&quot;kappa == &quot;, theta)) %&gt;% mutate(kappa = factor(kappa, levels = c(&quot;kappa == 30&quot;, &quot;kappa == 15&quot;, &quot;kappa == 5&quot;))) %&gt;% # plot ggplot() + geom_ribbon(aes(x = x, ymin = 0, ymax = density), fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, labels = NULL) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), axis.ticks.y = element_blank()) + facet_grid(kappa ~ mu, labeller = label_parsed) If you’d like to see how to make a similar plot in terms of \\(\\alpha\\) and \\(\\beta\\), see the chapter 6 document of my project recoding Kruschke’s text into tidyverse and brms code. But remember, we’re not fitting a beta model. We’re using the beta-binomial. “We’re going to bind our linear model to \\(\\overline p\\), so that changes in predictor variables change the central tendency of the distribution” (p. 348). The statistical model we’ll be fitting follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\text{BetaBinomial} (n_i, \\overline p_i, \\theta)\\\\ \\text{logit} (\\overline p_i) &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 2) \\\\ \\theta &amp; \\sim \\text{Exponential} (1) \\end{align*}\\] Here the size \\(n = \\text{applications}\\). In case you’re confused, yes, our statistical model is not the one McElreath presented at the top of page 348 in the text. If you look closely, the statistical formula he presented does not match up with the one implied by his R code 11.26. Our statistical formula and the brm() model we’ll be fitting, below, correspond to his R code 11.26. Before we fit, we have an additional complication. The beta-binomial distribution is not implemented in brms at this time. However, brms versions 2.2.0 and above allow users to define custom distributions. You can find the handy vignette here. Happily, Bürkner even used the beta-binomial distribution as the exemplar in the vignette. Before we get carried away, let’s load the data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Unload rethinking and load brms. rm(UCBadmit) detach(package:rethinking, unload = T) library(brms) I’m not going to go into great detail explaining the ins and outs of making custom distributions for brm(). You’ve got Bürkner’s vignette for that. For our purposes, we need two preparatory steps. First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some functions for Stan which are not defined in Stan itself. We’ll save them as stan_funs. Third, we’ll make a stanvar() statement which will allow us to pass our stan_funs to brm(). beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 0), type = &quot;int&quot;, vars = &quot;trials[n]&quot; ) stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int T) { return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int T) { return beta_binomial_rng(T, mu * phi, (1 - mu) * phi); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) With that out of the way, we’re almost ready to test this baby out. Before we do, a point of clarification: What McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar{p}\\), Bürkner simply called \\(\\mu\\). b11.5 &lt;- brm(data = d, family = beta_binomial2, # here&#39;s our custom likelihood admit | trials(applications) ~ 1, prior = c(prior(normal(0, 2), class = Intercept), prior(exponential(1), class = phi)), iter = 4000, warmup = 1000, cores = 2, chains = 2, stanvars = stanvars, # note our `stanvars` seed = 11) Success, our results look a lot like those in the text! print(b11.5) ## Family: beta_binomial2 ## Links: mu = logit; phi = identity ## Formula: admit | trials(applications) ~ 1 ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.37 0.30 -0.97 0.23 3625 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## phi 2.76 0.97 1.21 5.03 3839 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the corresponding posterior_samples() data object looks like. post &lt;- posterior_samples(b11.5) head(post) ## b_Intercept phi lp__ ## 1 -0.1330577 1.894299 -70.82044 ## 2 -0.7988703 2.490037 -71.11948 ## 3 -0.2958822 2.518267 -70.11604 ## 4 -0.9408431 2.022691 -72.23589 ## 5 -0.8461203 3.673938 -71.58017 ## 6 0.2201614 2.399016 -71.99957 Here’s our median and percentile-based 95% interval. post %&gt;% tidybayes::median_qi(inv_logit_scaled(b_Intercept)) %&gt;% mutate_if(is.double, round, digits = 3) ## inv_logit_scaled(b_Intercept) .lower .upper .width .point .interval ## 1 0.409 0.274 0.558 0.95 median qi To stay within the tidyverse while making the many thin lines in Figure 11.5.a, we’re going to need to do a bit of data processing. First, we’ll want a variable to index the rows in post (i.e., to index the posterior draws). And we’ll want to convert the b_Intercept to the \\(\\bar{p}\\) metric with the inv_logit_scaled() function. Then we’ll use sample_n() to randomly draw a subset of the posterior draws. Then with the expand() function, we’ll insert a tightly-spaced sequence of x values ranging between 0 and 1–the parameter space of beta distribution. Finally, we’ll use pmap_dbl() to compute the density values for the rethinking::dbeta2 distribution corresponding to the unique combination of x, p_bar, and phi values in each row. set.seed(11) lines &lt;- post %&gt;% mutate(iter = 1:n(), p_bar = inv_logit_scaled(b_Intercept)) %&gt;% sample_n(size = 100) %&gt;% expand(nesting(iter, p_bar, phi), x = seq(from = 0, to = 1, by = .005)) %&gt;% mutate(density = pmap_dbl(list(x, p_bar, phi), rethinking::dbeta2)) str(lines) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 20100 obs. of 5 variables: ## $ iter : int 6 6 6 6 6 6 6 6 6 6 ... ## $ p_bar : num 0.555 0.555 0.555 0.555 0.555 ... ## $ phi : num 2.4 2.4 2.4 2.4 2.4 ... ## $ x : num 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 ... ## $ density: num 0 0.249 0.313 0.358 0.394 ... All that was just for the thin lines. To make the thicker line for the posterior mean, we’ll get tricky with stat_function(). lines %&gt;% ggplot(aes(x = x, y = density)) + stat_function(fun = rethinking::dbeta2, args = list(prob = mean(inv_logit_scaled(post[, 1])), theta = mean(post[, 2])), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1.5) + geom_line(aes(group = iter), alpha = .2, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = 0:3) + xlab(&quot;probability admit&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) There are other ways to do this. For ideas, check out my blog post Make rotated Gaussians, Kruschke style. Before we can do our variant of Figure 11.5.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, fitted_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it. expose_functions(b11.5, vectorize = TRUE) # required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, draws) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi N &lt;- draws$data$trials[i] y &lt;- draws$data$Y[i] beta_binomial2_lpmf(y, mu, phi, N) } predict_beta_binomial2 &lt;- function(i, draws, ...) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi N &lt;- draws$data$trials[i] beta_binomial2_rng(mu, phi, N) } # required to use `fitted()` fitted_beta_binomial2 &lt;- function(draws) { mu &lt;- draws$dpars$mu trials &lt;- draws$data$trials trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } With those intermediary steps out of the way, we’re ready to make Figure 11.5.b. # the prediction intervals predict(b11.5) %&gt;% as_tibble() %&gt;% transmute(ll = Q2.5, ul = Q97.5) %&gt;% # the fitted intervals bind_cols( fitted(b11.5) %&gt;% as_tibble() ) %&gt;% # Tte original data used to fit the model bind_cols(b11.5$data) %&gt;% mutate(case = 1:12) %&gt;% # plot ggplot(aes(x = case)) + geom_linerange(aes(ymin = ll / applications, ymax = ul / applications), color = canva_pal(&quot;Green fields&quot;)(4)[1], size = 2.5, alpha = 1/4) + geom_pointrange(aes(ymin = Q2.5 / applications, ymax = Q97.5 / applications, y = Estimate/applications), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1/2, shape = 1) + geom_point(aes(y = admit/applications), color = canva_pal(&quot;Green fields&quot;)(4)[2], size = 2) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;Posterior validation check&quot;, y = &quot;Admittance probability&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) As in the text, the raw data are consistent with the prediction intervals. But those intervals are so incredibly wide, they’re hardly an endorsement of the model. Once we learn about hierarchical models, we’ll be able to do much better. 11.3.2 Negative-binomial or gamma-Poisson. Recall the Poisson distribution presumes \\(\\sigma^2\\) scales with \\(\\mu\\). The negative binomial distribution relaxes this assumption and presumes “each Poisson count observation has its own rate. It estimates the shape of a gamma distribution to describe the Poisson rates across cases” (p. 350). Here’s a look at the \\(\\gamma\\) distribution. mu &lt;- 3 theta &lt;- 1 ggplot(data = tibble(x = seq(from = 0, to = 12, by = .01)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = rethinking::dgamma2(x, mu, theta)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_vline(xintercept = mu, linetype = 3, color = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(NULL, breaks = c(0, mu, 10)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + ggtitle(expression(paste(&quot;Our sweet &quot;, gamma, &quot;(3, 1)&quot;))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) 11.3.2.1 Bonus: Let’s fit a negative-binomial model. McElreath didn’t give an example of negative-binomial regression in the text. Here’s one with the UCBadmit data. brm(data = d, family = negbinomial, admit ~ 1 + applicant.gender, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(gamma(0.01, 0.01), class = shape)), # this is the brms default iter = 4000, warmup = 1000, cores = 2, chains = 2, seed = 11) %&gt;% print() ## Family: negbinomial ## Links: mu = log; shape = identity ## Formula: admit ~ 1 + applicant.gender ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 4.70 0.41 3.99 5.59 3547 1.00 ## applicant.gendermale 0.58 0.50 -0.44 1.53 4355 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## shape 1.23 0.48 0.50 2.33 3708 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since the negative-binomial model uses the log link, you need to exponentiate to get the estimates back into the count metric. E.g., exp(4.7) ## [1] 109.9472 Also, you may have noticed we used the brms default prior(gamma(0.01, 0.01), class = shape) for the shape parameter. Here’s what that prior looks like. ggplot(data = tibble(x = seq(from = 0, to = 60, by = .1)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 0.01, 0.01)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:50) + ggtitle(expression(paste(&quot;Our brms default &quot;, gamma, &quot;(0.01, 0.01) prior&quot;))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) 11.3.3 Over-dispersion, entropy, and information criteria. Both the beta-binomial and the gamma-Poisson models are maximum entropy for the same constraints as the regular binomial and Poisson. They just try to account for unobserved heterogeneity in probabilities and rates. So while they can be a lot harder to fit to data, they can be usefully conceptualized much like ordinary binomial and Poisson GLMs. So in terms of model comparison using information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson (negative-binomial) is a Poisson model. (pp. 350–351) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] gridExtra_2.3 broom_0.5.2 ggthemes_4.2.0 ## [4] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [7] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 ## [10] tibble_2.1.3 tidyverse_1.2.1 brms_2.9.0 ## [13] Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 ## [16] StanHeaders_2.18.1 ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.13 ggstance_0.3.1 ## [5] markdown_1.0 rethinking_2.01 ## [7] base64enc_0.1-3 rstudioapi_0.10 ## [9] farver_2.0.3 svUnit_0.7-12 ## [11] DT_0.7 fansi_0.4.0 ## [13] mvtnorm_1.0-10 lubridate_1.7.4 ## [15] xml2_1.2.0 codetools_0.2-16 ## [17] bridgesampling_0.6-0 knitr_1.23 ## [19] shinythemes_1.1.2 zeallot_0.1.0 ## [21] bayesplot_1.7.0 jsonlite_1.6 ## [23] shiny_1.3.2 compiler_3.6.3 ## [25] httr_1.4.0 backports_1.1.4 ## [27] assertthat_0.2.1 Matrix_1.2-17 ## [29] lazyeval_0.2.2 cli_1.1.0 ## [31] later_0.8.0 htmltools_0.3.6 ## [33] prettyunits_1.0.2 tools_3.6.3 ## [35] igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 ## [39] reshape2_1.4.3 V8_2.2 ## [41] cellranger_1.1.0 vctrs_0.1.0 ## [43] nlme_3.1-144 crosstalk_1.0.0 ## [45] xfun_0.7 ps_1.3.0 ## [47] rvest_0.3.4 mime_0.7 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 ## [51] tidybayes_1.1.0 gtools_3.8.1 ## [53] MASS_7.3-51.5 zoo_1.8-6 ## [55] scales_1.1.1.9000 colourpicker_1.0 ## [57] hms_0.4.2 promises_1.0.1 ## [59] Brobdingnag_1.2-6 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.0 ## [63] curl_3.3 loo_2.1.0 ## [65] stringi_1.4.3 dygraphs_1.1.1.6 ## [67] boot_1.3-24 pkgbuild_1.0.3 ## [69] shape_1.4.4 rlang_0.4.0 ## [71] pkgconfig_2.0.2 matrixStats_0.54.0 ## [73] evaluate_0.14 lattice_0.20-38 ## [75] labeling_0.3 rstantools_1.5.1 ## [77] htmlwidgets_1.3 processx_3.3.1 ## [79] tidyselect_0.2.5 plyr_1.8.4 ## [81] magrittr_1.5 bookdown_0.11 ## [83] R6_2.4.0 generics_0.0.2 ## [85] pillar_1.4.1 haven_2.1.0 ## [87] withr_2.1.2 xts_0.11-2 ## [89] abind_1.4-5 modelr_0.1.4 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [93] utf8_1.1.4 rmarkdown_1.13 ## [95] grid_3.6.3 readxl_1.3.1 ## [97] callr_3.2.0 threejs_0.3.1 ## [99] digest_0.6.19 xtable_1.8-4 ## [101] httpuv_1.5.1 stats4_3.6.3 ## [103] munsell_0.5.0 shinyjs_1.0 "],
["multilevel-models.html", "12 Multilevel Models 12.1 Example: Multilevel tadpoles 12.2 Varying effects and the underfitting/overfitting trade-off 12.3 More than one type of cluster 12.4 Multilevel posterior predictions 12.5 Summary Bonus: tidybayes::spread_draws() Reference Session info", " 12 Multilevel Models Multilevel models… remember features of each cluster in the data as they learn about all of the clusters. Depending upon the variation among clusters, which is learned from the data as well, the model pools information across clusters. This pooling tends to improve estimates about each cluster. This improved estimation leads to several, more pragmatic sounding, benefits of the multilevel approach. (p. 356) These benefits include: improved estimates for repeated sampling (i.e., in longitudinal data) improved estimates when there are imbalances among subsamples estimates of the variation across subsamples avoiding simplistic averaging by retaining variation across subsamples All of these benefits flow out of the same strategy and model structure. You learn one basic design and you get all of this for free. When it comes to regression, multilevel regression deserves to be the default approach. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous. It is better to begin to build a multilevel analysis, and then realize it’s unnecessary, than to overlook it. And once you grasp the basic multilevel stragety, it becomes much easier to incorporate related tricks such as allowing for measurement error in the data and even model missing data itself (Chapter 14). (p. 356) I’m totally on board with this. After learning about the multilevel model, I see it everywhere. For more on the sentiment it should be the default, check out McElreath’s blog post, Multilevel Regression as Default. 12.1 Example: Multilevel tadpoles Let’s get the reedfrogs data from rethinking. library(rethinking) data(reedfrogs) d &lt;- reedfrogs Detach rethinking and load brms. rm(reedfrogs) detach(package:rethinking, unload = T) library(brms) Go ahead and acquaint yourself with the reedfrogs. library(tidyverse) d %&gt;% glimpse() ## Observations: 48 ## Variables: 5 ## $ density &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25, 25, 25, … ## $ pred &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pred, pred, pred, pred, … ## $ size &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, big, big, small, small… ## $ surv &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23, 22, 25, 23, 23, 23, 2… ## $ propsurv &lt;dbl&gt; 0.9000000, 1.0000000, 0.7000000, 1.0000000, 0.9000000, 0.9000000, 1.0000000, 0.9… Making the tank cluster variable is easy. d &lt;- d %&gt;% mutate(tank = 1:nrow(d)) Here’s the formula for the un-pooled model in which each tank gets its own intercept. \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim \\text{Normal} (0, 5) \\end{align*}\\] And \\(n_i = \\text{density}_i\\). Now we’ll fit this simple aggregated binomial model much like we practiced in Chapter 10. b12.1 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 0 + factor(tank), prior(normal(0, 5), class = b), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 12) The formula for the multilevel alternative is \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] You specify the corresponding multilevel model like this. b12.2 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 1 + (1 | tank), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 12) The syntax for the varying effects follows the lme4 style, ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(cauchy(0, 1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. Let’s do the WAIC comparisons. b12.1 &lt;- add_criterion(b12.1, &quot;waic&quot;) b12.2 &lt;- add_criterion(b12.2, &quot;waic&quot;) w &lt;- loo_compare(b12.1, b12.2, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b12.2 0.0 0.0 -99.9 3.6 20.8 0.8 199.9 7.3 ## b12.1 -0.5 2.2 -100.4 4.6 22.4 0.6 200.9 9.3 The se_diff is large relative to the elpd_diff. If we convert the \\(\\text{elpd}\\) difference to the WAIC metric, the message stays the same. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b12.2 0.0000000 0.000000 ## b12.1 0.9724156 4.491781 I’m not going to show it here, but if you’d like a challenge, try comparing the models with the LOO. You’ll learn all about high pareto_k values, kfold() recommendations, and challenges implementing those kfold() recommendations. If you’re interested, pour yourself a calming adult beverage, execute the code below, and check out the Kfold(): “Error: New factor levels are not allowed” thread in the Stan forums. b12.1 &lt;- add_criterion(b12.1, &quot;loo&quot;) b12.2 &lt;- add_criterion(b12.2, &quot;loo&quot;) But back on track, here’s our prep work for Figure 12.1. post &lt;- posterior_samples(b12.2, add_chain = T) post_mdn &lt;- coef(b12.2, robust = T)$tank[, , ] %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(post_mdn = inv_logit_scaled(Estimate)) post_mdn ## # A tibble: 48 x 11 ## Estimate Est.Error Q2.5 Q97.5 density pred size surv propsurv tank post_mdn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2.08 0.852 0.629 4.00 10 no big 9 0.9 1 0.889 ## 2 2.96 1.04 1.18 5.49 10 no big 10 1 2 0.951 ## 3 0.969 0.680 -0.226 2.41 10 no big 7 0.7 3 0.725 ## 4 2.95 1.07 1.16 5.44 10 no big 10 1 4 0.950 ## 5 2.06 0.852 0.599 4.00 10 no small 9 0.9 5 0.887 ## 6 2.06 0.849 0.587 4.06 10 no small 9 0.9 6 0.886 ## 7 2.96 1.10 1.18 5.46 10 no small 10 1 7 0.951 ## 8 2.07 0.850 0.617 4.02 10 no small 9 0.9 8 0.888 ## 9 -0.178 0.604 -1.42 1.03 10 pred big 4 0.4 9 0.456 ## 10 2.07 0.832 0.613 4.04 10 pred big 9 0.9 10 0.888 ## # … with 38 more rows For kicks and giggles, let’s use a FiveThirtyEight-like theme for this chapter’s plots. An easy way to do so is with help from the ggthemes package. # install.packages(&quot;ggthemes&quot;, dependencies = T) library(ggthemes) Finally, here’s the ggplot2 code to reproduce Figure 12.1. post_mdn %&gt;% ggplot(aes(x = tank)) + geom_hline(yintercept = inv_logit_scaled(median(post$b_Intercept)), linetype = 2, size = 1/4) + geom_vline(xintercept = c(16.5, 32.5), size = 1/4) + geom_point(aes(y = propsurv), color = &quot;orange2&quot;) + geom_point(aes(y = post_mdn), shape = 1) + coord_cartesian(ylim = c(0, 1)) + scale_x_continuous(breaks = c(1, 16, 32, 48)) + labs(title = &quot;Multilevel shrinkage!&quot;, subtitle = &quot;The empirical proportions are in orange while the model-\\nimplied proportions are the black circles. The dashed line is\\nthe model-implied average survival proportion.&quot;) + annotate(&quot;text&quot;, x = c(8, 16 + 8, 32 + 8), y = 0, label = c(&quot;small tanks&quot;, &quot;medium tanks&quot;, &quot;large tanks&quot;)) + theme_fivethirtyeight() + theme(panel.grid = element_blank()) Here is our version of Figure 12.2.a. # this makes the output of `sample_n()` reproducible set.seed(12) post %&gt;% sample_n(100) %&gt;% expand(nesting(iter, b_Intercept, sd_tank__Intercept), x = seq(from = -4, to = 5, length.out = 100)) %&gt;% ggplot(aes(x = x, group = iter)) + geom_line(aes(y = dnorm(x, b_Intercept, sd_tank__Intercept)), alpha = .2, color = &quot;orange2&quot;) + labs(title = &quot;Population survival distribution&quot;, subtitle = &quot;The Gaussians are on the log-odds scale.&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-3, 4)) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 13), plot.subtitle = element_text(size = 10)) Note the uncertainty in terms of both location \\(\\alpha\\) and scale \\(\\sigma\\). Now here’s the code for Figure 12.2.b. ggplot(data = post, aes(x = rnorm(n = nrow(post), mean = b_Intercept, sd = sd_tank__Intercept) %&gt;% inv_logit_scaled())) + geom_density(size = 0, fill = &quot;orange2&quot;) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Probability of survival&quot;) + theme_fivethirtyeight() Note how we sampled 12,000 imaginary tanks rather than McElreath’s 8,000. This is because we had 12,000 HMC iterations (i.e., execute nrow(post)). The aes() code, above, was a bit much. To get a sense of how it worked, consider this: set.seed(12) rnorm(n = 1, mean = post$b_Intercept, sd = post$sd_tank__Intercept) %&gt;% inv_logit_scaled() ## [1] 0.2902134 First, we took one random draw from a normal distribution with a mean of the first row in post$b_Intercept and a standard deviation of the value from the first row in post$sd_tank__Intercept, and passed it through the inv_logit_scaled() function. By replacing the 1 with nrow(post), we do this nrow(post) times (i.e., 12,000). Our orange density, then, is the summary of that process. 12.1.0.1 Overthinking: Prior for variance components. Yep, you can use the exponential distribution for your priors in brms. Here it is for model b12.2. b12.2.e &lt;- update(b12.2, prior = c(prior(normal(0, 1), class = Intercept), prior(exponential(1), class = sd))) The model summary: print(b12.2.e) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.61 0.21 1.25 2.08 3270 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.31 0.25 0.83 1.81 2196 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you’re curious how the exponential prior compares to the posterior, you might just plot. tibble(x = seq(from = 0, to = 6, by = .01)) %&gt;% ggplot() + # the prior geom_ribbon(aes(x = x, ymin = 0, ymax = dexp(x, rate = 1)), fill = &quot;orange2&quot;, alpha = 1/3) + # the posterior geom_density(data = posterior_samples(b12.2.e), aes(x = sd_tank__Intercept), fill = &quot;orange2&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 5)) + labs(title = &quot;Bonus prior/posterior plot\\nfor sd_tank__Intercept&quot;, subtitle = &quot;The prior is the semitransparent ramp in the\\nbackground. The posterior is the solid orange\\nmound.&quot;) + theme_fivethirtyeight() 12.2 Varying effects and the underfitting/overfitting trade-off Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp… A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provides better estimates is that they do a better job trading off underfitting and overfitting. (p. 364) In this section, we explicate this by contrasting three perspectives: Complete pooling (i.e., a single-\\(\\alpha\\) model) No pooling (i.e., the single-level \\(\\alpha_{\\text{tank}_i}\\) model) Partial pooling (i.e., the multilevel model for which \\(\\alpha_{\\text{tank}} \\sim \\text{Normal} (\\alpha, \\sigma)\\)) To demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 365) 12.2.1 The model. The simulation formula should look familiar. \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{pond}_i} \\\\ \\alpha_{\\text{pond}} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] 12.2.2 Assign values to the parameters. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(12) ( dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) ) ## # A tibble: 60 x 3 ## pond ni true_a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 -0.821 ## 2 2 5 3.77 ## 3 3 5 -0.0351 ## 4 4 5 0.0200 ## 5 5 5 -1.60 ## 6 6 5 0.992 ## 7 7 5 0.927 ## 8 8 5 0.458 ## 9 9 5 1.24 ## 10 10 5 2.04 ## # … with 50 more rows 12.2.3 Sumulate survivors. Each pond \\(i\\) has \\(n_i\\) potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival \\(p_i\\). This probability \\(p_i\\) is implied by the model definition, and is equal to: \\[p_i = \\frac{\\text{exp} (\\alpha_i)}{1 + \\text{exp} (\\alpha_i)}\\] The model uses a logit link, and so the probability is defined by the [inv_logit_scaled()] function. (p. 367) set.seed(12) ( dsim &lt;- dsim %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) ) ## # A tibble: 60 x 4 ## pond ni true_a si ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 5 -0.821 0 ## 2 2 5 3.77 5 ## 3 3 5 -0.0351 4 ## 4 4 5 0.0200 3 ## 5 5 5 -1.60 0 ## 6 6 5 0.992 5 ## 7 7 5 0.927 5 ## 8 8 5 0.458 3 ## 9 9 5 1.24 5 ## 10 10 5 2.04 5 ## # … with 50 more rows 12.2.4 Compute the no-pooling estimates. The no-pooling estimates (i.e., \\(\\alpha_{\\text{tank}_i}\\)) are the results of simple algebra. ( dsim &lt;- dsim %&gt;% mutate(p_nopool = si / ni) ) ## # A tibble: 60 x 5 ## pond ni true_a si p_nopool ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 -0.821 0 0 ## 2 2 5 3.77 5 1 ## 3 3 5 -0.0351 4 0.8 ## 4 4 5 0.0200 3 0.6 ## 5 5 5 -1.60 0 0 ## 6 6 5 0.992 5 1 ## 7 7 5 0.927 5 1 ## 8 8 5 0.458 3 0.6 ## 9 9 5 1.24 5 1 ## 10 10 5 2.04 5 1 ## # … with 50 more rows “These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induce no regularization” (p. 367). 12.2.5 Compute the partial-pooling estimates. To follow along with McElreath, set chains = 1, cores = 1 to fit with one chain. b12.3 &lt;- brm(data = dsim, family = binomial, si | trials(ni) ~ 1 + (1 | pond), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 10000, warmup = 1000, chains = 1, cores = 1, seed = 12) print(b12.3) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.30 0.19 0.97 1.71 3186 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.28 0.20 0.91 1.68 3016 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not aware that you can use McElreath’s depth=2 trick in brms for summary() or print(). But can get that information with the coef() function. coef(b12.3)$pond[c(1:2, 59:60), , ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.08 0.89 -3.01 0.54 ## 2 2.30 1.02 0.49 4.47 ## 59 0.97 0.37 0.26 1.71 ## 60 1.42 0.41 0.66 2.25 Note how we just peeked at the top and bottom two rows with the c(1:2, 59:60) part of the code, there. Somewhat discouragingly, coef() doesn’t return the ‘Eff.Sample’ or ‘Rhat’ columns as in McElreath’s output. We can still extract that information, though. For \\(\\hat{R}\\), the solution is simple; use the brms::rhat() function. rhat(b12.3) ## b_Intercept sd_pond__Intercept r_pond[1,Intercept] r_pond[2,Intercept] ## 1.0010076 0.9999010 1.0000283 0.9999926 ## r_pond[3,Intercept] r_pond[4,Intercept] r_pond[5,Intercept] r_pond[6,Intercept] ## 0.9999020 0.9999937 0.9998954 0.9999349 ## r_pond[7,Intercept] r_pond[8,Intercept] r_pond[9,Intercept] r_pond[10,Intercept] ## 0.9999039 0.9998913 0.9999165 0.9998949 ## r_pond[11,Intercept] r_pond[12,Intercept] r_pond[13,Intercept] r_pond[14,Intercept] ## 0.9999421 1.0000767 0.9999453 0.9999021 ## r_pond[15,Intercept] r_pond[16,Intercept] r_pond[17,Intercept] r_pond[18,Intercept] ## 1.0001642 1.0001294 1.0000320 0.9998950 ## r_pond[19,Intercept] r_pond[20,Intercept] r_pond[21,Intercept] r_pond[22,Intercept] ## 0.9998891 1.0000294 0.9999855 0.9999565 ## r_pond[23,Intercept] r_pond[24,Intercept] r_pond[25,Intercept] r_pond[26,Intercept] ## 0.9998916 0.9998919 1.0001516 1.0002602 ## r_pond[27,Intercept] r_pond[28,Intercept] r_pond[29,Intercept] r_pond[30,Intercept] ## 1.0001181 0.9998902 0.9998953 0.9999522 ## r_pond[31,Intercept] r_pond[32,Intercept] r_pond[33,Intercept] r_pond[34,Intercept] ## 1.0002802 0.9998892 1.0006365 1.0002835 ## r_pond[35,Intercept] r_pond[36,Intercept] r_pond[37,Intercept] r_pond[38,Intercept] ## 1.0001983 1.0004425 1.0000902 1.0000279 ## r_pond[39,Intercept] r_pond[40,Intercept] r_pond[41,Intercept] r_pond[42,Intercept] ## 1.0001111 1.0000348 1.0002134 1.0004902 ## r_pond[43,Intercept] r_pond[44,Intercept] r_pond[45,Intercept] r_pond[46,Intercept] ## 1.0006306 1.0004420 1.0000674 0.9999369 ## r_pond[47,Intercept] r_pond[48,Intercept] r_pond[49,Intercept] r_pond[50,Intercept] ## 0.9999679 0.9999494 0.9998969 1.0000209 ## r_pond[51,Intercept] r_pond[52,Intercept] r_pond[53,Intercept] r_pond[54,Intercept] ## 1.0002716 1.0004067 0.9999185 0.9998901 ## r_pond[55,Intercept] r_pond[56,Intercept] r_pond[57,Intercept] r_pond[58,Intercept] ## 1.0003178 1.0002133 0.9999249 0.9999009 ## r_pond[59,Intercept] r_pond[60,Intercept] lp__ ## 1.0001104 1.0000400 1.0000022 Extracting the ‘Eff.Sample’ values is a little more complicated. There is no effsamples() function. However, we do have neff_ratio(). neff_ratio(b12.3) ## b_Intercept sd_pond__Intercept r_pond[1,Intercept] r_pond[2,Intercept] ## 0.3350679 0.3540338 1.2134717 1.6142412 ## r_pond[3,Intercept] r_pond[4,Intercept] r_pond[5,Intercept] r_pond[6,Intercept] ## 1.8111203 1.5697587 1.3219172 1.6447324 ## r_pond[7,Intercept] r_pond[8,Intercept] r_pond[9,Intercept] r_pond[10,Intercept] ## 1.6139521 1.9021429 1.4611080 1.7580230 ## r_pond[11,Intercept] r_pond[12,Intercept] r_pond[13,Intercept] r_pond[14,Intercept] ## 1.9484691 1.8707600 1.7644988 2.0296344 ## r_pond[15,Intercept] r_pond[16,Intercept] r_pond[17,Intercept] r_pond[18,Intercept] ## 1.8619893 1.3099899 1.3956490 1.6420323 ## r_pond[19,Intercept] r_pond[20,Intercept] r_pond[21,Intercept] r_pond[22,Intercept] ## 1.7322525 1.6474163 1.7327984 1.3041271 ## r_pond[23,Intercept] r_pond[24,Intercept] r_pond[25,Intercept] r_pond[26,Intercept] ## 1.4883350 1.5551357 1.3973067 1.5916201 ## r_pond[27,Intercept] r_pond[28,Intercept] r_pond[29,Intercept] r_pond[30,Intercept] ## 1.6421408 1.7900393 1.7344241 1.7519775 ## r_pond[31,Intercept] r_pond[32,Intercept] r_pond[33,Intercept] r_pond[34,Intercept] ## 1.4170788 1.4571529 0.9085235 0.9761805 ## r_pond[35,Intercept] r_pond[36,Intercept] r_pond[37,Intercept] r_pond[38,Intercept] ## 1.0005429 1.0015009 1.1238062 0.9988977 ## r_pond[39,Intercept] r_pond[40,Intercept] r_pond[41,Intercept] r_pond[42,Intercept] ## 1.3998742 1.0532292 1.1991255 0.9586475 ## r_pond[43,Intercept] r_pond[44,Intercept] r_pond[45,Intercept] r_pond[46,Intercept] ## 1.0581333 1.0558455 0.9889237 1.2660532 ## r_pond[47,Intercept] r_pond[48,Intercept] r_pond[49,Intercept] r_pond[50,Intercept] ## 0.8995469 1.3890079 1.3271646 0.9313268 ## r_pond[51,Intercept] r_pond[52,Intercept] r_pond[53,Intercept] r_pond[54,Intercept] ## 0.9957368 0.9632408 1.1974916 1.1699048 ## r_pond[55,Intercept] r_pond[56,Intercept] r_pond[57,Intercept] r_pond[58,Intercept] ## 0.7901757 1.0671998 1.1244886 0.8253512 ## r_pond[59,Intercept] r_pond[60,Intercept] lp__ ## 1.0000085 1.0162799 0.2343567 The brms::neff_ratio() function returns ratios of the effective samples over the total number of post-warmup iterations. So if we know the neff_ratio() values and the number of post-warmup iterations, the ‘Eff.Sample’ values are just a little algebra away. A quick solution is to look at the ‘total post-warmup samples’ line at the top of our print() output. Another way is to extract that information from our brm() fit object. I’m not aware of a way to do that directly, but we can extract the iter value (i.e., b12.2$fit@sim$iter), the warmup value (i.e., b12.2$fit@sim$warmup), and the number of chains (i.e., b12.2$fit@sim$chains). With those values in hand, simple algebra will return the ‘total post-warmup samples’ value. E.g., (n_iter &lt;- (b12.3$fit@sim$iter - b12.3$fit@sim$warmup) * b12.3$fit@sim$chains) ## [1] 9000 And now we have n_iter, we can calculate the ‘Eff.Sample’ values. neff_ratio(b12.3) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;parameter&quot;, &quot;neff_ratio&quot;) %&gt;% mutate(eff_sample = (neff_ratio * n_iter) %&gt;% round(digits = 0)) %&gt;% head() ## parameter neff_ratio eff_sample ## 1 b_Intercept 0.3350679 3016 ## 2 sd_pond__Intercept 0.3540338 3186 ## 3 r_pond[1,Intercept] 1.2134717 10921 ## 4 r_pond[2,Intercept] 1.6142412 14528 ## 5 r_pond[3,Intercept] 1.8111203 16300 ## 6 r_pond[4,Intercept] 1.5697587 14128 Digressions aside, let’s get ready for the diagnostic plot of Figure 12.3. dsim %&gt;% glimpse() ## Observations: 60 ## Variables: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10,… ## $ true_a &lt;dbl&gt; -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1.59646315, 0.99155593, 0.926… ## $ si &lt;int&gt; 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6, 10, 9, 9, 9, 9, 10, 10, 6, 3, 8,… ## $ p_nopool &lt;dbl&gt; 0.00, 1.00, 0.80, 0.60, 0.00, 1.00, 1.00, 0.60, 1.00, 1.00, 0.60, 0.60, 0.60, 0.… # we could have included this step in the block of code below, if we wanted to p_partpool &lt;- coef(b12.3)$pond[, , ] %&gt;% as_tibble() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) dsim &lt;- dsim %&gt;% bind_cols(p_partpool) %&gt;% mutate(p_true = inv_logit_scaled(true_a)) %&gt;% mutate(nopool_error = abs(p_nopool - p_true), partpool_error = abs(p_partpool - p_true)) dsim %&gt;% glimpse() ## Observations: 60 ## Variables: 9 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 1… ## $ true_a &lt;dbl&gt; -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1.59646315, 0.99155593,… ## $ si &lt;int&gt; 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6, 10, 9, 9, 9, 9, 10, 10, 6,… ## $ p_nopool &lt;dbl&gt; 0.00, 1.00, 0.80, 0.60, 0.00, 1.00, 1.00, 0.60, 1.00, 1.00, 0.60, 0.60, 0.… ## $ p_partpool &lt;dbl&gt; 0.2534224, 0.9091564, 0.8106577, 0.6825460, 0.2580885, 0.9085185, 0.909099… ## $ p_true &lt;dbl&gt; 0.3055830, 0.9773737, 0.4912217, 0.5049979, 0.1684765, 0.7293951, 0.716461… ## $ nopool_error &lt;dbl&gt; 0.305582963, 0.022626343, 0.308778278, 0.095002134, 0.168476520, 0.2706048… ## $ partpool_error &lt;dbl&gt; 0.052160537, 0.068217208, 0.319435969, 0.177548141, 0.089612009, 0.1791233… Here is our code for Figure 12.3. The extra data processing for dfline is how we get the values necessary for the horizontal summary lines. dfline &lt;- dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key, ni) %&gt;% summarise(mean_error = mean(value)) %&gt;% mutate(x = c( 1, 16, 31, 46), xend = c(15, 30, 45, 60)) dsim %&gt;% ggplot(aes(x = pond)) + geom_vline(xintercept = c(15.5, 30.5, 45.4), color = &quot;white&quot;, size = 2/3) + geom_point(aes(y = nopool_error), color = &quot;orange2&quot;) + geom_point(aes(y = partpool_error), shape = 1) + geom_segment(data = dfline, aes(x = x, xend = xend, y = mean_error, yend = mean_error), color = rep(c(&quot;orange2&quot;, &quot;black&quot;), each = 4), linetype = rep(1:2, each = 4)) + scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) + annotate(&quot;text&quot;, x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, label = c(&quot;tiny (5)&quot;, &quot;small (10)&quot;, &quot;medium (25)&quot;, &quot;large (35)&quot;)) + labs(y = &quot;absolute error&quot;, title = &quot;Estimate error by model type&quot;, subtitle = &quot;The horizontal axis displays pond number. The vertical axis measures\\nthe absolute error in the predicted proportion of survivors, compared to\\nthe true value used in the simulation. The higher the point, the worse\\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\\nThe orange and dashed black lines show the average error for each kind\\nof estimate, across each initial density of tadpoles (pond size). Smaller\\nponds produce more error, but the partial pooling estimates are better\\non average, especially in smaller ponds.&quot;) + theme_fivethirtyeight() + theme(panel.grid = element_blank(), plot.subtitle = element_text(size = 10)) If you wanted to quantify the difference in simple summaries, you might do something like this: dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key) %&gt;% summarise(mean_error = mean(value) %&gt;% round(digits = 3), median_error = median(value) %&gt;% round(digits = 3)) ## # A tibble: 2 x 3 ## key mean_error median_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nopool_error 0.078 0.05 ## 2 partpool_error 0.067 0.052 I originally learned about the multilevel in order to work with longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, took me some time to wrap my head around. If you’re struggling with this, be patient and keep chipping away. When McElreath lectured on this topic in 2015, he traced partial pooling to statistician Charles M. Stein. In 1977, Efron and Morris wrote the now classic paper, Stein’s Paradox in Statistics, which does a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you’d like more practice seeing how partial pooling works–or if you just like baseball–, check out my blog post, Stein’s Paradox and What Partial Pooling Can Do For You. 12.2.5.1 Overthinking: Repeating the pond simulation. Within the brms workflow, we can reuse a compiled model with update(). But first, we’ll simulate new data. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(1999) # for new data, set a new seed new_dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) %&gt;% mutate(p_nopool = si / ni) glimpse(new_dsim) ## Observations: 60 ## Variables: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10,… ## $ true_a &lt;dbl&gt; 2.4990087, 1.3432554, 3.2045137, 3.6047030, 1.6005354, 2.1797409, 0.5759270, -0.… ## $ si &lt;int&gt; 4, 4, 5, 4, 4, 4, 2, 4, 3, 5, 4, 5, 2, 2, 5, 10, 7, 10, 10, 8, 10, 9, 5, 10, 10,… ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 1.00, 0.80, 0.80, 0.80, 0.40, 0.80, 0.60, 1.00, 0.80, 1.00, 0.40, 0.… Fit the new model. b12.3_new &lt;- update(b12.3, newdata = new_dsim, iter = 10000, warmup = 1000, chains = 1, cores = 1) print(b12.3_new) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: new_dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.26 0.18 0.95 1.65 2944 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.52 0.20 1.14 1.91 3446 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Why not plot the first simulation versus the second one? bind_rows(posterior_samples(b12.3), posterior_samples(b12.3_new)) %&gt;% mutate(model = rep(c(&quot;b12.3&quot;, &quot;b12.3_new&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = sd_pond__Intercept)) + stat_density_2d(geom = &quot;raster&quot;, aes(fill = stat(density)), contour = F, n = 200) + geom_vline(xintercept = a, color = &quot;orange3&quot;, linetype = 3) + geom_hline(yintercept = sigma, color = &quot;orange3&quot;, linetype = 3) + scale_fill_gradient(low = &quot;grey25&quot;, high = &quot;orange3&quot;) + ggtitle(&quot;Our simulation posteriors contrast a bit&quot;, subtitle = expression(paste(alpha, &quot; is on the x and &quot;, sigma, &quot; is on the y, both in log-odds. The dotted lines intersect at the true values.&quot;))) + coord_cartesian(xlim = c(.7, 2), ylim = c(.8, 1.9)) + theme_fivethirtyeight() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~model, ncol = 2) If you’d like the stanfit portion of your brm() object, subset with $fit. Take b12.3, for example. You might check out its structure via b12.3$fit %&gt;% str(). Here’s the actual Stan code. b12.3$fit@ stanmodel ## S4 class stanmodel &#39;124b09250c4c1f2a0b31156b2b547c4a&#39; coded as follows: ## // generated with brms 2.9.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## int Y[N]; // response variable ## int trials[N]; // number of trials ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; N_1; ## int&lt;lower=1&gt; M_1; ## int&lt;lower=1&gt; J_1[N]; ## vector[N] Z_1_1; ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real temp_Intercept; // temporary intercept ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## vector[N_1] z_1[M_1]; // unscaled group-level effects ## } ## transformed parameters { ## // group-level effects ## vector[N_1] r_1_1 = (sd_1[1] * (z_1[1])); ## } ## model { ## vector[N] mu = temp_Intercept + rep_vector(0, N); ## for (n in 1:N) { ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; ## } ## // priors including all constants ## target += normal_lpdf(temp_Intercept | 0, 1); ## target += cauchy_lpdf(sd_1 | 0, 1) ## - 1 * cauchy_lccdf(0 | 0, 1); ## target += normal_lpdf(z_1[1] | 0, 1); ## // likelihood including all constants ## if (!prior_only) { ## target += binomial_logit_lpmf(Y | trials, mu); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = temp_Intercept; ## } ## And you can get the data of a given brm() fit object like so. b12.3$data %&gt;% head() ## si ni pond ## 1 0 5 1 ## 2 5 5 2 ## 3 4 5 3 ## 4 3 5 4 ## 5 0 5 5 ## 6 5 5 6 12.3 More than one type of cluster “We can use and often should use more than one type of cluster in the same model” (p. 370). 12.3.1 Multilevel chimpanzees. The initial multilevel update from model b10.4 from the last chapter follows the statistical formula \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] Notice that \\(\\alpha\\) is inside the linear model, not inside the Gaussian prior for \\(\\alpha_\\text{actor}\\). This is mathematically equivalent to what [we] did with the tadpoles earlier in the chapter. You can always take the mean out of a Gaussian distribution and treat that distribution as a constant plus a Gaussian distribution centered on zero. This might seem a little weird at first, so it might help train your intuition by experimenting in R. (p. 371) Behold our two identical Gaussians in a tidy tibble. set.seed(12) two_gaussians &lt;- tibble(y1 = rnorm(n = 1e4, mean = 10, sd = 1), y2 = 10 + rnorm(n = 1e4, mean = 0, sd = 1)) Let’s follow McElreath’s advice to make sure they are same by superimposing the density of one on the other. two_gaussians %&gt;% ggplot() + geom_density(aes(x = y1), size = 0, fill = &quot;orange1&quot;, alpha = 1/3) + geom_density(aes(x = y2), size = 0, fill = &quot;orange4&quot;, alpha = 1/3) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Our simulated Gaussians&quot;) + theme_fivethirtyeight() Yep, those Gaussians look about the same. Let’s get the chimpanzees data from rethinking. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Detach rethinking and reload brms. rm(chimpanzees) detach(package:rethinking, unload = T) library(brms) For our brms model with varying intercepts for actor but not block, we employ the pulled_left ~ 1 + ... + (1 | actor) syntax, specifically omitting a (1 | block) section. b12.4 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), # I&#39;m using 4 cores, instead of the `cores=3` in McElreath&#39;s code iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12) The initial solutions came with a few divergent transitions. Increasing adapt_delta to 0.95 solved the problem. You can also solve the problem with more strongly regularizing priors such as normal(0, 2) on the intercept and slope parameters (see recommendations from the Stan team). Consider trying both methods and comparing the results. They’re similar. Here we add the actor-level deviations to the fixed intercept, the grand mean. post &lt;- posterior_samples(b12.4) post %&gt;% select(starts_with(&quot;r_actor&quot;)) %&gt;% gather() %&gt;% # this is how we might add the grand mean to the actor-level deviations mutate(value = value + post$b_Intercept) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value) %&gt;% round(digits = 2)) ## # A tibble: 7 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 r_actor[1,Intercept] -0.71 ## 2 r_actor[2,Intercept] 4.55 ## 3 r_actor[3,Intercept] -1.02 ## 4 r_actor[4,Intercept] -1.02 ## 5 r_actor[5,Intercept] -0.71 ## 6 r_actor[6,Intercept] 0.23 ## 7 r_actor[7,Intercept] 1.77 Here’s another way to get at the same information, this time using coef() and a little formatting help from the stringr::str_c() function. Just for kicks, we’ll throw in the 95% intervals, too. coef(b12.4)$actor[, c(1, 3:4), 1] %&gt;% as_tibble() %&gt;% round(digits = 2) %&gt;% # here we put the credible intervals in an APA-6-style format mutate(`95% CIs` = str_c(&quot;[&quot;, Q2.5, &quot;, &quot;, Q97.5, &quot;]&quot;), actor = str_c(&quot;chimp #&quot;, 1:7)) %&gt;% rename(mean = Estimate) %&gt;% select(actor, mean, `95% CIs`) %&gt;% knitr::kable() actor mean 95% CIs chimp #1 -0.71 [-1.25, -0.19] chimp #2 4.55 [2.56, 8.29] chimp #3 -1.02 [-1.59, -0.48] chimp #4 -1.02 [-1.57, -0.48] chimp #5 -0.71 [-1.24, -0.2] chimp #6 0.23 [-0.29, 0.76] chimp #7 1.77 [1.04, 2.59] If you prefer the posterior median to the mean, just add a robust = T argument inside the coef() function. 12.3.2 Two types of cluster. The full statistical model follows the form \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha_{\\text{block}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_{\\text{block}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] Our brms model with varying intercepts for both actor and block now employs the ... (1 | actor) + (1 | block) syntax. b12.5 &lt;- update(b12.4, newdata = d, formula = pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) + (1 | block), iter = 6000, warmup = 1000, cores = 4, chains = 4, control = list(adapt_delta = 0.99), seed = 12) This time we increased adapt_delta to 0.99 to avoid divergent transitions. We can look at the primary coefficients with print(). McElreath encouraged us to inspect the trace plots. Here they are. library(bayesplot) color_scheme_set(&quot;orange&quot;) post &lt;- posterior_samples(b12.5, add_chain = T) post %&gt;% select(-lp__, -iter) %&gt;% mcmc_trace(facet_args = list(ncol = 4)) + scale_x_continuous(breaks = c(0, 2500, 5000)) + theme_fivethirtyeight() + theme(legend.position = c(.75, .06)) The trace plots look great. We may as well examine the \\(n_\\text{eff} / N\\) ratios, too. neff_ratio(b12.5) %&gt;% mcmc_neff() + theme_fivethirtyeight() About half of them are lower than we might like, but none are in the embarrassing \\(n_\\text{eff} / N \\leq .1\\) range. Let’s look at the summary of the main parameters. print(b12.5) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ prosoc_left + (1 | actor) + (1 | block) + prosoc_left:condition ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 6000; warmup = 1000; thin = 1; ## total post-warmup samples = 20000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 2.29 0.95 1.13 4.67 5323 1.00 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.22 0.18 0.01 0.68 8197 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.46 0.97 -1.39 2.51 4400 1.00 ## prosoc_left 0.83 0.26 0.31 1.35 16343 1.00 ## prosoc_left:condition -0.14 0.30 -0.73 0.46 16404 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time, we’ll need to use brms::ranef() to get those depth=2-type estimates in the same metric displayed in the text. With ranef(), you get the group-specific estimates in a deviance metric. The coef() function, in contrast, yields the group-specific estimates in what you might call the natural metric. We’ll get more language for this in the next chapter. ranef(b12.5)$actor[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.18 0.98 -3.27 0.68 ## 2 4.18 1.68 1.79 8.24 ## 3 -1.48 0.99 -3.57 0.35 ## 4 -1.48 0.98 -3.58 0.38 ## 5 -1.18 0.98 -3.27 0.68 ## 6 -0.23 0.98 -2.29 1.62 ## 7 1.32 1.00 -0.73 3.27 ranef(b12.5)$block[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.18 0.23 -0.75 0.13 ## 2 0.04 0.19 -0.33 0.46 ## 3 0.06 0.19 -0.30 0.50 ## 4 0.01 0.18 -0.38 0.40 ## 5 -0.03 0.19 -0.46 0.33 ## 6 0.11 0.20 -0.21 0.60 We might make the coefficient plot of Figure 12.4.a like this: stanplot(b12.5, pars = c(&quot;^r_&quot;, &quot;^b_&quot;, &quot;^sd_&quot;)) + theme_fivethirtyeight() + theme(axis.text.y = element_text(hjust = 0)) Once we get the posterior samples, it’s easy to compare the random variances as in Figure 12.4.b. post %&gt;% ggplot(aes(x = sd_actor__Intercept)) + geom_density(size = 0, fill = &quot;orange1&quot;, alpha = 3/4) + geom_density(aes(x = sd_block__Intercept), size = 0, fill = &quot;orange4&quot;, alpha = 3/4) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 4)) + labs(title = expression(sigma)) + annotate(&quot;text&quot;, x = 2/3, y = 2, label = &quot;block&quot;, color = &quot;orange4&quot;) + annotate(&quot;text&quot;, x = 2, y = 3/4, label = &quot;actor&quot;, color = &quot;orange1&quot;) + theme_fivethirtyeight() We might compare our models by their PSIS-LOO values. b12.4 &lt;- add_criterion(b12.4, &quot;loo&quot;) b12.5 &lt;- add_criterion(b12.5, &quot;loo&quot;) loo_compare(b12.4, b12.5) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b12.4 0.0 0.0 -265.8 9.8 8.2 0.4 531.7 19.5 ## b12.5 -0.4 0.9 -266.3 9.9 10.3 0.5 532.6 19.7 The two models yield nearly-equivalent information criteria values. Yet recall what McElreath wrote: “There is nothing to gain here by selecting either model. The comparison of the two models tells a richer story” (p. 367). 12.4 Multilevel posterior predictions … producing implied predictions from a fit model, is very helpful for understanding what the model means. Every model is a merger of sense and nonsense. When we understand a model, we can find its sense and control its nonsense. But as models get more complex, it is very difficult to impossible to understand them just by inspecting tables of posterior means and intervals. Exploring implied posterior predictions helps much more… … The introduction of varying effects does introduce nuance, however. First, we should no longer expect the model to exactly retrodict the sample, because adaptive regularization has as its goal to trade off poorer fit in sample for better inference and hopefully better fit out of sample. This is what shrinkage does for us… Second, “prediction” in the context of a multilevel model requires additional choices. If we wish to validate a model against the specific clusters used to fit the model, that is one thing. But if we instead wish to compute predictions for new clusters, other than the one observed in the sample, that is quite another. We’ll consider each of these in turn, continuing to use the chimpanzees model from the previous section. (p. 376) 12.4.1 Posterior prediction for same clusters. Like McElreath did in the text, we’ll do this two ways. Recall we use brms::fitted() in place of rethinking::link(). chimp &lt;- 2 nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1), actor = chimp) ( chimp_2_fitted &lt;- fitted(b12.4, newdata = nd) %&gt;% as_tibble() %&gt;% mutate(condition = factor(c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 5 ## Estimate Est.Error Q2.5 Q97.5 condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.980 0.0200 0.928 1.000 0/0 ## 2 0.991 0.00981 0.965 1.000 1/0 ## 3 0.980 0.0200 0.928 1.000 0/1 ## 4 0.990 0.0109 0.961 1.000 1/1 ( chimp_2_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 1 ## 2 0 0/1 1 ## 3 1 1/0 1 ## 4 1 1/1 1 McElreath didn’t show the corresponding plot in the text. It might look like this. chimp_2_fitted %&gt;% # if you want to use `geom_line()` or `geom_ribbon()` with a factor on the x axis, # you need to code something like `group = 1` in `aes()` ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_2_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #2&quot;, subtitle = &quot;The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.&quot;) + coord_cartesian(ylim = c(.75, 1)) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Do note how severely we’ve restricted the y-axis range. But okay, now let’s do things by hand. We’ll need to extract the posterior samples and look at the structure of the data. post &lt;- posterior_samples(b12.4) glimpse(post) ## Observations: 16,000 ## Variables: 12 ## $ b_Intercept &lt;dbl&gt; 0.46254505, 0.82205339, 0.62844196, 0.81494071, 0.44966323, 0.1… ## $ b_prosoc_left &lt;dbl&gt; 0.4974513, 0.5739029, 0.4698631, 0.8875959, 1.0904743, 0.553110… ## $ `b_prosoc_left:condition` &lt;dbl&gt; -0.01745575, 0.02418713, 0.28953029, -0.11792927, -0.16496230, … ## $ sd_actor__Intercept &lt;dbl&gt; 2.129032, 1.954739, 1.498115, 1.792798, 1.609654, 2.555877, 1.2… ## $ `r_actor[1,Intercept]` &lt;dbl&gt; -1.11765226, -1.37929461, -1.03594846, -1.33777820, -0.83068734… ## $ `r_actor[2,Intercept]` &lt;dbl&gt; 4.499278, 4.595988, 3.385730, 2.819580, 1.983078, 7.935862, 3.9… ## $ `r_actor[3,Intercept]` &lt;dbl&gt; -1.5550260, -1.5721361, -1.2130308, -1.8853882, -1.3194376, -1.… ## $ `r_actor[4,Intercept]` &lt;dbl&gt; -1.35703638, -1.70032471, -1.23101804, -1.79687510, -1.20526894… ## $ `r_actor[5,Intercept]` &lt;dbl&gt; -1.1350801, -1.1780740, -0.9810809, -1.3676574, -1.2667202, -0.… ## $ `r_actor[6,Intercept]` &lt;dbl&gt; -0.33454544, -0.65141018, -0.25114406, -0.26845627, -0.27853406… ## $ `r_actor[7,Intercept]` &lt;dbl&gt; 1.1285056, 0.8306487, 0.5047493, 2.4218469, 1.1319008, 2.002509… ## $ lp__ &lt;dbl&gt; -279.5003, -279.9949, -285.2442, -286.5966, -284.9667, -285.270… McElreath didn’t show what his R code 12.29 dens( post$a_actor[,5] ) would look like. But here’s our analogue. post %&gt;% transmute(actor_5 = `r_actor[5,Intercept]`) %&gt;% ggplot(aes(x = actor_5)) + geom_density(size = 0, fill = &quot;blue&quot;) + scale_y_continuous(breaks = NULL) + ggtitle(&quot;Chimp #5&#39;s density&quot;) + theme_fivethirtyeight() And because we made the density only using the r_actor[5,Intercept] values (i.e., we didn’t add b_Intercept to them), the density is in a deviance-score metric. McElreath built his own link() function. Here we’ll build an alternative to fitted(). # our hand-made `brms::fitted()` alternative my_fitted &lt;- function(prosoc_left, condition){ post %&gt;% transmute(fitted = (b_Intercept + `r_actor[5,Intercept]` + b_prosoc_left * prosoc_left + `b_prosoc_left:condition` * prosoc_left * condition) %&gt;% inv_logit_scaled()) } # the posterior summaries ( chimp_5_my_fitted &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) %&gt;% mutate(post = map2(prosoc_left, condition, my_fitted)) %&gt;% unnest() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(condition) %&gt;% tidybayes::mean_qi(fitted) ) ## # A tibble: 4 x 7 ## condition fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0/0 0.332 0.224 0.450 0.95 mean qi ## 2 1/0 0.528 0.385 0.670 0.95 mean qi ## 3 0/1 0.332 0.224 0.450 0.95 mean qi ## 4 1/1 0.495 0.351 0.638 0.95 mean qi # the empirical summaries chimp &lt;- 5 ( chimp_5_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 0.333 ## 2 0 0/1 0.278 ## 3 1 1/0 0.556 ## 4 1 1/1 0.5 Okay, let’s see how good we are at retrodicting the pulled_left probabilities for actor == 5. chimp_5_my_fitted %&gt;% ggplot(aes(x = condition, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_5_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #5&quot;, subtitle = &quot;This plot is like the last except\\nwe did more by hand.&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Not bad. 12.4.2 Posterior prediction for new clusters. By average actor, McElreath referred to a chimp with an intercept exactly at the population mean \\(\\alpha\\). So this time we’ll only be working with the population parameters, or what are also sometimes called the fixed effects. When using brms::posterior_samples() output, this would mean working with columns beginning with the b_ prefix (i.e., b_Intercept, b_prosoc_left, and b_prosoc_left:condition). post_average_actor &lt;- post %&gt;% # here we use the linear regression formula to get the log_odds for the 4 conditions transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% # with `mutate_all()` we can convert the estimates to probabilities in one fell swoop mutate_all(inv_logit_scaled) %&gt;% # putting the data in the long format and grouping by condition (i.e., `key`) gather() %&gt;% mutate(key = factor(key, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(key) %&gt;% # here we get the summary values for the plot summarise(m = mean(value), # note we&#39;re using 80% intervals ll = quantile(value, probs = .1), ul = quantile(value, probs = .9)) post_average_actor ## # A tibble: 4 x 4 ## key m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.585 0.340 0.824 ## 2 1/0 0.742 0.535 0.917 ## 3 0/1 0.585 0.340 0.824 ## 4 1/1 0.718 0.500 0.906 Figure 12.5.a. p1 &lt;- post_average_actor %&gt;% ggplot(aes(x = key, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p1 If we want to depict the variability across the chimps, we need to include sd_actor__Intercept into the calculations. In the first block of code, below, we simulate a bundle of new intercepts defined by \\[\\alpha_\\text{actor} \\sim \\text{Normal} (0, \\sigma_\\text{actor})\\] # the random effects set.seed(12.42) ran_ef &lt;- tibble(random_effect = rnorm(n = 1000, mean = 0, sd = post$sd_actor__Intercept)) %&gt;% # with the `., ., ., .` syntax, we quadruple the previous line bind_rows(., ., ., .) # the fixed effects (i.e., the population parameters) fix_ef &lt;- post %&gt;% slice(1:1000) %&gt;% transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% gather() %&gt;% rename(condition = key, fixed_effect = value) %&gt;% mutate(condition = factor(condition, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # combine them ran_and_fix_ef &lt;- bind_cols(ran_ef, fix_ef) %&gt;% mutate(intercept = fixed_effect + random_effect) %&gt;% mutate(prob = inv_logit_scaled(intercept)) # to simplify things, we&#39;ll reduce them to summaries ( marginal_effects &lt;- ran_and_fix_ef %&gt;% group_by(condition) %&gt;% summarise(m = mean(prob), ll = quantile(prob, probs = .1), ul = quantile(prob, probs = .9)) ) ## # A tibble: 4 x 4 ## condition m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.560 0.0673 0.965 ## 2 1/0 0.671 0.161 0.986 ## 3 0/1 0.560 0.0673 0.965 ## 4 1/1 0.652 0.140 0.983 Behold Figure 12.5.b. p2 &lt;- marginal_effects %&gt;% ggplot(aes(x = condition, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p2 Figure 12.5.c just takes a tiny bit more wrangling. p3 &lt;- ran_and_fix_ef %&gt;% mutate(iter = rep(1:1000, times = 4)) %&gt;% filter(iter %in% c(1:50)) %&gt;% ggplot(aes(x = condition, y = prob, group = iter)) + theme_fivethirtyeight() + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + geom_line(alpha = 1/2, color = &quot;orange3&quot;) + theme(plot.title = element_text(size = 14, hjust = .5)) p3 For the finale, we’ll stitch the three plots together. library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) 12.4.2.1 Bonus: Let’s use fitted() this time. We just made those plots using various wrangled versions of post, the data frame returned by posterior_samples(b.12.4). If you followed along closely, part of what made that a great exercise is that it forced you to consider what the various vectors in post meant with respect to the model formula. But it’s also handy to see how to do that from a different perspective. So in this section, we’ll repeat that process by relying on the fitted() function, instead. We’ll go in the same order, starting with the average actor. nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) ( f &lt;- fitted(b12.4, newdata = nd, re_formula = NA, probs = c(.1, .9)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.585 0.184 0.340 0.824 0 0/0 ## 2 0.742 0.154 0.535 0.917 1 1/0 ## 3 0.585 0.184 0.340 0.824 0 0/1 ## 4 0.718 0.161 0.500 0.906 1 1/1 You should notice a few things. Since b12.4 is a multilevel model, it had three predictors: prosoc_left, condition, and actor. However, our nd data only included the first two of those predictors. The reason fitted() permitted that was because we set re_formula = NA. When you do that, you tell fitted() to ignore group-level effects (i.e., focus only on the fixed effects). This was our fitted() version of ignoring the r_ vectors returned by posterior_samples(). Here’s the plot. p4 &lt;- f %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p4 For marginal of actor, we can continue using the same nd data. This time we’ll be sticking with the default re_formula setting, which will accommodate the multilevel nature of the model. However, we’ll also be adding allow_new_levels = T and sample_new_levels = &quot;gaussian&quot;. The former will allow us to marginalize across the specific actors in our data and the latter will instruct fitted() to use the multivariate normal distribution implied by the random effects. It’ll make more sense why I say multivariate normal by the end of the next chapter. For now, just go with it. ( f &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.557 0.328 0.0714 0.971 0 0/0 ## 2 0.667 0.310 0.148 0.987 1 1/0 ## 3 0.557 0.328 0.0714 0.971 0 0/1 ## 4 0.649 0.314 0.133 0.985 1 1/1 Here’s our fitted()-based marginal of actor plot. p5 &lt;- f %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p5 For the simulated actors plot, we’ll just amend our process from the last one. This time we’re setting summary = F, in order to keep the iteration-specific results, and setting nsamples = n_sim. n_sim is just a name for the number of actors we’d like to simulate (i.e., 50, as in the text). # how many simulated actors would you like? n_sim &lt;- 50 ( f &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;, summary = F, nsamples = n_sim) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n_sim) %&gt;% gather(key, value, -iter) %&gt;% bind_cols(nd %&gt;% transmute(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% expand(condition, iter = 1:n_sim)) ) ## # A tibble: 200 x 5 ## iter key value condition iter1 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 V1 0.255 0/0 1 ## 2 2 V1 0.528 0/0 2 ## 3 3 V1 0.597 0/0 3 ## 4 4 V1 0.757 0/0 4 ## 5 5 V1 0.962 0/0 5 ## 6 6 V1 0.459 0/0 6 ## 7 7 V1 0.157 0/0 7 ## 8 8 V1 0.148 0/0 8 ## 9 9 V1 0.741 0/0 9 ## 10 10 V1 0.786 0/0 10 ## # … with 190 more rows p6 &lt;- f %&gt;% ggplot(aes(x = condition, y = value, group = iter)) + geom_line(alpha = 1/2, color = &quot;blue&quot;) + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p6 Here they are altogether. grid.arrange(p4, p5, p6, ncol = 3) 12.4.3 Focus and multilevel prediction. First, let’s load the Kline data. # prep data library(rethinking) data(Kline) d &lt;- Kline Switch out the packages, once again. detach(package:rethinking, unload = T) library(brms) rm(Kline) The statistical formula for our multilevel count model is \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\text{Poisson} (\\mu_i) \\\\ \\text{log} (\\mu_i) &amp; = \\alpha + \\alpha_{\\text{culture}_i} + \\beta \\text{log} (\\text{population}_i) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 1) \\\\ \\alpha_{\\text{culture}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{culture}}) \\\\ \\sigma_{\\text{culture}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\end{align*}\\] With brms, we don’t actually need to make the logpop or society variables. We’re ready to fit the multilevel Kline model with the data in hand. b12.6 &lt;- brm(data = d, family = poisson, total_tools ~ 0 + intercept + log(population) + (1 | culture), prior = c(prior(normal(0, 10), class = b, coef = intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 3, chains = 3, seed = 12) Note how we used the special 0 + intercept syntax rather than using the default Intercept. This is because our predictor variable was not mean centered. For more info, see here. Though we used the 0 + intercept syntax for the fixed effect, it was not necessary for the random effect. Both ways work. Here is the data-processing work for our variant of Figure 12.6. nd &lt;- tibble(population = seq(from = 1000, to = 400000, by = 5000), # to &quot;simulate counterfactual societies, using the hyper-parameters&quot; (p. 383), # we&#39;ll plug a new island into the `culture` variable culture = &quot;my_island&quot;) p &lt;- predict(b12.6, # this allows us to simulate values for our counterfactual island, &quot;my_island&quot; allow_new_levels = T, # here we explicitly tell brms we want to include the group-level effects re_formula = ~ (1 | culture), # from the brms manual, this uses the &quot;(multivariate) normal distribution implied by # the group-level standard deviations and correlations&quot;, which appears to be # what McElreath did in the text. sample_new_levels = &quot;gaussian&quot;, newdata = nd, probs = c(.015, .055, .165, .835, .945, .985)) %&gt;% as_tibble() %&gt;% bind_cols(nd) p %&gt;% glimpse() ## Observations: 80 ## Variables: 10 ## $ Estimate &lt;dbl&gt; 19.91589, 31.19600, 36.43122, 40.20144, 43.20922, 45.66733, 47.95200, 50.05578… ## $ Est.Error &lt;dbl&gt; 10.00401, 13.65630, 15.48353, 17.27621, 18.55424, 19.88237, 20.92730, 21.93365… ## $ Q1.5 &lt;dbl&gt; 5.000, 10.000, 13.000, 14.000, 15.000, 16.000, 16.000, 17.000, 18.000, 18.000,… ## $ Q5.5 &lt;dbl&gt; 8, 15, 18, 20, 21, 22, 23, 24, 25, 26, 26, 27, 28, 28, 29, 29, 29, 30, 30, 31,… ## $ Q16.5 &lt;dbl&gt; 12, 20, 24, 26, 28, 30, 31, 33, 34, 34, 35, 36, 37, 38, 38, 39, 40, 40, 41, 42… ## $ Q83.5 &lt;dbl&gt; 27.000, 41.000, 48.000, 53.000, 57.000, 60.000, 63.000, 66.000, 68.000, 70.000… ## $ Q94.5 &lt;dbl&gt; 36.000, 53.000, 61.000, 68.000, 72.000, 78.000, 80.000, 84.000, 88.000, 90.000… ## $ Q98.5 &lt;dbl&gt; 47.000, 69.000, 78.000, 88.000, 95.000, 100.000, 107.000, 112.000, 117.000, 12… ## $ population &lt;dbl&gt; 1000, 6000, 11000, 16000, 21000, 26000, 31000, 36000, 41000, 46000, 51000, 560… ## $ culture &lt;chr&gt; &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, … For a detailed discussion on this way of using brms::predict(), see Andrew MacDonald’s great blogpost on this very figure. Here’s what we’ve been working for: p %&gt;% ggplot(aes(x = log(population), y = Estimate)) + geom_ribbon(aes(ymin = Q1.5, ymax = Q98.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q16.5, ymax = Q83.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_line(color = &quot;orange4&quot;) + geom_text(data = d, aes(y = total_tools, label = culture), size = 2.33, color = &quot;blue&quot;) + ggtitle(&quot;Total tools as a function of log(population)&quot;) + coord_cartesian(ylim = range(d$total_tools)) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 12, hjust = .5)) Glorious. The envelope of predictions is a lot wider here than it was back in Chapter 10. This is a consequene of the varying intercepts, combined with the fact that there is much more variation in the data than a pure-Poisson model anticipates. (p. 384) 12.5 Summary Bonus: tidybayes::spread_draws() A big part of this chapter, both what McElreath focused on in the text and even our plotting digression a bit above, focused on how to combine the fixed effects of a multilevel with the group-level. Given some binomial variable, \\(\\text{criterion}\\), and some group term, \\(\\text{grouping variable}\\), we’ve learned the simple multilevel model follows a form like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\text{Binomial} (n_i \\geq 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{grouping variable}_i}\\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\alpha_{\\text{grouping variable}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{grouping variable}}) \\\\ \\sigma_{\\text{grouping variable}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] and we’ve been grappling with the relation between the grand mean \\(\\alpha\\) and the group-level deviations \\(\\alpha_{\\text{grouping variable}}\\). For situations where we have the brms::brm() model fit in hand, we’ve been playing with various ways to use the iterations, particularly with either the posterior_samples() method and the fitted()/predict() method. Both are great. But (a) we have other options, which I’d like to share, and (b) if you’re like me, you probably need more practice than following along with the examples in the text. In this bonus section, we are going to introduce two simplified models and then practice working with combining the grand mean various combinations of the random effects. For our first step, we’ll introduce the models. 12.5.1 Intercepts-only models with one or two grouping variables If you recall, b12.4 was our first multilevel model with the chimps data. We can retrieve the model formula like so. b12.4$formula ## pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) In addition to the model intercept and random effects for the individual chimps (i.e., actor), we also included fixed effects for the study conditions. For our bonus section, it’ll be easier if we reduce this to a simple intercepts-only model with the sole actor grouping factor. That model will follow the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i}\\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] Before we fit the model, you might recall that (a) we’ve already removed the chimpanzees data after saving the data as d and (b) we subsequently reassigned the Kline data to d. Instead of reloading the rethinking package to retrieve the chimpanzees data, we might also acknowledge that the data has also been saved within our b12.4 fit object. [It’s easy to forget such things.] b12.4$data %&gt;% glimpse() ## Observations: 504 ## Variables: 4 ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ actor &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… So there’s no need to reload anything. Everything we need is already at hand. Let’s fit the intercepts-only model. b12.7 &lt;- brm(data = b12.4$data, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12) Here’s the model summary: print(b12.7) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) ## Data: b12.4$data (Number of observations: 504) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 2.22 0.94 1.09 4.52 1101 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.84 1.02 -0.91 2.90 939 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now recall that our competing cross-classified model, b12.5 added random effects for the trial blocks. Here was that formula. b12.5$formula ## pulled_left | trials(1) ~ prosoc_left + (1 | actor) + (1 | block) + prosoc_left:condition And, of course, we can retrieve the data from that model, too. b12.5$data %&gt;% glimpse() ## Observations: 504 ## Variables: 5 ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ actor &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ block &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,… It’s the same data we used from the b12.4 model, but with the addition of the block index. With those data in hand, we can fit the intercepts-only version of our cross-classified model. This model formula follows the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block}_i}\\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha_{\\text{block}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{block}}) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_{\\text{block}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] Fit the model. b12.8 &lt;- brm(data = b12.5$data, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12) Here’s the summary. print(b12.8) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) ## Data: b12.5$data (Number of observations: 504) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 2.22 0.89 1.10 4.46 4536 1.00 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.23 0.18 0.01 0.69 6108 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.82 0.92 -0.92 2.77 3524 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ve fit our two intercepts-only models, let’s get to the heart of this section. We are going to practice four methods for working with the posterior samples. Each method will revolve around a different primary function. In order, they are brms::posterior_samples() brms::coef() brms::fitted() tidybayes::spread_draws() We’ve already had some practice with the first three, but I hope this section will make them even more clear. The tidybayes::spread_draws() method will be new, to us. I think you’ll find it’s a handy alternative. With each of the four methods, we’ll practice three different model summaries. Getting the posterior draws for the actor-level estimates from the b12.7 model Getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block Getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1 So to be clear, our goal is to accomplish those three tasks with four methods, each of which should yield equivalent results. 12.5.2 brms::posterior_samples() To warm up, let’s take a look at the structure of the posterior_samples() output for the simple b12.7 model. posterior_samples(b12.7) %&gt;% str() ## &#39;data.frame&#39;: 16000 obs. of 10 variables: ## $ b_Intercept : num 0.929 0.388 0.195 0.223 -0.612 ... ## $ sd_actor__Intercept : num 3.3 2.41 2.3 2.72 2.93 ... ## $ r_actor[1,Intercept]: num -1.5618 -0.94732 -0.40974 -0.83024 0.00504 ... ## $ r_actor[2,Intercept]: num 3.61 4.72 7.62 8.63 3.18 ... ## $ r_actor[3,Intercept]: num -1.789 -0.91 -1.031 -0.587 0.142 ... ## $ r_actor[4,Intercept]: num -2.203 -1.251 -0.688 -1.074 -0.146 ... ## $ r_actor[5,Intercept]: num -1.384 -0.84 -0.511 -0.556 0.364 ... ## $ r_actor[6,Intercept]: num -0.0667 0.3732 0.0569 0.2945 1.1865 ... ## $ r_actor[7,Intercept]: num 1.54 1.78 1.52 2.17 2.97 ... ## $ lp__ : num -282 -279 -282 -282 -282 ... The b_Intercept vector corresponds to the \\(\\alpha\\) term in the statistical model. The second vector, sd_actor__Intercept, corresponds to the \\(\\sigma_{\\text{actor}}\\) term. And the next 7 vectors beginning with the r_actor suffix are the \\(\\alpha_{\\text{actor}}\\) deviations from the grand mean, \\(\\alpha\\). Thus if we wanted to get the model-implied probability for our first chimp, we’d add b_Intercept to r_actor[1,Intercept] and then take the inverse logit. posterior_samples(b12.7) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = (b_Intercept + `r_actor[1,Intercept]`) %&gt;% inv_logit_scaled()) %&gt;% head() ## chimp 1&#39;s average probability of pulling left ## 1 0.3469689 ## 2 0.3636186 ## 3 0.4466062 ## 4 0.3527511 ## 5 0.3527666 ## 6 0.4897990 To complete our first task, then, of getting the posterior draws for the actor-level estimates from the b12.7 model, we can do that in bulk. p1 &lt;- posterior_samples(b12.7) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p1) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.347 0.364 0.447 0.353 0.353 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.989 0.994 1 1 0.929 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.297 0.372 0.302 0.41 0.385 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.219 0.297 0.379 0.299 0.319 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.388 0.389 0.422 0.418 0.438 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.703 0.682 0.563 0.627 0.64 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.922 0.897 0.847 0.916 0.913 ... One of the things I really like about this method is the b_Intercept + r_actor[i,Intercept] part of the code makes it very clear, to me, how the porterior_samples() columns correspond to the statistical model, \\(\\text{logit} (p_i) = \\alpha + \\alpha_{\\text{actor}_i}\\). This method easily extends to our next task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. In fact, other than switching out b12.7 for b12.8, the method is identical. p2 &lt;- posterior_samples(b12.8) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p2) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.472 0.423 0.384 0.496 0.302 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.982 0.993 0.998 0.99 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.328 0.347 0.32 0.409 0.317 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.28 0.374 0.392 0.292 0.283 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.395 0.385 0.467 0.376 0.375 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.653 0.576 0.618 0.71 0.551 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.909 0.914 0.881 0.811 ... The reason we can still get away with this is because the grand mean in the b12.8 model is the grand mean across all levels of actor and block. AND it’s the case that the r_actor and r_block vectors returned by posterior_samples(b12.8) are all in deviation metrics–execute posterior_samples(b12.8) %&gt;% glimpse() if it will help you follow along. So if we simply leave out the r_block vectors, we are ignoring the specific block-level deviations, effectively averaging over them. Now for our third task, we’ve decided we wanted to retrieve the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1. To get the chimp-specific estimates for the first block, we simply add + r_block[1,Intercept] to the end of each formula. p3 &lt;- posterior_samples(b12.8) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]` + `r_block[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]` + `r_block[1,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]` + `r_block[1,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]` + `r_block[1,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]` + `r_block[1,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]` + `r_block[1,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]` + `r_block[1,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p3) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.475 0.336 0.334 0.471 0.316 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.974 0.991 0.998 0.991 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.33 0.269 0.275 0.384 0.332 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.283 0.292 0.342 0.272 0.297 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.398 0.302 0.414 0.353 0.391 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.655 0.484 0.566 0.689 0.568 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.873 0.896 0.87 0.821 ... Again, I like this method because of how close the wrangling code within transmute() is to the statistical model formula. I wrote a lot of code like this in my early days of working with these kinds of models, and I think the pedagogical insights were helpful. But this method has its limitations. It works fine if you’re working with some small number of groups. But that’s a lot of repetitious code and it would be utterly un-scalable to situations where you have 50 or 500 levels in your grouping variable. We need alternatives. 12.5.3 brms::coef() First, let’s review what the coef() function returns. coef(b12.7) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.3258351 0.2387985 -0.7945794 0.1419688 ## 2 4.8723958 1.5689613 2.8545653 8.7823773 ## 3 -0.6162426 0.2444905 -1.1020032 -0.1483985 ## 4 -0.6155056 0.2452988 -1.1078114 -0.1410936 ## 5 -0.3227853 0.2380135 -0.7963350 0.1406402 ## 6 0.5786282 0.2442195 0.1109735 1.0688268 ## 7 2.0770453 0.3702068 1.3956496 2.8488727 By default, we get the familiar summaries for mean performances for each of our seven chimps. These, of course, are in the log-odds metric and simply tacking on inv_logit_scaled() isn’t going to fully get the job done. So to get things in the probability metric, we’ll want to first set summary = F in order to work directly with un-summarized samples and then wrangle quite a bit. Part of the wrangling challenge is because coef() returns a list, rather than a data frame. With that in mind, the code for our first task of getting the posterior draws for the actor-level estimates from the b12.7 model looks like so. c1 &lt;- coef(b12.7, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), # we need an iteration index for `spread()` to work properly iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.347 0.364 0.447 0.353 0.353 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.989 0.994 1 1 0.929 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.297 0.372 0.302 0.41 0.385 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.219 0.297 0.379 0.299 0.319 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.388 0.389 0.422 0.418 0.438 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.703 0.682 0.563 0.627 0.64 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.922 0.897 0.847 0.916 0.913 ... So with this method, you get a little practice with three-dimensional indexing, which is a good skill to have. Now let’s extend it to our second task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. c2 &lt;- coef(b12.8, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.472 0.423 0.384 0.496 0.302 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.982 0.993 0.998 0.99 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.328 0.347 0.32 0.409 0.317 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.28 0.374 0.392 0.292 0.283 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.395 0.385 0.467 0.376 0.375 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.653 0.576 0.618 0.71 0.551 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.909 0.914 0.881 0.811 ... As with our posterior_samples() method, this code was near identical to the block, above. All we did was switch out b12.7 for b12.8. [Okay, we removed a line of annotations. But that doesn’t really count.] We should point something out, though. Consider what coef() yields when working with a cross-classified model. coef(b12.8) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.3247235 0.2688330 -0.85572928 0.19778169 ## 2 4.9029215 1.5706782 2.85073026 8.80095264 ## 3 -0.6207587 0.2775315 -1.17877374 -0.08300196 ## 4 -0.6179356 0.2748473 -1.16001597 -0.08521013 ## 5 -0.3262990 0.2666483 -0.84668181 0.19152270 ## 6 0.5872925 0.2726632 0.06094433 1.12349276 ## 7 2.0849640 0.3882744 1.35517600 2.88680438 ## ## ## $block ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6272241 0.9270623 -1.1364317 2.618713 ## 2 0.8729194 0.9209817 -0.8599580 2.850919 ## 3 0.8736776 0.9203855 -0.8524904 2.841719 ## 4 0.8018632 0.9196948 -0.9287296 2.771870 ## 5 0.8010938 0.9185675 -0.9314255 2.767559 ## 6 0.9221205 0.9245301 -0.8199536 2.889170 Now we have a list of two elements, one for actor and one for block. What might not be immediately obvious is that the summaries returned by one grouping level are based off of averaging over the other. Although this made our second task easy, it provides a challenge for our third task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1. To accomplish that, we’ll need to bring in ranef(). Let’s review what that returns. ranef(b12.8) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.1415598 0.9328233 -3.0941856 0.6170603 ## 2 4.0860852 1.6204352 1.7573932 7.9603480 ## 3 -1.4375950 0.9314227 -3.4150644 0.3029414 ## 4 -1.4347719 0.9327609 -3.4353776 0.2948453 ## 5 -1.1431353 0.9282478 -3.1432070 0.6032561 ## 6 -0.2295438 0.9312132 -2.2371739 1.5168560 ## 7 1.2681277 0.9588380 -0.7373642 3.1367315 ## ## ## $block ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.18961221 0.2329755 -0.7601650 0.1140436 ## 2 0.05608313 0.1880142 -0.3043344 0.4875170 ## 3 0.05684135 0.1901712 -0.2931361 0.5001637 ## 4 -0.01497311 0.1877653 -0.4178607 0.3738005 ## 5 -0.01574246 0.1855662 -0.4171200 0.3665485 ## 6 0.10528424 0.2021796 -0.2285119 0.5882794 The format of the ranef() output is identical to that from coef(). However, the summaries are in the deviance metric. They’re all centered around zero, which corresponds to the part of the statistical model that specifies how \\(\\alpha_{\\text{block}} \\sim \\text{Normal} (0, \\sigma_{\\text{block}})\\). So then, if we want to continue using our coef() method, we’ll need to augment it with ranef() to accomplish our last task. c3 &lt;- coef(b12.8, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% # here we add in the `block == 1` deviations from the grand mean mutate(value = value + ranef(b12.8, summary = F)$block[, 1, ] %&gt;% rep(., times = 7)) %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.475 0.336 0.334 0.471 0.316 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.974 0.991 0.998 0.991 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.33 0.269 0.275 0.384 0.332 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.283 0.292 0.342 0.272 0.297 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.398 0.302 0.414 0.353 0.391 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.655 0.484 0.566 0.689 0.568 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.873 0.896 0.87 0.821 ... One of the nicest things about the coef() method is how is scales well. This code is no more burdensome for 5 group levels than it is for 5000. It’s also a post-processing version of the distinction McElreath made on page 372 between the two equivalent ways you might define a Gaussian: \\[\\text{Normal}(10, 1)\\] and \\[10 + \\text{Normal}(0, 1)\\] Conversely, it can be a little abstract. Let’s keep expanding our options. 12.5.4 brms::fitted() As is often the case, we’re going to want to define our predictor values for fitted(). (nd &lt;- b12.7$data %&gt;% distinct(actor)) ## actor ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 Now we have our new data, nd, here’s how we might use fitted() to accomplish our first task, getting the posterior draws for the actor-level estimates from the b12.7 model. f1 &lt;- fitted(b12.7, newdata = nd, summary = F, # within `fitted()`, this line does the same work that # `inv_logit_scaled()` did with the other two methods scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.347 0.364 0.447 0.353 0.353 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.989 0.994 1 1 0.929 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.297 0.372 0.302 0.41 0.385 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.219 0.297 0.379 0.299 0.319 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.388 0.389 0.422 0.418 0.438 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.703 0.682 0.563 0.627 0.64 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.922 0.897 0.847 0.916 0.913 ... This scales reasonably well. But might not work well if the vectors you wanted to rename didn’t follow a serial order, like ours. If you’re willing to pay with a few more lines of wrangling code, this method is more general, but still scalable. f1 &lt;- fitted(b12.7, newdata = nd, summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% # you&#39;ll need this line to make the `spread()` line work properly mutate(iter = 1:n()) %&gt;% gather(key, value, -iter) %&gt;% mutate(key = str_replace(key, &quot;V&quot;, &quot;chimp &quot;)) %&gt;% mutate(key = str_c(key, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(key = key, value = value) str(f1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.347 0.364 0.447 0.353 0.353 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.989 0.994 1 1 0.929 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.297 0.372 0.302 0.41 0.385 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.219 0.297 0.379 0.299 0.319 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.388 0.389 0.422 0.418 0.438 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.703 0.682 0.563 0.627 0.64 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.922 0.897 0.847 0.916 0.913 ... Now unlike with the previous two methods, our fitted() method will not allow us to simply switch out b12.7 for b12.8 to accomplish our second task of getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. This is because when we use fitted() in combination with its newdata argument, the function expects us to define values for all the predictor variables in the formula. Because the b12.8 model has both actor and block grouping variables as predictors, the default requires we include both in our new data. But if we were to specify a value for block in the nd data, we would no longer be averaging over the levels of block anymore; we’d be selecting one of the levels of block in particular, which we don’t yet want to do. Happily, brms::fitted() has a re_formula argument. If we would like to average out block, we simply drop it from the formula. Here’s how to do so. f2 &lt;- fitted(b12.8, newdata = nd, # this line allows us to average over the levels of `block` re_formula = pulled_left ~ 1 + (1 | actor), summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.472 0.423 0.384 0.496 0.302 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.982 0.993 0.998 0.99 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.328 0.347 0.32 0.409 0.317 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.28 0.374 0.392 0.292 0.283 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.395 0.385 0.467 0.376 0.375 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.653 0.576 0.618 0.71 0.551 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.909 0.914 0.881 0.811 ... If we want to use fitted() for our third task of getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1, we’ll need to augment our nd data. ( nd &lt;- b12.8$data %&gt;% distinct(actor) %&gt;% mutate(block = 1) ) ## actor block ## 1 1 1 ## 2 2 1 ## 3 3 1 ## 4 4 1 ## 5 5 1 ## 6 6 1 ## 7 7 1 This time, we no longer need that re_formula argument. f3 &lt;- fitted(b12.8, newdata = nd, summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.475 0.336 0.334 0.471 0.316 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.974 0.991 0.998 0.991 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.33 0.269 0.275 0.384 0.332 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.283 0.292 0.342 0.272 0.297 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.398 0.302 0.414 0.353 0.391 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.655 0.484 0.566 0.689 0.568 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.873 0.896 0.87 0.821 ... Let’s learn one more option. 12.5.5 tidybayes::spread_draws() Up till this point, we’ve really only used the tidybayes package for plotting (e.g., with geom_halfeyeh()) and summarizing (e.g., with median_qi()). But tidybayes is more general; it offers a handful of convenience functions for wrangling posterior draws from a tidyverse perspective. One such function is spread_draws(), which you can learn all about in Matthew Kay’s vignette Extracting and visualizing tidy draws from brms models. Let’s take a look at how we’ll be using it. library(tidybayes) b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) ## # A tibble: 112,000 x 6 ## # Groups: actor [7] ## .chain .iteration .draw b_Intercept actor r_actor ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 0.929 1 -1.56 ## 2 1 1 1 0.929 2 3.61 ## 3 1 1 1 0.929 3 -1.79 ## 4 1 1 1 0.929 4 -2.20 ## 5 1 1 1 0.929 5 -1.38 ## 6 1 1 1 0.929 6 -0.0667 ## 7 1 1 1 0.929 7 1.54 ## 8 1 2 2 0.388 1 -0.947 ## 9 1 2 2 0.388 2 4.72 ## 10 1 2 2 0.388 3 -0.910 ## # … with 111,990 more rows First, notice tidybayes::spread_draws() took the model fit itself, b12.7, as input. No need for posterior_samples(). Now, notice we fed it two additional arguments. By the first argument, we that requested spead_draws() extract the posterior samples for the b_Intercept. By the second argument, r_actor[actor,], we instructed spead_draws() to extract all the random effects for the actor variable. Also notice how within the brackets [] we specified actor, which then became the name of the column in the output that indexed the levels of the grouping variable actor. By default, the code returns the posterior samples for all the levels of actor. However, had we only wanted those from chimps #1 and #3, we might use typical tidyverse-style indexing. E.g., b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% filter(actor %in% c(1, 3)) ## # A tibble: 32,000 x 6 ## # Groups: actor [2] ## .chain .iteration .draw b_Intercept actor r_actor ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 0.929 1 -1.56 ## 2 1 1 1 0.929 3 -1.79 ## 3 1 2 2 0.388 1 -0.947 ## 4 1 2 2 0.388 3 -0.910 ## 5 1 3 3 0.195 1 -0.410 ## 6 1 3 3 0.195 3 -1.03 ## 7 1 4 4 0.223 1 -0.830 ## 8 1 4 4 0.223 3 -0.587 ## 9 1 5 5 -0.612 1 0.00504 ## 10 1 5 5 -0.612 3 0.142 ## # … with 31,990 more rows Also notice those first three columns. By default, spread_draws() extracted information about which Markov chain a given draw was from, which iteration a given draw was within a given chain, and which draw from an overall standpoint. If it helps to keep track of which vector indexed what, consider this. b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% ungroup() %&gt;% select(.chain:.draw) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(min = min(value), max = max(value)) ## # A tibble: 3 x 3 ## key min max ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 .chain 1 4 ## 2 .draw 1 16000 ## 3 .iteration 1 4000 Above we simply summarized each of the three variables by their minimum and maximum values. If you recall that we fit b12.7 with four Markov chains, each with 4000 post-warmup iterations, hopefully it’ll make sense what each of those three variables index. Now we’ve done a little clarification, let’s use spread_draws() to accomplish our first task, getting the posterior draws for the actor-level estimates from the b12.7 model. s1 &lt;- b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% mutate(p = (b_Intercept + r_actor) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) str(s1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.347 0.364 0.447 0.353 0.353 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.989 0.994 1 1 0.929 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.297 0.372 0.302 0.41 0.385 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.219 0.297 0.379 0.299 0.319 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.388 0.389 0.422 0.418 0.438 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.703 0.682 0.563 0.627 0.64 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.922 0.897 0.847 0.916 0.913 ... The method remains essentially the same for accomplishing our second task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. s2 &lt;- b12.8 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% mutate(p = (b_Intercept + r_actor) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) str(s2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.472 0.423 0.384 0.496 0.302 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.982 0.993 0.998 0.99 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.328 0.347 0.32 0.409 0.317 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.28 0.374 0.392 0.292 0.283 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.395 0.385 0.467 0.376 0.375 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.653 0.576 0.618 0.71 0.551 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.909 0.914 0.881 0.811 ... To accomplish our third task, we augment the spread_draws() and first mutate() lines, and add a filter() line between them. s3 &lt;- b12.8 %&gt;% spread_draws(b_Intercept, r_actor[actor,], r_block[block,]) %&gt;% filter(block == 1) %&gt;% mutate(p = (b_Intercept + r_actor + r_block) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) ## Adding missing grouping variables: `block` str(s3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 9 variables: ## $ block : int 1 1 1 1 1 1 1 1 1 1 ... ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.475 0.336 0.334 0.471 0.316 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.993 0.974 0.991 0.998 0.991 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.33 0.269 0.275 0.384 0.332 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.283 0.292 0.342 0.272 0.297 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.398 0.302 0.414 0.353 0.391 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.655 0.484 0.566 0.689 0.568 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.941 0.873 0.896 0.87 0.821 ... Hopefully working through these examples gave you some insight on the relation between fixed and random effects within multilevel models, and perhaps added to your posterior-iteration-wrangling toolkit. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 gridExtra_2.3 bayesplot_1.7.0 ggthemes_4.2.0 forcats_0.4.0 ## [6] stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 ## [11] tibble_2.1.3 tidyverse_1.2.1 brms_2.9.0 Rcpp_1.0.1 dagitty_0.2-2 ## [16] rstan_2.18.2 StanHeaders_2.18.1 ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 jsonlite_1.6 broom_0.5.2 ## [22] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 ## [25] backports_1.1.4 assertthat_0.2.1 Matrix_1.2-17 ## [28] lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [31] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [34] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 ## [37] glue_1.3.1 reshape2_1.4.3 V8_2.2 ## [40] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-144 ## [43] crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [46] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.5 ## [52] zoo_1.8-6 scales_1.1.1.9000 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] curl_3.3 loo_2.1.0 stringi_1.4.3 ## [64] highr_0.8 dygraphs_1.1.1.6 boot_1.3-24 ## [67] pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [70] pkgconfig_2.0.2 matrixStats_0.54.0 evaluate_0.14 ## [73] lattice_0.20-38 labeling_0.3 rstantools_1.5.1 ## [76] htmlwidgets_1.3 processx_3.3.1 tidyselect_0.2.5 ## [79] plyr_1.8.4 magrittr_1.5 bookdown_0.11 ## [82] R6_2.4.0 generics_0.0.2 pillar_1.4.1 ## [85] haven_2.1.0 withr_2.1.2 xts_0.11-2 ## [88] abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_1.13 ## [94] grid_3.6.3 readxl_1.3.1 callr_3.2.0 ## [97] threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [100] httpuv_1.5.1 stats4_3.6.3 munsell_0.5.0 ## [103] shinyjs_1.0 "],
["adventures-in-covariance.html", "13 Adventures in Covariance 13.1 Varying slopes by construction 13.2 Example: Admission decisions and gender 13.3 Example: Cross-classified chimpanzees with varying slopes 13.4 Continuous categories and the Gaussian process 13.5 Summary Bonus: Another Berkley-admissions-data-like example. Reference Session info", " 13 Adventures in Covariance In this chapter, you’ll see how to… specify varying slopes in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. It will also improve estimates of intercepts, by borrowing information across parameter types. Essentially, varying slopes models are massive interaction machines. They allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling. When the variation in slopes is large, the average slope is of less interest. Sometimes, the pattern of variation in slopes provides hints about omitted variables that explain why some units respond more or less. We’ll see an example in this chapter. The machinery that makes such complex varying effects possible will be used later in the chapter to extend the varying effects strategy to more subtle model types, including the use of continuous categories, using Gaussian process. (p. 388) 13.1 Varying slopes by construction How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 389) In the Rethinking: Why Gaussian? box, McElreath discussed how researchers might use other multivariate distributions to model multiple random effects. The only one he named as an alternative to the Gaussian was the multivariate Student’s \\(t\\). As it turns out, brms does currently allow users to use multivariate Student’s \\(t\\) in this way. For details, check out this discussion in the brms GitHub page. Bürkner’s exemplar syntax from his comment on May 13, 2018, was y ~ x + (x | gr(g, dist = &quot;student&quot;)). I haven’t experimented with this, but if you do, do consider commenting on how it went. 13.1.1 Simulate the population. If you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done some nice work along these lines, too. a &lt;- 3.5 # average morning wait time b &lt;- -1 # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- -.7 # correlation between intercepts and slopes # the next three lines of code simply combine the terms, above mu &lt;- c(a, b) cov_ab &lt;- sigma_a * sigma_b * rho sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) If you haven’t used matirx() before, you might get a sense of the elements like so. matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 This next block of code will finally yield our café data. library(tidyverse) sigmas &lt;- c(sigma_a, sigma_b) # standard deviations rho &lt;- matrix(c(1, rho, # correlation matrix rho, 1), nrow = 2) # now matrix multiply to get covariance matrix sigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas) # how many cafes would you like? n_cafes &lt;- 20 set.seed(13) # used to replicate example vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) %&gt;% data.frame() %&gt;% set_names(&quot;a_cafe&quot;, &quot;b_cafe&quot;) head(vary_effects) ## a_cafe b_cafe ## 1 2.917639 -0.8649154 ## 2 3.552770 -1.6814372 ## 3 1.694390 -0.4168858 ## 4 3.442417 -0.6011724 ## 5 2.289988 -0.7461953 ## 6 3.069283 -0.8839639 Let’s make sure we’re keeping this all straight. a_cafe = our café-specific intercepts; b_cafe = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask What’s the distribution of a_cafe and b_cafe? Our variant of Figure 13.2 contains the answer. For our plots in this chapter, we’ll use a custom theme. The color palette will come from the “pearl_earring” palette of the dutchmasters package. You can learn more about the original painting, Vermeer’s Girl with a Pearl Earring, here. # devtools::install_github(&quot;EdwinTh/dutchmasters&quot;) library(dutchmasters) dutchmasters$pearl_earring ## red(lips) skin blue(scarf1) blue(scarf2) white(colar) ## &quot;#A65141&quot; &quot;#E7CDC2&quot; &quot;#80A0C7&quot; &quot;#394165&quot; &quot;#FCF9F0&quot; ## gold(dress) gold(dress2) black(background) grey(scarf3) yellow(scarf4) ## &quot;#B1934A&quot; &quot;#DCA258&quot; &quot;#100F14&quot; &quot;#8B9DAF&quot; &quot;#EEDA9D&quot; ## ## &quot;#E8DCCF&quot; We’ll name our custom theme theme_pearl_earring. theme_pearl_earring &lt;- theme(text = element_text(color = &quot;#E8DCCF&quot;, family = &quot;Courier&quot;), strip.text = element_text(color = &quot;#E8DCCF&quot;, family = &quot;Courier&quot;), axis.text = element_text(color = &quot;#E8DCCF&quot;), axis.ticks = element_line(color = &quot;#E8DCCF&quot;), line = element_line(color = &quot;#E8DCCF&quot;), plot.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), panel.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;#E8DCCF&quot;), strip.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), panel.grid = element_blank(), legend.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), legend.key = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), axis.line = element_blank()) Now we’re ready to plot Figure 13.2. vary_effects %&gt;% ggplot(aes(x = a_cafe, y = b_cafe)) + geom_point(color = &quot;#80A0C7&quot;) + geom_rug(color = &quot;#8B9DAF&quot;, size = 1/7) + theme_pearl_earring Again, these are not “data.” Figure 13.2 shows a distribution of parameters. 13.1.2 Simulate observations. Here we put those simulated parameters to use. n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes set.seed(13) # used to replicate example d &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = a_cafe + b_cafe * afternoon) %&gt;% mutate(wait = rnorm(n = n(), mean = mu, sd = sigma)) We might peek at the data. d %&gt;% head() ## # A tibble: 6 x 7 ## cafe a_cafe b_cafe visit afternoon mu wait ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.92 -0.865 1 0 2.92 3.19 ## 2 1 2.92 -0.865 2 1 2.05 1.91 ## 3 1 2.92 -0.865 3 0 2.92 3.81 ## 4 1 2.92 -0.865 4 1 2.05 2.15 ## 5 1 2.92 -0.865 5 0 2.92 3.49 ## 6 1 2.92 -0.865 6 1 2.05 2.26 Now we’ve finally simulated our data, we are ready to make our version of Figure 13.1, from way back on page 388. d %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;M&quot;, &quot;A&quot;), day = rep(rep(1:5, each = 2), times = n_cafes)) %&gt;% filter(cafe %in% c(3, 5)) %&gt;% mutate(cafe = ifelse(cafe == 3, &quot;cafe #3&quot;, &quot;cafe #5&quot;)) %&gt;% ggplot(aes(x = visit, y = wait, group = day)) + geom_point(aes(color = afternoon), size = 2) + geom_line(color = &quot;#8B9DAF&quot;) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(NULL, breaks = 1:10, labels = rep(c(&quot;M&quot;, &quot;A&quot;), times = 5)) + coord_cartesian(ylim = 0:4) + ylab(&quot;wait time in minutes&quot;) + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.x = element_blank()) + facet_wrap(~cafe, ncol = 1) 13.1.3 The varying slopes model. The statistical formula for our varying-slopes model follows the form \\[\\begin{align*} \\text{wait}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{cafe}_i} + \\beta_{\\text{cafe}_i} \\text{afternoon}_i \\\\ \\begin{bmatrix} \\alpha_\\text{cafe} \\\\ \\beta_\\text{cafe} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\alpha &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\beta &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\mathbf R &amp; \\sim \\text{LKJcorr} (2) \\end{align*}\\] Of the notable new parts, \\(\\mathbf S\\) is the covariance matrix and \\(\\mathbf R\\) is the corresponding correlation matrix, which we might more fully express as \\[\\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix}\\] And according to our prior, \\(\\mathbf R\\) is distributed as \\(\\text{LKJcorr} (2)\\). We’ll use rethinking::rlkjcorr() to get a better sense of what that even is. library(rethinking) n_sim &lt;- 1e5 set.seed(13) r_1 &lt;- rlkjcorr(n_sim, K = 2, eta = 1) %&gt;% as_tibble() set.seed(13) r_2 &lt;- rlkjcorr(n_sim, K = 2, eta = 2) %&gt;% as_tibble() set.seed(13) r_4 &lt;- rlkjcorr(n_sim, K = 2, eta = 4) %&gt;% as_tibble() Here are the \\(\\text{LKJcorr}\\) distributions of Figure 13.3. ggplot(data = r_1, aes(x = V2)) + geom_density(color = &quot;transparent&quot;, fill = &quot;#DCA258&quot;, alpha = 2/3) + geom_density(data = r_2, color = &quot;transparent&quot;, fill = &quot;#FCF9F0&quot;, alpha = 2/3) + geom_density(data = r_4, color = &quot;transparent&quot;, fill = &quot;#394165&quot;, alpha = 2/3) + geom_text(data = tibble(x = c(.83, .62, .46), y = c(.54, .74, 1), label = c(&quot;eta = 1&quot;, &quot;eta = 2&quot;, &quot;eta = 4&quot;)), aes(x = x, y = y, label = label), color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring Okay, let’s get ready to model and switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) As defined above, our first model has both varying intercepts and afternoon slopes. I should point out that the (1 + afternoon | cafe) syntax specifies that we’d like brm() to fit the random effects for 1 (i.e., the intercept) and the afternoon slope as correlated. Had we wanted to fit a model in which they were orthogonal, we’d have coded (1 + afternoon || cafe). b13.1 &lt;- brm(data = d, family = gaussian, wait ~ 1 + afternoon + (1 + afternoon | cafe), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sd), prior(cauchy(0, 2), class = sigma), prior(lkj(2), class = cor)), iter = 5000, warmup = 2000, chains = 2, cores = 2, seed = 13) With Figure 13.4, we assess how the posterior for the correlation of the random effects compares to its prior. post &lt;- posterior_samples(b13.1) post %&gt;% ggplot(aes(x = cor_cafe__Intercept__afternoon)) + geom_density(data = r_2, aes(x = V2), color = &quot;transparent&quot;, fill = &quot;#EEDA9D&quot;, alpha = 3/4) + geom_density(color = &quot;transparent&quot;, fill = &quot;#A65141&quot;, alpha = 9/10) + annotate(&quot;text&quot;, label = &quot;posterior&quot;, x = -0.35, y = 2.2, color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + annotate(&quot;text&quot;, label = &quot;prior&quot;, x = 0, y = 0.9, color = &quot;#EEDA9D&quot;, alpha = 2/3, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring McElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the cafe-specific intercepts and afternoon slopes with coef(), which returns a three-dimensional list. # coef(b13.1) %&gt;% glimpse() coef(b13.1) ## $cafe ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 3.259169 0.1992591 2.878429 3.653497 ## 2 3.163126 0.2077802 2.764231 3.571470 ## 3 1.613333 0.2048788 1.214071 2.020841 ## 4 3.335507 0.1974706 2.949594 3.734603 ## 5 2.297652 0.2021495 1.902246 2.694731 ## 6 3.144568 0.2000372 2.749660 3.537188 ## 7 2.649992 0.2038993 2.242362 3.045645 ## 8 3.640516 0.2024258 3.247909 4.042081 ## 9 4.115420 0.1996007 3.723008 4.501883 ## 10 2.324889 0.2079583 1.908449 2.727862 ## 11 4.443441 0.2001195 4.055028 4.835634 ## 12 2.832879 0.1957106 2.448516 3.221008 ## 13 4.650760 0.2098050 4.237949 5.057530 ## 14 5.524169 0.2177284 5.097501 5.959747 ## 15 3.560360 0.1970021 3.164246 3.949434 ## 16 3.487883 0.2025698 3.086784 3.882643 ## 17 2.339923 0.1998181 1.958140 2.726994 ## 18 3.957517 0.2016838 3.564045 4.358334 ## 19 3.497136 0.1981075 3.106978 3.875221 ## 20 3.171816 0.1984817 2.772357 3.567600 ## ## , , afternoon ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.9079492 0.2121886 -1.3423270 -0.514139515 ## 2 -1.0423543 0.2421118 -1.5661461 -0.632140118 ## 3 -0.4258897 0.2309383 -0.8744802 0.019952192 ## 4 -0.7371826 0.2082224 -1.1318710 -0.301357371 ## 5 -0.5473586 0.2146359 -0.9547521 -0.114297666 ## 6 -0.7922547 0.1984415 -1.1808542 -0.394098256 ## 7 -0.4923971 0.2273306 -0.8963698 -0.009312471 ## 8 -0.8182089 0.2064763 -1.2103869 -0.391837649 ## 9 -1.0795703 0.2080591 -1.5121735 -0.694212786 ## 10 -0.4328146 0.2339193 -0.8541180 0.055244404 ## 11 -1.0918230 0.2181998 -1.5358138 -0.660927426 ## 12 -0.7572787 0.2058866 -1.1944722 -0.357107659 ## 13 -1.2090494 0.2262855 -1.6698164 -0.774227341 ## 14 -1.5270601 0.2677825 -2.0575562 -1.017941161 ## 15 -0.9245022 0.1973630 -1.3248250 -0.541319337 ## 16 -0.7358376 0.2157176 -1.1299586 -0.283584441 ## 17 -0.5644189 0.2135701 -0.9745232 -0.132798311 ## 18 -1.0058122 0.2049376 -1.4275187 -0.611649272 ## 19 -0.7452130 0.2103995 -1.1244911 -0.296927032 ## 20 -0.7322933 0.2046215 -1.1238668 -0.302647160 Here’s the code to extract the relevant elements from the coef() list, convert them to a tibble, and add the cafe index. partially_pooled_params &lt;- # with this line we select each of the 20 cafe&#39;s posterior mean (i.e., Estimate) # for both `Intercept` and `afternoon` coef(b13.1)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% # convert the two vectors to a tibble rename(Slope = afternoon) %&gt;% mutate(cafe = 1:nrow(.)) %&gt;% # add the `cafe` index select(cafe, everything()) # simply moving `cafe` to the leftmost position Like McElreath, we’ll compute the unpooled estimates directly from the data. # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% # with these two lines, we compute the mean value for each cafe&#39;s wait time # in the morning and then the afternoon group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% ungroup() %&gt;% # ungrouping allows us to alter afternoon, one of the grouping variables mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% spread(key = afternoon, value = mean) %&gt;% # use `spread()` just as in the previous block mutate(Slope = Slope - Intercept) # finally, here&#39;s our slope! # here we combine the partially-pooled and unpooled means into a single data object, # which will make plotting easier. params &lt;- # `bind_rows()` will stack the second tibble below the first bind_rows(partially_pooled_params, un_pooled_params) %&gt;% # index whether the estimates are pooled mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = nrow(.)/2)) # here&#39;s a glimpse at what we&#39;ve been working for params %&gt;% slice(c(1:5, 36:40)) ## # A tibble: 10 x 4 ## cafe Intercept Slope pooled ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 3.26 -0.908 partially ## 2 2 3.16 -1.04 partially ## 3 3 1.61 -0.426 partially ## 4 4 3.34 -0.737 partially ## 5 5 2.30 -0.547 partially ## 6 16 3.38 -0.465 not ## 7 17 2.29 -0.531 not ## 8 18 4.01 -1.07 not ## 9 19 3.39 -0.484 not ## 10 20 3.12 -0.617 not Finally, here’s our code for Figure 13.5.a, showing shrinkage in two dimensions. ggplot(data = params, aes(x = Intercept, y = Slope)) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 1/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 2/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 3/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 4/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 5/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 6/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 7/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 8/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 9/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = .99, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(params$Intercept), ylim = range(params$Slope)) + theme_pearl_earring Learn more about stat_ellipse(), here. Let’s prep for Figure 13.5.b. # retrieve the partially-pooled estimates with `coef()` partially_pooled_estimates &lt;- coef(b13.1)$cafe[ , 1, 1:2] %&gt;% # convert the two vectors to a tibble as_tibble() %&gt;% # the Intercept is the wait time for morning (i.e., `afternoon == 0`) rename(morning = Intercept) %&gt;% # `afternoon` wait time is the `morning` wait time plus the afternoon slope mutate(afternoon = morning + afternoon, cafe = 1:n()) %&gt;% # add the `cafe` index select(cafe, everything()) # compute unpooled estimates directly from data un_pooled_estimates &lt;- d %&gt;% # as above, with these two lines, we compute each cafe&#39;s mean wait value by time of day group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% # ungrouping allows us to alter the grouping variable, afternoon ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;morning&quot;, &quot;afternoon&quot;)) %&gt;% # this seperates out the values into morning and afternoon columns spread(key = afternoon, value = mean) estimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) The code for Figure 13.5.b. ggplot(data = estimates, aes(x = morning, y = afternoon)) + # nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` # functions in the previous plot mapply(function(level) { stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, # Enter the levels here level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(estimates$morning), ylim = range(estimates$afternoon)) + labs(x = &quot;morning wait (mins)&quot;, y = &quot;afternoon wait (mins)&quot;) + theme_pearl_earring 13.2 Example: Admission decisions and gender Let’s revisit the infamous UCB admissions data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Here we detach rethinking, reload brms, and augment the data a bit. detach(package:rethinking, unload = T) library(brms) rm(UCBadmit) d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0), dept_id = rep(1:6, each = 2)) 13.2.1 Varying intercepts. The statistical formula for our varying-intercepts logistic regression model follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept_id}_i} + \\beta \\text{male}_i \\\\ \\alpha_\\text{dept_id} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ \\end{align*}\\] Since there’s only one left-hand term in our (1 | dept_id) code, there’s only one random effect. b13.2 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd)), iter = 4500, warmup = 500, chains = 3, cores = 3, seed = 13, control = list(adapt_delta = 0.99)) Since we don’t have a depth=2 argument in brms::summary(), we’ll have to get creative. One way to look at the parameters is with b13.2$fit: b13.2$fit ## Inference for Stan model: 3056b2b8cf03dcc8f1a993c6328a1e5a. ## 3 chains, each with iter=4500; warmup=500; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept -0.61 0.02 0.65 -1.95 -0.97 -0.59 -0.23 0.68 1699 1 ## b_male -0.09 0.00 0.08 -0.26 -0.15 -0.09 -0.04 0.07 5332 1 ## sd_dept_id__Intercept 1.48 0.01 0.60 0.77 1.10 1.35 1.71 2.99 1794 1 ## r_dept_id[1,Intercept] 1.28 0.02 0.65 0.00 0.91 1.26 1.64 2.62 1716 1 ## r_dept_id[2,Intercept] 1.24 0.02 0.65 -0.05 0.86 1.22 1.60 2.59 1719 1 ## r_dept_id[3,Intercept] 0.03 0.02 0.65 -1.25 -0.35 0.01 0.39 1.37 1717 1 ## r_dept_id[4,Intercept] -0.01 0.02 0.65 -1.29 -0.38 -0.03 0.35 1.34 1708 1 ## r_dept_id[5,Intercept] -0.45 0.02 0.65 -1.75 -0.83 -0.47 -0.08 0.89 1707 1 ## r_dept_id[6,Intercept] -2.00 0.02 0.66 -3.32 -2.38 -2.01 -1.63 -0.69 1749 1 ## lp__ -61.86 0.05 2.55 -67.83 -63.34 -61.52 -59.96 -57.89 2570 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Jul 12 16:29:44 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). However, notice that the group-specific parameters don’t match up with those in the text. Though our r_dept_id[1,Intercept] had a posterior mean of 1.25, the number for a_dept[1] in the text is 0.67. This is because the brms package presented the random effects in the non-centered metric. The rethinking package, in contrast, presented the random effects in the centered metric. On page 399, McElreath wrote: Remember, the values above are the \\(\\alpha_\\text{DEPT}\\) estimates, and so they are deviations from the global mean \\(\\alpha\\), which in this case has posterior mean -0.58. So department A, “[1]” in the table, has the highest average admission rate. Department F, “[6]” in the table, has the lowest. Here’s another fun fact: # numbers taken from the mean column on page 399 in the text c(0.67, 0.63, -0.59, -0.62, -1.06, -2.61) %&gt;% mean() ## [1] -0.5966667 The average of the rethinking-based centered random effects is within rounding error of the global mean, -0.58. If you want the random effects in the centered metric from brms, you can use the coef() function: coef(b13.2) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6737911 0.09910493 0.4774741 0.8683414 ## 2 0.6278895 0.11640794 0.3960303 0.8532183 ## 3 -0.5834629 0.07497124 -0.7318164 -0.4374785 ## 4 -0.6157137 0.08554453 -0.7835556 -0.4457197 ## 5 -1.0571918 0.09784084 -1.2511932 -0.8678057 ## 6 -2.6082413 0.15503628 -2.9207624 -2.3103545 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.09429009 0.08100324 -0.2551681 0.06779126 ## 2 -0.09429009 0.08100324 -0.2551681 0.06779126 ## 3 -0.09429009 0.08100324 -0.2551681 0.06779126 ## 4 -0.09429009 0.08100324 -0.2551681 0.06779126 ## 5 -0.09429009 0.08100324 -0.2551681 0.06779126 ## 6 -0.09429009 0.08100324 -0.2551681 0.06779126 And just to confirm, the average of the posterior means of the Intercept random effects with brms::coef() is also the global mean within rounding error: mean(coef(b13.2)$dept_id[ , &quot;Estimate&quot;, &quot;Intercept&quot;]) ## [1] -0.5938215 Note how coef() returned a three-dimensional list. coef(b13.2) %&gt;% str() ## List of 1 ## $ dept_id: num [1:6, 1:4, 1:2] 0.674 0.628 -0.583 -0.616 -1.057 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;male&quot; If you just want the parameter summaries for the random intercepts, you have to use three-dimensional indexing. coef(b13.2)$dept_id[ , , &quot;Intercept&quot;] # this also works: coef(b13.2)$dept_id[ , , 1] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6737911 0.09910493 0.4774741 0.8683414 ## 2 0.6278895 0.11640794 0.3960303 0.8532183 ## 3 -0.5834629 0.07497124 -0.7318164 -0.4374785 ## 4 -0.6157137 0.08554453 -0.7835556 -0.4457197 ## 5 -1.0571918 0.09784084 -1.2511932 -0.8678057 ## 6 -2.6082413 0.15503628 -2.9207624 -2.3103545 So to get our brms summaries in a similar format to those in the text, we’ll have to combine coef() with fixef() and VarCorr(). rbind(coef(b13.2)$dept_id[, , &quot;Intercept&quot;], fixef(b13.2), VarCorr(b13.2)$dept_id$sd) ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.67379113 0.09910493 0.4774741 0.86834140 ## 2 0.62788949 0.11640794 0.3960303 0.85321826 ## 3 -0.58346287 0.07497124 -0.7318164 -0.43747846 ## 4 -0.61571373 0.08554453 -0.7835556 -0.44571966 ## 5 -1.05719178 0.09784084 -1.2511932 -0.86780565 ## 6 -2.60824132 0.15503628 -2.9207624 -2.31035450 ## Intercept -0.60909419 0.65114446 -1.9506815 0.68033295 ## male -0.09429009 0.08100324 -0.2551681 0.06779126 ## Intercept 1.48293014 0.59542542 0.7652144 2.98623954 And a little more data wrangling will make the summaries easier to read: rbind(coef(b13.2)$dept_id[, , &quot;Intercept&quot;], fixef(b13.2), VarCorr(b13.2)$dept_id$sd) %&gt;% as_tibble() %&gt;% mutate(parameter = c(paste(&quot;Intercept [&quot;, 1:6, &quot;]&quot;, sep = &quot;&quot;), &quot;Intercept&quot;, &quot;male&quot;, &quot;sigma&quot;)) %&gt;% select(parameter, everything()) %&gt;% mutate_if(is_double, round, digits = 2) ## # A tibble: 9 x 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Intercept [1] 0.67 0.1 0.48 0.87 ## 2 Intercept [2] 0.63 0.12 0.4 0.85 ## 3 Intercept [3] -0.580 0.07 -0.73 -0.44 ## 4 Intercept [4] -0.62 0.09 -0.78 -0.45 ## 5 Intercept [5] -1.06 0.1 -1.25 -0.87 ## 6 Intercept [6] -2.61 0.16 -2.92 -2.31 ## 7 Intercept -0.61 0.65 -1.95 0.68 ## 8 male -0.09 0.08 -0.26 0.07 ## 9 sigma 1.48 0.6 0.77 2.99 I’m not aware of a slick and easy way to get the n_eff and Rhat summaries into the mix. But if you’re fine with working with the brms-default non-centered parameterization, b13.2$fit gets you those just fine. One last thing. The broom package offers a very handy way to get those brms random effects. Just throw the model brm() fit into the tidy() function. library(broom) tidy(b13.2) %&gt;% mutate_if(is.numeric, round, digits = 2) # this line just rounds the output ## term estimate std.error lower upper ## 1 b_Intercept -0.61 0.65 -1.64 0.40 ## 2 b_male -0.09 0.08 -0.23 0.04 ## 3 sd_dept_id__Intercept 1.48 0.60 0.83 2.56 ## 4 r_dept_id[1,Intercept] 1.28 0.65 0.26 2.35 ## 5 r_dept_id[2,Intercept] 1.24 0.65 0.21 2.29 ## 6 r_dept_id[3,Intercept] 0.03 0.65 -1.00 1.07 ## 7 r_dept_id[4,Intercept] -0.01 0.65 -1.02 1.04 ## 8 r_dept_id[5,Intercept] -0.45 0.65 -1.47 0.60 ## 9 r_dept_id[6,Intercept] -2.00 0.66 -3.04 -0.95 ## 10 lp__ -61.86 2.55 -66.58 -58.34 But note how, just as with b13.2$fit, this approach summarizes the posterior with the non-centered parameterization. Which is a fine parameterization. It’s just a little different from what you’ll get when using precis( m13.2 , depth=2 ), as in the text. 13.2.2 Varying effects of being male. Now we’re ready to allow our male dummy to varies, too, the statistical model follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept_id}_i} + \\beta_{\\text{dept_id}_i} \\text{male}_i \\\\ \\begin{bmatrix} \\alpha_\\text{dept_id} \\\\ \\beta_\\text{dept_id} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 1) \\\\ (\\sigma_\\alpha, \\sigma_\\beta) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R &amp; \\sim \\text{LKJcorr} (2) \\end{align*}\\] Fit the model. b13.3 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 + male | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(2), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = .99, max_treedepth = 12)) McElreath encouraged us to make sure the chains look good. Instead of relying on convenience functions, let’s do it by hand. post &lt;- posterior_samples(b13.3, add_chain = T) post %&gt;% select(-lp__) %&gt;% gather(key, value, -chain, -iter) %&gt;% mutate(chain = as.character(chain)) %&gt;% ggplot(aes(x = iter, y = value, group = chain, color = chain)) + geom_line(size = 1/15) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#B1934A&quot;, &quot;#A65141&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(NULL, breaks = c(1001, 5000)) + ylab(NULL) + theme_pearl_earring + theme(legend.position = c(.825, .06), legend.direction = &quot;horizontal&quot;) + facet_wrap(~key, ncol = 3, scales = &quot;free_y&quot;) Our chains look great. While we’re at it, let’s examine the \\(\\hat{R}\\) vales in a handmade plot, too. rhat(b13.3) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;parameter&quot;, &quot;rhat&quot;) %&gt;% filter(parameter != &quot;lp__&quot;) %&gt;% ggplot(aes(x = rhat, y = reorder(parameter, rhat))) + geom_segment(aes(xend = 1, yend = parameter), color = &quot;#EEDA9D&quot;) + geom_point(aes(color = rhat &gt; 1), size = 2) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(x = NULL, y = NULL) + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Them are some respectable \\(\\hat{R}\\) values. The plot accentuates their differences, but they’re all basically 1 (e.g., see what happens is you set coord_cartesian(xlim = c(0.99, 1.01))). Here are the random effects in the centered metric: coef(b13.3) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.2997054 0.25696449 0.80211049 1.8160258 ## 2 0.7374013 0.33068668 0.09348209 1.4105884 ## 3 -0.6456650 0.08550191 -0.81546958 -0.4814419 ## 4 -0.6184620 0.10503486 -0.82668160 -0.4137505 ## 5 -1.1324600 0.11601956 -1.36503108 -0.9069383 ## 6 -2.6015706 0.20268140 -3.01268421 -2.2139207 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.78674874 0.2707370 -1.3231027 -0.2616853 ## 2 -0.20647250 0.3316969 -0.8856595 0.4452435 ## 3 0.08070568 0.1401311 -0.1875375 0.3616676 ## 4 -0.09209919 0.1413379 -0.3682070 0.1845360 ## 5 0.12099722 0.1886615 -0.2409682 0.4978260 ## 6 -0.12376542 0.2716941 -0.6674311 0.4057879 We may as well keep our doing-things-by-hand kick going. Instead relying on bayesplog::mcmc_intervals() or tidybayes::pointintervalh() to make our coefficient plot, we’ll combine geom_pointrange() and coord_flip(). But we will need to wrangle a bit to get those brms-based centered random effects into a usefully-formatted tidy tibble. # as far as I can tell, because `coef()` yields a list, you have to take out the two # random effects one at a time and then bind them together to get them ready for a tibble rbind(coef(b13.3)$dept_id[, , 1], coef(b13.3)$dept_id[, , 2]) %&gt;% as_tibble() %&gt;% mutate(param = c(paste(&quot;Intercept&quot;, 1:6), paste(&quot;male&quot;, 1:6)), reorder = c(6:1, 12:7)) %&gt;% # plot ggplot(aes(x = reorder(param, reorder))) + geom_hline(yintercept = 0, linetype = 3, color = &quot;#8B9DAF&quot;) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, y = Estimate, color = reorder &lt; 7), shape = 20, size = 3/4) + scale_color_manual(values = c(&quot;#394165&quot;, &quot;#A65141&quot;)) + xlab(NULL) + coord_flip() + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Just like in the text, our male slopes are much less dispersed than our intercepts. 13.2.3 Shrinkage. Figure 13.6.a depicts the correlation between the full UCB model’s varying intercepts and slopes. library(tidybayes) post &lt;- posterior_samples(b13.3) post %&gt;% ggplot(aes(x = cor_dept_id__Intercept__male, y = 0)) + geom_halfeyeh(fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;, point_interval = median_qi, .width = .95) + scale_x_continuous(breaks = c(-1, median(post$cor_dept_id__Intercept__male), 1), labels = c(-1, &quot;-.35&quot;, 1)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -1:1) + labs(subtitle = &quot;The dot is at the median; the\\nhorizontal bar is the 95% CI.&quot;, x = &quot;correlation&quot;) + theme_pearl_earring Much like for Figure 13.5.b, above, it’ll take a little data processing before we’re ready to reproduce Figure 13.6.b. # here we put the partially-pooled estimate summaries in a tibble partially_pooled_params &lt;- coef(b13.3)$dept_id[ , 1, ] %&gt;% as_tibble() %&gt;% set_names(&quot;intercept&quot;, &quot;slope&quot;) %&gt;% mutate(dept = 1:n()) %&gt;% select(dept, everything()) # in order to calculate the unpooled estimates from the data, we&#39;ll need a function that # can convert probabilities into the logit metric. if you do the algebra, this is just # a transformation of the `inv_logit_scaled()` function. prob_to_logit &lt;- function(x){ -log((1 / x) -1) } # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% group_by(male, dept_id) %&gt;% summarise(prob_admit = mean(admit / applications)) %&gt;% ungroup() %&gt;% mutate(male = ifelse(male == 0, &quot;intercept&quot;, &quot;slope&quot;)) %&gt;% spread(key = male, value = prob_admit) %&gt;% rename(dept = dept_id) %&gt;% # here we put our `prob_to_logit()` function to work mutate(intercept = prob_to_logit(intercept), slope = prob_to_logit(slope)) %&gt;% mutate(slope = slope - intercept) # here we combine the partially-pooled and unpooled means into a single data object params &lt;- bind_rows(partially_pooled_params, un_pooled_params) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) %&gt;% mutate(dept_letter = rep(LETTERS[1:6], times = 2)) # this will help with plotting params ## # A tibble: 12 x 5 ## dept intercept slope pooled dept_letter ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1.30 -0.787 partially A ## 2 2 0.737 -0.206 partially B ## 3 3 -0.646 0.0807 partially C ## 4 4 -0.618 -0.0921 partially D ## 5 5 -1.13 0.121 partially E ## 6 6 -2.60 -0.124 partially F ## 7 1 1.54 -1.05 not A ## 8 2 0.754 -0.220 not B ## 9 3 -0.660 0.125 not C ## 10 4 -0.622 -0.0820 not D ## 11 5 -1.16 0.200 not E ## 12 6 -2.58 -0.189 not F Here’s our version of Figure 13.6.b, depicting two-dimensional shrinkage for the partially-pooled multilevel estimates (posterior means) relative to the unpooled coefficients, calculated from the data. The ggrepel::geom_text_repel() function will help us with the in-plot labels. library(ggrepel) ggplot(data = params, aes(x = intercept, y = slope)) + mapply(function(level){ stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = dept, color = pooled)) + geom_line(aes(group = dept), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + geom_text_repel(data = params %&gt;% filter(pooled == &quot;partially&quot;), aes(label = dept_letter), color = &quot;#E8DCCF&quot;, size = 4, family = &quot;Courier&quot;, seed = 13.6) + coord_cartesian(xlim = range(params$intercept), ylim = range(params$slope)) + labs(x = expression(paste(&quot;intercept (&quot;, alpha[dept_id], &quot;)&quot;)), y = expression(paste(&quot;slope (&quot;, beta[dept_id], &quot;)&quot;))) + theme_pearl_earring 13.2.4 Model comparison. Fit the no-gender model. b13.4 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 2), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = .99, max_treedepth = 12)) Compare the three models by the WAIC. b13.2 &lt;- add_criterion(b13.2, &quot;waic&quot;) b13.3 &lt;- add_criterion(b13.3, &quot;waic&quot;) b13.4 &lt;- add_criterion(b13.4, &quot;waic&quot;) loo_compare(b13.2, b13.3, b13.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.3 0.0 0.0 -45.7 2.4 7.0 1.4 91.4 4.7 ## b13.4 -6.8 7.5 -52.5 9.0 6.5 2.3 105.1 18.1 ## b13.2 -8.5 6.6 -54.2 8.2 9.3 3.0 108.4 16.5 In terms of the WAIC estimates and \\(\\text{elpd}\\) differences, the models are similar. The story changes when we look at the WAIC weights. model_weights(b13.2, b13.3, b13.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b13.2 b13.3 b13.4 ## 0.000 0.999 0.001 The varying slopes model, [b13.3], dominates [the other two]. This is despite the fact that the average slope in [b13.3] is nearly zero. The average isn’t what matters, however. It is the individual slopes, one for each department, that matter. If we wish to generalize to new departments, the variation in slopes suggest that it’ll be worth paying attention to gender, even if the average slope is nearly zero in the population. (pp. 402–403, emphasis in the original) 13.3 Example: Cross-classified chimpanzees with varying slopes Retrieve the chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees detach(package:rethinking, unload = T) library(brms) rm(chimpanzees) d &lt;- d %&gt;% select(-recipient) %&gt;% mutate(block_id = block) My math’s aren’t the best. But if I’m following along correctly, here’s a fuller statistical expression of our cross-classified model. \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (n = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_i + (\\beta_{1i} + \\beta_{2i} \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_i &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block_id}_i} \\\\ \\beta_{1i} &amp; = \\beta_1 + \\beta_{1, \\text{actor}_i} + \\beta_{1, \\text{block_id}_i} \\\\ \\beta_{2i} &amp; = \\beta_2 + \\beta_{2, \\text{actor}_i} + \\beta_{2, \\text{block_id}_i} \\\\ \\begin{bmatrix} \\alpha_\\text{actor} \\\\ \\beta_{1, \\text{actor}} \\\\ \\beta_{2, \\text{actor}} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{actor} \\end{pmatrix} \\\\ \\begin{bmatrix} \\alpha_\\text{block_id} \\\\ \\beta_{1, \\text{block_id}} \\\\ \\beta_{2, \\text{block_id}} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{block_id} \\end{pmatrix} \\\\ \\mathbf S_\\text{actor} &amp; = \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\mathbf R_\\text{actor} \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\\\ \\mathbf S_\\text{block_id} &amp; = \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\mathbf R_\\text{block_id} \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 1) \\\\ (\\sigma_{\\alpha_\\text{actor}}, \\sigma_{\\beta_{1_\\text{actor}}}, \\sigma_{\\beta_{2_\\text{actor}}}) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ (\\sigma_{\\alpha_\\text{block_id}}, \\sigma_{\\beta_{1_\\text{block_id}}}, \\sigma_{\\beta_{2_\\text{block_id}}}) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R_\\text{actor} &amp; \\sim \\text{LKJcorr} (4) \\\\ \\mathbf R_\\text{block_id} &amp; \\sim \\text{LKJcorr} (4) \\end{align*}\\] And now each \\(\\mathbf R\\) is a \\(3 \\times 3\\) correlation matrix. Let’s fit this beast. b13.6 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + condition:prosoc_left + (1 + prosoc_left + condition:prosoc_left | actor) + (1 + prosoc_left + condition:prosoc_left | block_id), prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 3, cores = 3, seed = 13) Even though it’s not apparent in the syntax, our model b13.6 was already fit using the non-centered parameterization. Behind the scenes, Bürkner has brms do this automatically. It’s been that way all along. If you recall from last chapter, we can compute the number of effective samples for our parameters like so. ratios_cp &lt;- neff_ratio(b13.6) neff &lt;- ratios_cp %&gt;% as_tibble %&gt;% rename(neff_ratio = value) %&gt;% mutate(neff = neff_ratio * 12000) head(neff) ## # A tibble: 6 x 2 ## neff_ratio neff ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.235 2816. ## 2 0.582 6982. ## 3 0.576 6913. ## 4 0.328 3940. ## 5 0.493 5921. ## 6 0.501 6009. Now we’re ready for our variant of Figure 13.7. The handy ggbeeswarm package and its geom_quasirandom() function will give a better sense of the distribution. library(ggbeeswarm) neff %&gt;% ggplot(aes(x = factor(0), y = neff)) + geom_boxplot(fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;) + geom_quasirandom(method = &quot;tukeyDense&quot;, size = 2/3, color = &quot;#EEDA9D&quot;, alpha = 2/3) + scale_x_discrete(NULL, breaks = NULL, expand = c(.75, .75)) + scale_y_continuous(breaks = c(0, 6000, 12000)) + coord_cartesian(ylim = 0:12000) + labs(y = &quot;effective samples&quot;, subtitle = &quot;The non-centered\\nparameterization is the\\nbrms default. No fancy\\ncoding required.&quot;) + theme_pearl_earring As in the last chapter, we’ll use the bayesplot::mcmc_neff() function to examine the ratio of n.eff and the fill number of post-warm-up iterations, \\(N\\). Ideally, that ratio is closer to 1 than not. library(bayesplot) color_scheme_set(c(&quot;#DCA258&quot;, &quot;#EEDA9D&quot;, &quot;#394165&quot;, &quot;#8B9DAF&quot;, &quot;#A65141&quot;, &quot;#A65141&quot;)) mcmc_neff(ratios_cp, size = 2) + theme_pearl_earring Here are our standard deviation parameters. tidy(b13.6) %&gt;% filter(str_detect(term , &quot;sd_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 sd_actor__Intercept 2.33 0.88 1.30 3.94 ## 2 sd_actor__prosoc_left 0.46 0.37 0.04 1.14 ## 3 sd_actor__prosoc_left:condition 0.52 0.48 0.03 1.46 ## 4 sd_block_id__Intercept 0.23 0.21 0.01 0.62 ## 5 sd_block_id__prosoc_left 0.56 0.41 0.06 1.31 ## 6 sd_block_id__prosoc_left:condition 0.51 0.42 0.04 1.29 McElreath discussed rethinking::link() in the middle of page 407. He showed how his link(m13.6NC) code returned a list of four matrices, of which the p matrix was of primary interest. The brms::fitted() function doesn’t work quite the same way, here. fitted(b13.6, summary = F, nsamples = 1000) %&gt;% str() ## num [1:1000, 1:504] 0.32 0.359 0.503 0.262 0.331 ... First off, recall that fitted() returns summary values, by default. If we want individual values, set summary = FALSE. It’s also the fitted() default to use all posterior iterations, which is 12,000 in this case. To match the text, we need to set nsamples = 1000. But those are just details. The main point is that fitted() only returns one matrix, which is the analogue to the p matrix in the text. Moving forward, before we can follow along with McElreath’s R code 13.27, we need to refit the simpler model from way back in Chapter 12. b12.5 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + condition:prosoc_left + (1 | actor) + (1 | block_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 3, cores = 3, seed = 13) Now we can compare them by the WAIC. b13.6 &lt;- add_criterion(b13.6, &quot;waic&quot;) b12.5 &lt;- add_criterion(b12.5, &quot;waic&quot;) loo_compare(b13.6, b12.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b12.5 0.0 0.0 -266.3 9.8 10.3 0.5 532.5 19.7 ## b13.6 -1.2 2.0 -267.5 9.9 18.4 0.9 535.0 19.9 model_weights(b13.6, b12.5, weights = &quot;waic&quot;) ## b13.6 b12.5 ## 0.224034 0.775966 In this example, no matter which varying effect structure you use, you’ll find that actors vary a lot in their baseline preference for the left-hand lever. Everything else is much less important. But using the most complex model, [b13.6], tells the correct story. Because the varying slopes are adaptively regularized, the model hasn’t overfit much, relative to the simpler model that contains only the important intercept variation. (p. 408) 13.4 Continuous categories and the Gaussian process There is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works. We’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 410) 13.4.1 Example: Spatial autocorrelation in Oceanic tools. # load the distance matrix library(rethinking) data(islandsDistMatrix) # display short column names, so fits on screen d_mat &lt;- islandsDistMatrix colnames(d_mat) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) round(d_mat, 1) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 If you wanted to use color to more effectively visualize the values in the matirx, you might do something like this. d_mat %&gt;% as_tibble() %&gt;% gather() %&gt;% rename(column = key, distance = value) %&gt;% mutate(row = rep(rownames(d_mat), times = 10), row_order = rep(9:0, times = 10), column_order = rep(0:9, each = 10)) %&gt;% ggplot(aes(x = reorder(column, column_order), y = reorder(row, row_order))) + geom_raster(aes(fill = distance)) + geom_text(aes(label = round(distance, digits = 1)), size = 3, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme_pearl_earring + theme(axis.ticks = element_blank(), axis.text.y = element_text(hjust = 0)) Figure 13.8 shows the “shape of the function relating distance to the covariance \\(\\mathbf{K}_{ij}\\).” tibble( x = seq(from = 0, to = 4, by = .01), linear = exp(-1 * x), squared = exp(-1 * x^2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = linear), color = &quot;#B1934A&quot;, linetype = 2) + geom_line(aes(y = squared), color = &quot;#DCA258&quot;) + scale_x_continuous(&quot;distance&quot;, expand = c(0, 0)) + scale_y_continuous(&quot;correlation&quot;, breaks = c(0, .5, 1), labels = c(0, &quot;.5&quot;, 1)) + theme_pearl_earring Load the data. data(Kline2) # load the ordinary data, now with coordinates d &lt;- Kline2 %&gt;% mutate(society = 1:10) rm(Kline2) d %&gt;% glimpse() ## Observations: 10 ## Variables: 10 ## $ culture &lt;fct&gt; Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tonga,… ## $ population &lt;int&gt; 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000 ## $ contact &lt;fct&gt; low, low, low, high, high, high, high, low, high, low ## $ total_tools &lt;int&gt; 13, 22, 24, 43, 33, 19, 40, 28, 55, 71 ## $ mean_TU &lt;dbl&gt; 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6 ## $ lat &lt;dbl&gt; -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9 ## $ lon &lt;dbl&gt; 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6 ## $ lon2 &lt;dbl&gt; -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4 ## $ logpop &lt;dbl&gt; 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.47270… ## $ society &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Okay, it appears this is going to be a bit of a ride. It’s not entirely clear to me if we can fit a Gaussian process model in brms that’s a direct equivalent to what McElreath did with rethinking. But we can try. First, note our use of the gp() syntax in the brm() function, below. We’re attempting to tell brms that we would like to include latitude and longitude (i.e., lat and long2, respectively) in a Gaussian process. Also note how our priors are a little different than those in the text. I’ll explain, below. Let’s just move ahead and fit the model. b13.7 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(inv_gamma(2.874624, 0.393695), class = lscale), prior(cauchy(0, 1), class = sdgp)), iter = 1e4, warmup = 2000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = 0.999, max_treedepth = 12)) Here’s the model summary. posterior_summary(b13.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.45 1.12 -0.78 3.77 ## b_logpop 0.23 0.11 0.02 0.45 ## sdgp_gplatlon2 0.52 0.36 0.16 1.47 ## lscale_gplatlon2 0.23 0.13 0.07 0.56 ## zgp_gplatlon2[1] -0.60 0.79 -2.16 0.94 ## zgp_gplatlon2[2] 0.44 0.85 -1.25 2.09 ## zgp_gplatlon2[3] -0.63 0.70 -1.97 0.91 ## zgp_gplatlon2[4] 0.88 0.70 -0.45 2.29 ## zgp_gplatlon2[5] 0.25 0.75 -1.23 1.73 ## zgp_gplatlon2[6] -1.00 0.79 -2.56 0.60 ## zgp_gplatlon2[7] 0.13 0.72 -1.40 1.53 ## zgp_gplatlon2[8] -0.18 0.88 -1.90 1.59 ## zgp_gplatlon2[9] 0.41 0.92 -1.49 2.14 ## zgp_gplatlon2[10] -0.31 0.82 -1.93 1.28 ## lp__ -51.50 3.18 -58.69 -46.33 Our Gaussian process parameters are different than McElreath’s. From the brms reference manual, here’s the brms parameterization: \\[k(x_{i},x_{j}) = sdgp^2 \\text{exp} \\big (-||x_i - x_j||^2 / (2 lscale^2) \\big )\\] What McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^2\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp_gplatlon2 before it’s on the same scale as etasq in the text. Here it is. posterior_samples(b13.7) %&gt;% transmute(sdgp_squared = sdgp_gplatlon2^2) %&gt;% mean_hdi(sdgp_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## sdgp_squared .lower .upper .width .point .interval ## 1 0.407 0 0.761 0.89 mean hdi Now we’re in the ballpark. In our model brm() code, above, we just went with the flow and kept the cauchy(0, 1) prior on sdgp. Now look at the denominator of the inner part of Bürkner equation, \\(2 lscale^2\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^2\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^2\\) directly as rhosq. If I’m doing the algebra correctly–and that may well be a big if–, we might expect: \\[\\rho^2 = 1/(2 \\cdot lscale^2)\\] But that doesn’t appear to be the case. Sigh. posterior_samples(b13.7) %&gt;% transmute(rho_squared = 1 / (2 * lscale_gplatlon2^2)) %&gt;% mean_hdi(rho_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## rho_squared .lower .upper .width .point .interval ## 1 21.829 0.433 47.516 0.89 mean hdi Oh man, that isn’t even close to the 2.67 McElreath reported in the text. The plot deepens. If you look back, you’ll see we used a very different prior for \\(lscale\\). Here is it: inv_gamma(2.874624, 0.393695). Use get_prior() to discover where that came from. get_prior(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b logpop ## 3 student_t(3, 3, 10) Intercept ## 4 normal(0, 0.5) lscale ## 5 inv_gamma(2.874624, 0.393695) lscale gplatlon2 ## 6 student_t(3, 0, 10) sdgp ## 7 sdgp gplatlon2 That is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt [of the Stan team]’s vignette on the topic. Though it isn’t included in this document, I also ran the model with the cauchy(0, 1) prior and the results were quite similar. So the big discrepancy between our model and the one in the text isn’t based on that prior. Now that we’ve hopped on the comparison train, we may as well keep going down the track. Let’s reproduce McElreath’s model with rethinking. Switch out brms for rethinking. detach(package:brms, unload = T) library(rethinking) Now fit the rethinking::map2stan() model. m13.7 &lt;- map2stan( alist( total_tools ~ dpois(lambda), log(lambda) &lt;- a + g[society] + bp*logpop, g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ), a ~ dnorm(0,10), bp ~ dnorm(0,1), etasq ~ dcauchy(0,1), rhosq ~ dcauchy(0,1) ), data=list( total_tools=d$total_tools, logpop=d$logpop, society=d$society, Dmat=islandsDistMatrix), warmup=2000 , iter=1e4 , chains=4) Alright, now we’ll work directly with the posteriors to make some visual comparisons. # rethinking-based posterior post_m13.7 &lt;- rethinking::extract.samples(m13.7)[2:5] %&gt;% as_tibble() detach(package:rethinking, unload = T) library(brms) # brms-based posterior post_b13.7 &lt;- posterior_samples(b13.7) Here’s the model intercept posterior, by package. post_m13.7[, &quot;a&quot;] %&gt;% bind_rows(post_b13.7%&gt;% transmute(a = b_Intercept)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = a, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Not identical, but pretty close&quot;, x = &quot;intercept&quot;) + theme_pearl_earring Now check the slopes. post_m13.7[, &quot;bp&quot;] %&gt;% bind_rows(post_b13.7 %&gt;% transmute(bp = b_logpop) ) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = bp, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Again, pretty close&quot;, x = &quot;slope&quot;) + theme_pearl_earring This one, \\(\\eta^2\\), required a little transformation. post_m13.7[, &quot;etasq&quot;] %&gt;% bind_rows(post_b13.7 %&gt;% transmute(etasq = sdgp_gplatlon2^2)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = etasq, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Still in the same ballpark&quot;, x = expression(eta^2)) + coord_cartesian(xlim = 0:3) + theme_pearl_earring \\(\\rho^2\\) required more extensive transformation of the brms posterior: post_m13.7[, &quot;rhosq&quot;] %&gt;% bind_rows(post_b13.7%&gt;% transmute(rhosq = 1 / (2 * (lscale_gplatlon2^2))) ) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = rhosq, fill = package)) + geom_density(size = 0) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(title = &quot;Holy smokes are those not the same!&quot;, subtitle = &quot;Notice how differently the y axes got scaled. Also, that brms density is\\nright skewed for days.&quot;, x = expression(rho^2)) + coord_cartesian(xlim = 0:50) + theme_pearl_earring + theme(legend.position = &quot;none&quot;) + facet_wrap(~package, scales = &quot;free_y&quot;) I’m in clinical psychology. Folks in my field don’t tend to use Gaussian processes, so getting to the bottom of this is low on my to-do list. Perhaps one of y’all are more experienced with Gaussian processes and see a flaw somewhere in my code. Please hit me up if you do. Anyways, here’s our brms + ggplot2 version of Figure 13.9. # for `sample_n()` set.seed(13) # wrangle post_b13.7 %&gt;% transmute(iter = 1:n(), etasq = sdgp_gplatlon2^2, rhosq = lscale_gplatlon2^2 * .5) %&gt;% sample_n(100) %&gt;% expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 55, by = 1)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% # plot ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, alpha = 1/4, color = &quot;#EEDA9D&quot;) + stat_function(fun = function(x) median(post_b13.7$sdgp_gplatlon2)^2 * exp(-median(post_b13.7$lscale_gplatlon2)^2 *.5 * x^2), color = &quot;#EEDA9D&quot;, size = 1.1) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = seq(from = 0, to = 50, by = 10)) + coord_cartesian(xlim = 0:50, ylim = 0:1) + theme_pearl_earring Do note the scale on which we placed our x axis. Our brms parameterization resulted in a gentler decline in spatial covariance. Let’s finish this up and “push the parameters back through the function for \\(\\mathbf{K}\\), the covariance matrix” (p. 415). # compute posterior median covariance among societies k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(post_b13.7$sdgp_gplatlon2^2) * exp(-median(post_b13.7$lscale_gplatlon2^2) * islandsDistMatrix[i, j]^2) diag(k) &lt;- median(post_b13.7$sdgp_gplatlon2^2) + 0.01 k %&gt;% round(2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.20 0.19 0.19 0.09 0.18 0.16 0.13 0.14 0.16 0.05 ## [2,] 0.19 0.20 0.19 0.09 0.18 0.16 0.13 0.14 0.16 0.06 ## [3,] 0.19 0.19 0.20 0.10 0.17 0.17 0.14 0.15 0.15 0.06 ## [4,] 0.09 0.09 0.10 0.20 0.06 0.15 0.17 0.17 0.04 0.02 ## [5,] 0.18 0.18 0.17 0.06 0.20 0.12 0.10 0.10 0.18 0.07 ## [6,] 0.16 0.16 0.17 0.15 0.12 0.20 0.17 0.18 0.10 0.03 ## [7,] 0.13 0.13 0.14 0.17 0.10 0.17 0.20 0.18 0.07 0.05 ## [8,] 0.14 0.14 0.15 0.17 0.10 0.18 0.18 0.20 0.08 0.03 ## [9,] 0.16 0.16 0.15 0.04 0.18 0.10 0.07 0.08 0.20 0.07 ## [10,] 0.05 0.06 0.06 0.02 0.07 0.03 0.05 0.03 0.07 0.20 And we’ll continue to follow suit and change these to a correlation matrix. # convert to correlation matrix rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;,&quot;Ti&quot;,&quot;SC&quot;,&quot;Ya&quot;,&quot;Fi&quot;,&quot;Tr&quot;,&quot;Ch&quot;,&quot;Mn&quot;,&quot;To&quot;,&quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.94 0.93 0.44 0.89 0.80 0.63 0.69 0.83 0.26 ## Ti 0.94 1.00 0.95 0.47 0.89 0.81 0.68 0.71 0.81 0.31 ## SC 0.93 0.95 1.00 0.52 0.86 0.84 0.72 0.76 0.77 0.29 ## Ya 0.44 0.47 0.52 1.00 0.29 0.74 0.86 0.85 0.21 0.12 ## Fi 0.89 0.89 0.86 0.29 1.00 0.62 0.49 0.51 0.93 0.36 ## Tr 0.80 0.81 0.84 0.74 0.62 1.00 0.83 0.92 0.51 0.16 ## Ch 0.63 0.68 0.72 0.86 0.49 0.83 1.00 0.89 0.37 0.24 ## Mn 0.69 0.71 0.76 0.85 0.51 0.92 0.89 1.00 0.40 0.15 ## To 0.83 0.81 0.77 0.21 0.93 0.51 0.37 0.40 1.00 0.34 ## Ha 0.26 0.31 0.29 0.12 0.36 0.16 0.24 0.15 0.34 1.00 The correlations in our rho matrix look a little higher than those in the text. Before we get see them in a plot, let’s consider psize. If you really want to scale the points in Figure 13.10.a like McElreath did, you can make the psize variable in a tidyverse sort of way as follows. However, if you compare the psize method and the default ggplot2 method using just logpop, you’ll see the difference is negligible. In that light, I’m going to be lazy and just use logpop in my plots. d %&gt;% transmute(psize = logpop / max(logpop)) %&gt;% transmute(psize = exp(psize * 1.5) - 2) ## psize ## 1 0.3134090 ## 2 0.4009582 ## 3 0.6663711 ## 4 0.7592196 ## 5 0.9066890 ## 6 0.9339560 ## 7 0.9834797 ## 8 1.1096138 ## 9 1.2223112 ## 10 2.4816891 As far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so. tidy_rho &lt;- rho %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(d %&gt;% select(culture, logpop, total_tools, lon2, lat)) %&gt;% gather(colname, correlation, -rowname, -culture, -logpop, -total_tools, -lon2, -lat) %&gt;% mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) %&gt;% select(rowname, colname, group, culture, everything()) head(tidy_rho) ## rowname colname group culture logpop total_tools lon2 lat correlation ## 1 Ml Ml MlMl Malekula 7.003065 13 -12.5 -16.3 1.00 ## 2 Ti Ml MlTi Tikopia 7.313220 22 -11.2 -12.3 0.94 ## 3 SC Ml MlSC Santa Cruz 8.188689 24 -14.0 -10.7 0.93 ## 4 Ya Ml MlYa Yap 8.474494 43 -41.9 9.5 0.44 ## 5 Fi Ml FiMl Lau Fiji 8.909235 33 -1.9 -17.7 0.89 ## 6 Tr Ml MlTr Trobriand 8.987197 19 -29.1 -8.7 0.80 Okay, here’s our version of Figure 13.10.a. tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_line(aes(group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(d$lon2), ylim = range(d$lat)) + theme(legend.position = &quot;none&quot;) + theme_pearl_earring Yep, as expressed by the intensity of the colors of the connecting lines, those correlations are more pronounced than those in the text. Here’s our version of Figure 13.10.b. # new data for `fitted()` nd &lt;- tibble(logpop = seq(from = 6, to = 14, length.out = 30), lat = median(d$lat), lon2 = median(d$lon2)) # `fitted()` f &lt;- fitted(b13.7, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # plot tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#394165&quot;, color = &quot;#100F14&quot;, alpha = .5, size = 1.1) + geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(y = total_tools, size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(y = total_tools, label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(d$logpop), ylim = range(d$total_tools)) + theme(legend.position = &quot;none&quot;) + theme_pearl_earring Same deal. Our higher correlations make for a more intensely-webbed plot. To learn more on Bürkner’s thoughts on this model in brms, check out the thread on this issue. 13.5 Summary Bonus: Another Berkley-admissions-data-like example. McElreath uploaded recordings of him teaching out of his text for a graduate course during the 2017/2018 fall semester. In the beginning of lecture 13 from week 7, he discussed a paper from van der Lee and Ellemers (2015) published an article in PNAS. Their paper suggested male researchers were more likely than female researchers to get research funding in the Netherlands. In their initial analysis (p. 12350) they provided a simple \\(\\chi^2\\) test to test the null hypothesis there was no difference in success for male versus female researchers, for which they reported \\(\\chi_{df = 1}^2 = 4.01, p = .045\\). Happily, van der Lee and Ellemers provided their data values in their supplemental material (i.e., Table S1.), which McElreath also displayed in his video. Their data follows the same structure as the Berkley admissions data. In his lecture, McElreath suggested their \\(\\chi^2\\) test is an example of Simpson’s paradox, just as with the Berkley data. He isn’t the first person to raise this criticism (see Volker and SteenBeek’s critique, which McElreath also pointed to in the lecture). Here are the data: funding &lt;- tibble( discipline = rep(c(&quot;Chemical sciences&quot;, &quot;Physical sciences&quot;, &quot;Physics&quot;, &quot;Humanities&quot;, &quot;Technical sciences&quot;, &quot;Interdisciplinary&quot;, &quot;Earth/life sciences&quot;, &quot;Social sciences&quot;, &quot;Medical sciences&quot;), each = 2), gender = rep(c(&quot;m&quot;, &quot;f&quot;), times = 9), applications = c(83, 39, 135, 39, 67, 9, 230, 166, 189, 62, 105, 78, 156, 126, 425, 409, 245, 260) %&gt;% as.integer(), awards = c(22, 10, 26, 9, 18, 2, 33, 32, 30, 13, 12, 17, 38, 18, 65, 47, 46, 29) %&gt;% as.integer(), rejects = c(61, 29, 109, 30, 49, 7, 197, 134, 159, 49, 93, 61, 118, 108, 360, 362, 199, 231) %&gt;% as.integer(), male = ifelse(gender == &quot;f&quot;, 0, 1) %&gt;% as.integer() ) funding ## # A tibble: 18 x 6 ## discipline gender applications awards rejects male ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Chemical sciences m 83 22 61 1 ## 2 Chemical sciences f 39 10 29 0 ## 3 Physical sciences m 135 26 109 1 ## 4 Physical sciences f 39 9 30 0 ## 5 Physics m 67 18 49 1 ## 6 Physics f 9 2 7 0 ## 7 Humanities m 230 33 197 1 ## 8 Humanities f 166 32 134 0 ## 9 Technical sciences m 189 30 159 1 ## 10 Technical sciences f 62 13 49 0 ## 11 Interdisciplinary m 105 12 93 1 ## 12 Interdisciplinary f 78 17 61 0 ## 13 Earth/life sciences m 156 38 118 1 ## 14 Earth/life sciences f 126 18 108 0 ## 15 Social sciences m 425 65 360 1 ## 16 Social sciences f 409 47 362 0 ## 17 Medical sciences m 245 46 199 1 ## 18 Medical sciences f 260 29 231 0 Let’s fit a few models. First, we’ll fit an analogue to the initial van der Lee and Ellemers \\(\\chi^2\\) test. Since we’re Bayesian modelers, we’ll use a simple logistic regression, using male (dummy coded 0 = female, 1 = male) to predict admission (i.e., awards). b13.bonus_0 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male, # note our continued use of weakly-regularizing priors prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13) If you inspect them, the chains look great. Here are the posterior summaries: tidy(b13.bonus_0) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.74 0.08 -1.88 -1.61 ## 2 b_male 0.21 0.11 0.04 0.38 Yep, the 95% intervals for male dummy exclude zero. If you wanted a one-sided Bayesian \\(p\\)-value, you might do something like: posterior_samples(b13.bonus_0) %&gt;% summarise(one_sided_Bayesian_p_value = mean(b_male &lt;= 0)) ## one_sided_Bayesian_p_value ## 1 0.02225 Pretty small. But recall how Simpson’s paradox helped us understand the Berkley data. Different departments in Berkley had different acceptance rates AND different ratios of male and female applicants. Similarly, different academic disciplines in the Netherlands might have different award rates for funding AND different ratios of male and female applications. Just like in section 13.2, let’s fit two more models. The first model will allow intercepts to vary by discipline. The second model will allow intercepts and the male dummy slopes to vary by discipline. b13.bonus_1 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99), seed = 13) b13.bonus_2 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 + male | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99), seed = 13) We’ll compare the models with information criteria. b13.bonus_0 &lt;- add_criterion(b13.bonus_0, &quot;waic&quot;) b13.bonus_1 &lt;- add_criterion(b13.bonus_1, &quot;waic&quot;) b13.bonus_2 &lt;- add_criterion(b13.bonus_2, &quot;waic&quot;) loo_compare(b13.bonus_0, b13.bonus_1, b13.bonus_2, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.bonus_2 0.0 0.0 -58.3 2.8 8.8 1.1 116.6 5.6 ## b13.bonus_1 -4.6 1.4 -62.9 3.7 10.1 1.5 125.9 7.3 ## b13.bonus_0 -6.6 2.7 -64.9 4.5 4.8 1.4 129.9 9.0 The WAIC suggests the varying intercepts/varying slopes model made the best sense of the data. Here we’ll practice our tidybayes::spread_draws() skills from the end of the last chapter to see what the random intercepts look like in a coefficient plot. b13.bonus_2 %&gt;% spread_draws(b_male, r_discipline[discipline,term]) %&gt;% filter(term == &quot;male&quot;) %&gt;% ungroup() %&gt;% mutate(effect = b_male + r_discipline, discipline = str_replace(discipline, &quot;[.]&quot;, &quot; &quot;)) %&gt;% ggplot(aes(x = effect, y = reorder(discipline, effect))) + geom_vline(xintercept = 0, color = &quot;#E8DCCF&quot;, alpha = 1/2) + geom_halfeyeh(.width = .95, size = .9, relative_scale = .9, color = &quot;#80A0C7&quot;, fill = &quot;#394165&quot;) + labs(title = &quot;Random slopes for the male dummy&quot;, subtitle = &quot;The dots and horizontal lines are the posterior means and percentile-based\\n95% intervals, respectively. The values are on the log scale.&quot;, x = NULL, y = NULL) + coord_cartesian(xlim = c(-1.5, 1.5)) + theme_pearl_earring + theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Note how the 95% intervals for all the random male slopes contain zero within their bounds. Here are the fixed effects: tidy(b13.bonus_2) %&gt;% filter(str_detect(term , &quot;b_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.63 0.15 -1.85 -1.38 ## 2 b_male 0.15 0.17 -0.14 0.42 And if you wanted a one-sided Bayesian \\(p\\)-value for the male dummy for the full model: posterior_samples(b13.bonus_2) %&gt;% summarise(one_sided_Bayesian_p_value = mean(b_male &lt;= 0)) ## one_sided_Bayesian_p_value ## 1 0.1761875 So, the estimate of the gender bias is small and consistent with the null hypothesis. Which is good! We want gender equality for things like funding success. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rethinking_2.01 brms_2.9.0 bayesplot_1.7.0 ggbeeswarm_0.6.0 ggrepel_0.8.1 ## [6] tidybayes_1.1.0 broom_0.5.2 Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 ## [11] StanHeaders_2.18.1 dutchmasters_0.1.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [16] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 ## [21] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 jsonlite_1.6 shiny_1.3.2 ## [22] compiler_3.6.3 httr_1.4.0 backports_1.1.4 ## [25] assertthat_0.2.1 Matrix_1.2-17 lazyeval_0.2.2 ## [28] cli_1.1.0 later_0.8.0 htmltools_0.3.6 ## [31] prettyunits_1.0.2 tools_3.6.3 igraph_1.2.4.1 ## [34] coda_0.19-2 gtable_0.3.0 glue_1.3.1 ## [37] reshape2_1.4.3 V8_2.2 cellranger_1.1.0 ## [40] vctrs_0.1.0 nlme_3.1-144 crosstalk_1.0.0 ## [43] xfun_0.7 ps_1.3.0 rvest_0.3.4 ## [46] mime_0.7 miniUI_0.1.1.1 lifecycle_0.1.0 ## [49] gtools_3.8.1 nleqslv_3.3.2 MASS_7.3-51.5 ## [52] zoo_1.8-6 scales_1.1.1.9000 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] curl_3.3 gridExtra_2.3 loo_2.1.0 ## [64] stringi_1.4.3 dygraphs_1.1.1.6 boot_1.3-24 ## [67] pkgbuild_1.0.3 shape_1.4.4 rlang_0.4.0 ## [70] pkgconfig_2.0.2 matrixStats_0.54.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [76] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [79] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [82] bookdown_0.11 R6_2.4.0 generics_0.0.2 ## [85] pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [88] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [94] rmarkdown_1.13 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [100] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.3 ## [103] munsell_0.5.0 beeswarm_0.2.3 vipor_0.4.5 ## [106] shinyjs_1.0 "],
["missing-data-and-other-opportunities.html", "14 Missing Data and Other Opportunities 14.1 Measurement error 14.2 Missing data 14.3 Summary Bonus: Meta-analysis Reference Session info", " 14 Missing Data and Other Opportunities For the opening example, we’re playing with the conditional probability \\[ \\text{Pr(burnt down | burnt up)} = \\frac{\\text{Pr(burnt up, burnt down)}}{\\text{Pr(burnt up)}} \\] It works out that \\[ \\text{Pr(burnt down | burnt up)} = \\frac{1/3}{1/2} = \\frac{2}{3} \\] We might express the math in the middle of page 423 in tibble form like this. library(tidyverse) p_pancake &lt;- 1/3 ( d &lt;- tibble(pancake = c(&quot;BB&quot;, &quot;BU&quot;, &quot;UU&quot;), p_burnt = c(1, .5, 0)) %&gt;% mutate(p_burnt_up = p_burnt * p_pancake) ) ## # A tibble: 3 x 3 ## pancake p_burnt p_burnt_up ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BB 1 0.333 ## 2 BU 0.5 0.167 ## 3 UU 0 0 d %&gt;% summarise(`p (burnt_down | burnt_up)` = p_pancake / sum(p_burnt_up)) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.667 I understood McElreath’s simulation better after breaking it apart. The first part of sim_pancake() takes one random draw from the integers 1, 2, and 3. It just so happens that if we set set.seed(1), the code returns a 1. set.seed(1) sample(x = 1:3, size = 1) ## [1] 1 So here’s what it looks like if we use seeds 2:11. take_sample &lt;- function(seed){ set.seed(seed) sample(x = 1:3, size = 1) } tibble(seed = 2:11) %&gt;% mutate(value_returned = map_dbl(seed, take_sample)) ## # A tibble: 10 x 2 ## seed value_returned ## &lt;int&gt; &lt;dbl&gt; ## 1 2 1 ## 2 3 1 ## 3 4 3 ## 4 5 2 ## 5 6 1 ## 6 7 2 ## 7 8 3 ## 8 9 3 ## 9 10 3 ## 10 11 2 Each of those value_returned values stands for one of the three pancakes: 1 = BB, 2 = BU, 3 = UU. In the next line, McElreath made slick use of a matrix to specify that. Here’s what the matrix looks like: matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] 1 0 0 See how the three columns are identified as [,1], [,2], and [,3]? If, say, we wanted to subset the values in the second column, we’d execute matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] ## [1] 1 0 which returns a numeric vector. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] %&gt;% str() ## num [1:2] 1 0 And that 1 0 corresponds to the pancake with one burnt (i.e., 1) and one unburnt (i.e., 0) side. So when McElreath then executed sample(sides), he randomly sampled from one of those two values. In the case of pancake == 2, he randomly sampled one the pancake with one burnt and one unburnt side. Had he sampled from pancake == 1, he would have sampled from the pancake with both sides burnt. Going forward, let’s amend McElreath’s sim_pancake() function a bit. First, we’ll add a seed argument, with will allow us to make the output reproducible. We’ll be inserting seed into set.seed() in the two places preceding the sample() function. The second major change is that we’re going to convert the output of the sim_pancake() function to a tibble and adding a side column, which will contain the values c(&quot;up&quot;, &quot;down&quot;). Just for pedagogical purposes, we’ll also add pancake_n and pancake_chr columns to help index which pancake the draws came from. # simulate a `pancake` and return randomly ordered `sides` sim_pancake &lt;- function(seed) { set.seed(seed) pancake &lt;- sample(x = 1:3, size = 1) sides &lt;- matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, pancake] set.seed(seed) sample(sides) %&gt;% as_tibble() %&gt;% mutate(side = c(&quot;up&quot;, &quot;down&quot;), pancake_n = pancake, pancake_chr = ifelse(pancake == 1, &quot;BB&quot;, ifelse(pancake == 2, &quot;BU&quot;, &quot;UU&quot;))) } Let’s take this baby for a whirl. # how many simulations would you like? n_sim &lt;- 1e4 ( d &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(r = map(seed, sim_pancake)) %&gt;% unnest() ) ## # A tibble: 20,000 x 5 ## seed value side pancake_n pancake_chr ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 up 1 BB ## 2 1 1 down 1 BB ## 3 2 1 up 1 BB ## 4 2 1 down 1 BB ## 5 3 1 up 1 BB ## 6 3 1 down 1 BB ## 7 4 0 up 3 UU ## 8 4 0 down 3 UU ## 9 5 0 up 2 BU ## 10 5 1 down 2 BU ## # … with 19,990 more rows And now we’ll spread() and summarise() to get the value we’ve been working for. d %&gt;% spread(key = side, value = value) %&gt;% summarise(`p (burnt_down | burnt_up)` = sum(up == 1 &amp; down == 1) / ( sum(up == 1))) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 1 The results are within rounding error of the ideal 2/3. Probability theory is not difficult mathematically. It’s just counting. But it is hard to interpret and apply. Doing so often seems to require some cleverness, and authors have an incentive to solve problems in clever ways, just to show off. But we don’t need that cleverness, if we ruthlessly apply conditional probability… In this chapter, [we’ll] meet two commonplace applications of this assume-and-deduce strategy. The first is the incorporation of measurement error into our models. The second is the estimation of missing data through Bayesian imputation… In neither application do [we] have to intuit the consequences of measurement errors nor the implications of missing values in order to design the models. All [we] have to do is state [the] information about the error or about the variables with missing values. Logic does the rest. (p. 424) 14.1 Measurement error First, let’s grab our WaffleDivorce data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce rm(WaffleDivorce) Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) The brms package currently supports theme_black(), which changes the default ggplot2 theme to a black background with white lines, text, and so forth. You can find the origins of the code, here. Though I like the idea of brms including theme_black(), I’m not a fan of some of the default settings (e.g., it includes gridlines). Happily, data scientist Tyler Rinker has some nice alternative theme_black() code you can find here. The version of theme_black() used for this chapter is based on his version, with a few amendments of my own. theme_black &lt;- function(base_size=12, base_family=&quot;&quot;) { theme_grey(base_size=base_size, base_family=base_family) %+replace% theme( # specify axis options axis.line=element_blank(), # all text colors used to be &quot;grey55&quot; axis.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9, vjust=1), axis.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9,hjust=1), axis.ticks=element_line(color=&quot;grey55&quot;, size = 0.2), axis.title.x=element_text(size=base_size, color=&quot;grey85&quot;, vjust=1, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;)), axis.title.y=element_text(size=base_size, color=&quot;grey85&quot;, angle=90, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;), vjust=0.5), axis.ticks.length=grid::unit(0.3, &quot;lines&quot;), # specify legend options legend.background=element_rect(color=NA, fill=&quot;black&quot;), legend.key=element_rect(color=&quot;grey55&quot;, fill=&quot;black&quot;), legend.key.size=grid::unit(1.2, &quot;lines&quot;), legend.key.height=NULL, legend.key.width=NULL, legend.text=element_text(size=base_size*0.8, color=&quot;grey85&quot;), legend.title=element_text(size=base_size*0.8, face=&quot;bold&quot;,hjust=0, color=&quot;grey85&quot;), # legend.position=&quot;right&quot;, legend.position = &quot;none&quot;, legend.text.align=NULL, legend.title.align=NULL, legend.direction=&quot;vertical&quot;, legend.box=NULL, # specify panel options panel.background=element_rect(fill=&quot;black&quot;, color = NA), panel.border=element_rect(fill=NA, color=&quot;grey55&quot;), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.spacing=grid::unit(0.25,&quot;lines&quot;), # specify facetting options strip.background=element_rect(fill = &quot;black&quot;, color=&quot;grey10&quot;), # fill=&quot;grey30&quot; strip.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;), strip.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, angle=-90), # specify plot options plot.background=element_rect(color=&quot;black&quot;, fill=&quot;black&quot;), plot.title=element_text(size=base_size*1.2, color=&quot;grey85&quot;, hjust = 0), # added hjust = 0 plot.subtitle=element_text(size=base_size*.9, color=&quot;grey85&quot;, hjust = 0), # added line # plot.margin=grid::unit(c(1, 1, 0.5, 0.5), &quot;lines&quot;) plot.margin=grid::unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;) ) } One way to use our theme_black() is to make it part of the code for an individual plot, such as ggplot() + geom_point() + theme_back(). Another way is to make theme_black() the default setting with ggplot2::theme_set(). That’s the method we’ll use. theme_set(theme_black()) # to reset the default ggplot2 theme to its default parameters, # execute `theme_set(theme_default())` In the brms reference manual, Bürkner recommended complimenting theme_black() with color scheme “C” from the viridis package, which provides a variety of colorblind-safe color palettes. # install.packages(&quot;viridis&quot;) library(viridis) The viridis_pal() function gives a list of colors within a given palette. The colors in each palette fall on a spectrum. Within viridis_pal(), the option argument allows one to select a given spectrum, “C”, in our case. The final parentheses, (), allows one to determine how many discrete colors one would like to break the spectrum up by. We’ll choose 7. viridis_pal(option = &quot;C&quot;)(7) ## [1] &quot;#0D0887FF&quot; &quot;#5D01A6FF&quot; &quot;#9C179EFF&quot; &quot;#CC4678FF&quot; &quot;#ED7953FF&quot; &quot;#FDB32FFF&quot; &quot;#F0F921FF&quot; With a little data wrangling, we can put the colors of our palette in a tibble and display them in a plot. tibble(number = 1:7, color_number = str_c(1:7, &quot;. &quot;, viridis_pal(option = &quot;C&quot;)(7))) %&gt;% ggplot(aes(x = factor(0), y = reorder(color_number, number))) + geom_tile(aes(fill = factor(number))) + geom_text(aes(color = factor(number), label = color_number)) + scale_color_manual(values = c(rep(&quot;black&quot;, times = 4), rep(&quot;white&quot;, times = 3))) + scale_fill_viridis(option = &quot;C&quot;, discrete = T, direction = -1) + scale_x_discrete(NULL, breaks = NULL) + scale_y_discrete(NULL, breaks = NULL) + ggtitle(&quot;Behold: viridis C!&quot;) Now, let’s make use of our custom theme and reproduce/reimagine Figure 14.1.a. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] d %&gt;% ggplot(aes(x = MedianAgeMarriage, y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate&quot;) Notice how viridis_pal(option = &quot;C&quot;)(7)[7] called the seventh color in the color scheme, &quot;#F0F921FF&quot;. For Figure 14.1.b, we’ll select the sixth color in the palette by coding viridis_pal(option = &quot;C&quot;)(7)[6]. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6] d %&gt;% ggplot(aes(x = log(Population), y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;log population&quot;, y = &quot;Divorce rate&quot;) Just like in the text, our plot shows states with larger populations tend to have smaller measurement error. 14.1.1 Error on the outcome. To get a better sense of what we’re about to do, imagine for a moment that each state’s divorce rate is normally distributed with a mean of Divorce and standard deviation Divorce.SE. Those distributions would be: d %&gt;% mutate(Divorce_distribution = str_c(&quot;Divorce ~ Normal(&quot;, Divorce, &quot;, &quot;, Divorce.SE, &quot;)&quot;)) %&gt;% select(Loc, Divorce_distribution) %&gt;% head() ## Loc Divorce_distribution ## 1 AL Divorce ~ Normal(12.7, 0.79) ## 2 AK Divorce ~ Normal(12.5, 2.05) ## 3 AZ Divorce ~ Normal(10.8, 0.74) ## 4 AR Divorce ~ Normal(13.5, 1.22) ## 5 CA Divorce ~ Normal(8, 0.24) ## 6 CO Divorce ~ Normal(11.6, 0.94) As in the text, in [the following] example we’ll use a Gaussian distribution with mean equal to the observed value and standard deviation equal to the measurement’s standard error. This is the logical choice, because if all we know about the error is its standard deviation, then the maximum entropy distribution for it will be Gaussian… Here’s how to define the distribution for each divorce rate. For each observed value \\(D_{\\text{OBS}i}\\), there will be one parameter, \\(D_{\\text{EST}i}\\), defined by: \\[D_{\\text{OBS}i} \\sim \\text{Normal} (D_{\\text{EST}i}, D_{\\text{SE}i})\\] All this does is define the measurement \\(D_{\\text{OBS}i}\\) as having the specified Gaussian distribution centered on the unknown parameter \\(D_{\\text{EST}i}\\). So the above defines a probability for each State \\(i\\)’s observed divorce rate, given a known measurement error. (pp. 426–427) Now we’re ready to fit some models. In brms, there are at least two ways to accommodate measurement error in the criterion. The first way uses the se() syntax, following the form &lt;response&gt; | se(&lt;se_response&gt;, sigma = TRUE). With this syntax, se stands for standard error, the loose frequentist analogue to the Bayesian posterior \\(SD\\). Unless you’re fitting a meta-analysis on summary information, which we’ll be doing at the end of this chapter, make sure to specify sigma = TRUE. Without that you’ll have no estimate for \\(\\sigma\\)! For more information on the se() method, go to the brms reference manual and find the Additional response information subsection of the brmsformula section. The second way uses the mi() syntax, following the form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the newer missing data capabilities for brms. We will cover that in more detail in the second half of this chapter. We’ll start off useing both methods. Our first model, b14.1_se, will follow the se() syntax; the second model, b14.1_mi, will follow the mi() syntax. # put the data into a `list()` dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, R = d$Marriage, A = d$MedianAgeMarriage) # here we specify the initial (i.e., starting) values inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # fit the models b14.1_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), inits = inits_list) b14.1_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, # note this line for the `mi()` model inits = inits_list) Before we dive into the model summaries, notice how the starting values (i.e., inits) differ by model. Even though we coded inits = inits_list for both models, the differ by fit@inits. b14.1_se$fit@inits ## [[1]] ## [[1]]$b ## [1] 0.6133048 -1.9171497 1.7551789 ## ## [[1]]$sigma ## [1] 0.4668127 ## ## ## [[2]] ## [[2]]$b ## [1] 0.9114156 1.2512265 -0.4276127 ## ## [[2]]$sigma ## [1] 1.906943 b14.1_mi$fit@inits ## [[1]] ## [[1]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[1]]$b ## [1] -0.5034648 1.1693530 -1.0539336 ## ## [[1]]$sigma ## [1] 1.281562 ## ## ## [[2]] ## [[2]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[2]]$b ## [1] -0.1543955 1.1642108 -0.4231833 ## ## [[2]]$sigma ## [1] 4.802142 As we explore further, it should become apparent why. Here are the primary model summaries. print(b14.1_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 21.11 6.43 8.16 33.34 1824 1.00 ## R 0.13 0.08 -0.02 0.28 2210 1.00 ## A -0.54 0.21 -0.94 -0.13 1924 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.13 0.20 0.76 1.55 3110 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.1_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 21.26 6.51 8.15 33.63 4666 1.00 ## R 0.13 0.08 -0.02 0.28 5463 1.00 ## A -0.55 0.21 -0.95 -0.12 4694 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.13 0.21 0.76 1.57 3045 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Based on the print()/summary() information, the main parameters for the models are about the same. However, the plot deepens when we summarize the models with the broom::tidy() method. library(broom) tidy(b14.1_se) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_intercept 21.11 6.43 10.27 31.41 ## 2 b_R 0.13 0.08 0.00 0.25 ## 3 b_A -0.54 0.21 -0.88 -0.19 ## 4 sigma 1.13 0.20 0.81 1.48 ## 5 lp__ -105.35 1.43 -108.15 -103.67 tidy(b14.1_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_intercept 21.26 6.51 10.14 31.90 ## 2 b_R 0.13 0.08 0.00 0.25 ## 3 b_A -0.55 0.21 -0.89 -0.19 ## 4 sigma 1.13 0.21 0.81 1.49 ## 5 Yl[1] 11.78 0.69 10.68 12.91 ## 6 Yl[2] 11.19 1.05 9.48 12.93 ## 7 Yl[3] 10.47 0.63 9.45 11.50 ## 8 Yl[4] 12.32 0.87 10.91 13.79 ## 9 Yl[5] 8.05 0.23 7.66 8.43 ## 10 Yl[6] 11.02 0.74 9.82 12.25 ## 11 Yl[7] 7.23 0.64 6.18 8.26 ## 12 Yl[8] 9.36 0.91 7.85 10.87 ## 13 Yl[9] 7.04 1.08 5.26 8.85 ## 14 Yl[10] 8.55 0.31 8.02 9.06 ## 15 Yl[11] 11.15 0.53 10.27 12.04 ## 16 Yl[12] 9.10 0.91 7.59 10.60 ## 17 Yl[13] 9.68 0.92 8.12 11.13 ## 18 Yl[14] 8.11 0.40 7.45 8.77 ## 19 Yl[15] 10.69 0.55 9.79 11.60 ## 20 Yl[16] 10.16 0.70 9.03 11.31 ## 21 Yl[17] 10.51 0.81 9.20 11.82 ## 22 Yl[18] 11.94 0.63 10.92 12.98 ## 23 Yl[19] 10.50 0.71 9.35 11.67 ## 24 Yl[20] 10.17 1.02 8.56 11.87 ## 25 Yl[21] 8.76 0.58 7.81 9.71 ## 26 Yl[22] 7.78 0.46 7.02 8.53 ## 27 Yl[23] 9.15 0.48 8.36 9.95 ## 28 Yl[24] 7.73 0.53 6.86 8.61 ## 29 Yl[25] 10.43 0.76 9.22 11.68 ## 30 Yl[26] 9.54 0.58 8.58 10.50 ## 31 Yl[27] 9.42 1.00 7.73 11.06 ## 32 Yl[28] 9.26 0.73 8.04 10.45 ## 33 Yl[29] 9.18 0.95 7.66 10.78 ## 34 Yl[30] 6.38 0.43 5.66 7.08 ## 35 Yl[31] 9.97 0.78 8.70 11.26 ## 36 Yl[32] 6.69 0.31 6.19 7.20 ## 37 Yl[33] 9.89 0.44 9.16 10.62 ## 38 Yl[34] 9.76 0.97 8.10 11.31 ## 39 Yl[35] 9.43 0.41 8.76 10.11 ## 40 Yl[36] 11.96 0.77 10.72 13.23 ## 41 Yl[37] 10.07 0.65 9.02 11.13 ## 42 Yl[38] 7.79 0.40 7.14 8.45 ## 43 Yl[39] 8.22 1.00 6.62 9.88 ## 44 Yl[40] 8.40 0.60 7.43 9.37 ## 45 Yl[41] 10.01 1.03 8.32 11.70 ## 46 Yl[42] 10.94 0.64 9.91 11.99 ## 47 Yl[43] 10.02 0.33 9.48 10.56 ## 48 Yl[44] 11.08 0.80 9.74 12.37 ## 49 Yl[45] 8.90 0.96 7.33 10.49 ## 50 Yl[46] 9.00 0.47 8.22 9.77 ## 51 Yl[47] 9.96 0.57 9.03 10.88 ## 52 Yl[48] 10.62 0.86 9.21 12.04 ## 53 Yl[49] 8.47 0.51 7.62 9.29 ## 54 Yl[50] 11.50 1.11 9.63 13.27 ## 55 lp__ -152.42 6.46 -163.31 -142.14 # you can get similar output with `b14.1_mi$fit` Again, from b_intercept to sigma, the output is about the same. But model b14.1_mi, based on the mi() syntax, contained posterior summaries for all 50 of the criterion values. The se() method gave us similar model result, but no posterior summaries for the 50 criterion values. The rethinking package indexed those additional 50 as div_est[i]; with the mi() method, brms indexed them as Yl[i]–no big deal. So while both brms methods accommodated measurement error, the mi() method appears to be the brms analogue to what McElreath did with his model m14.1 in the text. Thus, it’s our b14.1_mi model that follows the form \\[\\begin{align*} \\text{Divorce}_{\\text{estimated}, i} &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu &amp; = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\text R_i \\\\ \\text{Divorce}_{\\text{observed}, i} &amp; \\sim \\text{Normal} (\\text{Divorce}_{\\text{estimated}, i}, \\text{Divorce}_{\\text{standard error}, i}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 2.5) \\end{align*}\\] Note. The normal(0, 10) prior McElreath used was quite informative and can lead to discrepancies between the rethinking and brms results if you’re not careful. A large issue is the default way brms handles intercept priors. From the hyperlink, Bürkner wrote: The formula for the original intercept is b_intercept = temp_intercept - dot_product(means_X, b), where means_X is the vector of means of the predictor variables and b is the vector of regression coefficients (fixed effects). That is, when transforming a prior on the intercept to an “equivalent” prior on the temporary intercept, you have to take the means of the predictors and well as the priors on the other coefficients into account. If this seems confusing, you have an alternative. The 0 + intercept part of the brm formula kept the intercept in the metric of the untransformed data, leading to similar results to those from rethinking. When your priors are vague, this might not be much of an issue. And since many of the models in Statistical Rethinking use only weakly-regularizing priors, this hasn’t been much of an issue up to this point. But this model is quite sensitive to the intercept syntax. My general recommendation for applied data analysis is this: If your predictors aren’t mean centered, default to the 0 + intercept syntax for the formula argument when using brms::brm(). Otherwise, your priors might not be doing what you think they’re doing. Anyway, since our mi()-syntax b14.1_mi model appears to be the analogue to McElreath’s m14.1, we’ll use that one for our plots. Here’s our Figure 14.2.a. data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5] data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) Before we make Figure 14.2.b, we need to fit a model that ignores measurement error. b14.1b &lt;- brm(data = dlist, family = gaussian, div_obs ~ 0 + intercept + R + A, prior = c(prior(normal(0, 50), class = b, coef = intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), chains = 2, iter = 5000, warmup = 1000, cores = 2, seed = 14, control = list(adapt_delta = 0.95)) print(b14.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 35.71 7.91 20.52 51.28 2194 1.00 ## R -0.05 0.08 -0.21 0.12 2492 1.00 ## A -0.96 0.25 -1.46 -0.47 2291 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.51 0.16 1.25 1.88 3000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With the ignore-measurement-error fit in hand, we’re ready for Figure 14.2.b. nd &lt;- tibble(R = mean(d$Marriage), A = seq(from = 22, to = 30.2, length.out = 30), div_sd = mean(d$Divorce.SE)) # red line f_error &lt;- fitted(b14.1_mi, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # yellow line f_no_error &lt;- fitted(b14.1b, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # white dots data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(b14.1_mi$data) color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_r &lt;- viridis_pal(option = &quot;C&quot;)(7)[4] # plot f_no_error %&gt;% ggplot(aes(x = A, y = Estimate)) + # `f_no_error` geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = color_y, color = color_y, alpha = 1/4, size = 1/2, linetype = 2) + # `f_error` geom_smooth(data = f_error, aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = color_r, color = color_r, alpha = 1/3, size = 1/2, linetype = 1) + geom_pointrange(data = data_error, aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), color = &quot;white&quot;, shape = 20, alpha = 1/2) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(xlim = range(data_error$A), ylim = c(4, 15)) In our plot, it’s the reddish regression line that accounts for measurement error. 14.1.2 Error on both outcome and predictor. In brms, you can specify error on predictors with an me() statement in the form of me(predictor, sd_predictor) where sd_predictor is a vector in the data denoting the size of the measurement error, presumed to be in a standard-deviation metric. # the data dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, mar_obs = d$Marriage, mar_sd = d$Marriage.SE, A = d$MedianAgeMarriage) # the `inits` inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # the models b14.2_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, chains = 3, cores = 3, seed = 14, control = list(adapt_delta = 0.95), save_mevars = TRUE) # note the lack if `inits` b14.2_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, inits = inits_list) We already know including inits values for our Yl[i] estimates is a waste of time for our se() model. But note how we still defined our inits values as inits &lt;- list(Yl = dlist$div_obs) for the mi() model. Although it’s easy in brms to set the starting values for our Yl[i] estimates, much the way McElreath did, that isn’t the case when you have measurement error on the predictors. The brms package uses a non-centered parameterization for these, which requires users to have a deeper understanding of the underlying Stan code. This is where I get off the train, but if you want to go further, execute stancode(b14.2_mi). Here are the two versions of the model. print(b14.2_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 3 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 15.65 6.79 2.29 28.71 5555 1.00 ## A -0.44 0.20 -0.84 -0.03 6303 1.00 ## memar_obsmar_sd 0.27 0.11 0.07 0.49 5644 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.00 0.21 0.62 1.45 11895 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.2_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 15.64 6.90 1.90 29.16 1807 1.00 ## A -0.44 0.21 -0.84 -0.02 2081 1.00 ## memar_obsmar_sd 0.28 0.11 0.07 0.49 1697 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.00 0.21 0.62 1.45 1644 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll use broom::tidy(), again, to get a sense of depth=2 summaries. tidy(b14.2_se) %&gt;% mutate_if(is.numeric, round, digits = 2) tidy(b14.2_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) Due to space concerns, I’m not going to show the results, here. You can do that on your own. Both methods yielded the posteriors for Xme_memar_obs[1], but only the b14.2_mi model based on the mi() syntax yielded posteriors for the criterion, the Yl[i] summaries. Note that you’ll need to specify save_mevars = TRUE in the brm() function in order to save the posterior samples of error-adjusted variables obtained by using the me() argument. Without doing so, functions like predict() may give you trouble. Here is the code for Figure 14.3.a. data_error &lt;- fitted(b14.2_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[3] data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) To get the posterior samples for error-adjusted Marriage rate, we’ll use posterior_samples. If you examine the object with glimpse(), you’ll notice 50 Xme_memar_obsmar_sd[i] vectors, with \\(i\\) ranging from 1 to 50, each corresponding to one of the 50 states. With a little data wrangling, you can get the mean of each to put in a plot. Once we have those summaries, we can make our version of Figure 14.4.b. color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_p &lt;- viridis_pal(option = &quot;C&quot;)(7)[2] posterior_samples(b14.2_mi) %&gt;% select(starts_with(&quot;Xme&quot;)) %&gt;% gather() %&gt;% # this extracts the numerals from the otherwise cumbersome names in `key` and saves them as integers mutate(key = str_extract(key, &quot;\\\\d+&quot;) %&gt;% as.integer()) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) %&gt;% bind_cols(data_error) %&gt;% ggplot(aes(x = mean, y = Estimate)) + geom_segment(aes(xend = Marriage, yend = Divorce), color = &quot;white&quot;, size = 1/4) + geom_point(size = 2, alpha = 2/3, color = color_y) + geom_point(aes(x = Marriage, y = Divorce), size = 2, alpha = 2/3, color = color_p) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Marriage rate (posterior)&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(ylim = c(4, 14.5)) The yellow points are model-implied; the purple ones are of the original data. It turns out our brms model regularized more aggressively than McElreath’s rethinking model. I’m unsure of why. If you understand the difference, please share with the rest of the class. Anyway, the big take home point for this section is that when you have a distribution of values, don’t reduce it down to a single value to use in a regression. Instead, use the entire distribution. Anytime we use an average value, discarding the uncertainty around that average, we risk overconfidence and spurious inference. This doesn’t only apply to measurement error, but also to cases which data are averaged before analysis. Do not average. Instead, model. (p. 431) 14.2 Missing data Starting with version 2.2.0 brms now supports Bayesian missing data imputation using adaptations of the multivariate syntax. Bürkner’s Handle Missing Values with brms vignette is quite helpful. 14.2.1 Imputing neocortex Once again, here are the milk data. library(rethinking) data(milk) d &lt;- milk d &lt;- d %&gt;% mutate(neocortex.prop = neocortex.perc / 100, logmass = log(mass)) Now we’ll switch out rethinking for brms and do a little data wrangling. detach(package:rethinking, unload = T) library(brms) rm(milk) # prep data data_list &lt;- list( kcal = d$kcal.per.g, neocortex = d$neocortex.prop, logmass = d$logmass) Here’s the structure of our data list. data_list ## $kcal ## [1] 0.49 0.51 0.46 0.48 0.60 0.47 0.56 0.89 0.91 0.92 0.80 0.46 0.71 0.71 0.73 0.68 0.72 0.97 0.79 ## [20] 0.84 0.48 0.62 0.51 0.54 0.49 0.53 0.48 0.55 0.71 ## ## $neocortex ## [1] 0.5516 NA NA NA NA 0.6454 0.6454 0.6764 NA 0.6885 0.5885 0.6169 0.6032 ## [14] NA NA 0.6997 NA 0.7041 NA 0.7340 NA 0.6753 NA 0.7126 0.7260 NA ## [27] 0.7024 0.7630 0.7549 ## ## $logmass ## [1] 0.6678294 0.7371641 0.9202828 0.4824261 0.7839015 1.6582281 1.6808279 0.9202828 ## [9] -0.3424903 -0.3856625 -2.1202635 -0.7550226 -1.1394343 -0.5108256 1.2441546 0.4382549 ## [17] 1.9572739 1.1755733 2.0719133 2.5095993 2.0268316 1.6808279 2.3721112 3.5689692 ## [25] 4.3748761 4.5821062 3.7072104 3.4998354 4.0064237 Our statistical model follows the form \\[\\begin{align*} \\text{kcal}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex}_i + \\beta_2 \\text{logmass}_i \\\\ \\text{neocortex}_i &amp; \\sim \\text{Normal} (\\nu, \\sigma_\\text{neocortex}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\nu &amp; \\sim \\text{Normal} (0.5, 1) \\\\ \\sigma_\\text{neocortex} &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] When writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things get cluttered in a hurry. b_model &lt;- # here&#39;s the primary `kcal` model bf(kcal ~ 1 + mi(neocortex) + logmass) + # here&#39;s the model for the missing `neocortex` data bf(neocortex | mi() ~ 1) + # here we set the residual correlations for the two models to zero set_rescor(FALSE) Note the mi(neocortex) syntax in the kcal model. This indicates that the predictor, neocortex, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model, use the get_prior() function. get_prior(data = data_list, family = gaussian, b_model) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 Intercept ## 3 b kcal ## 4 b logmass kcal ## 5 b mineocortex kcal ## 6 student_t(3, 1, 10) Intercept kcal ## 7 student_t(3, 0, 10) sigma kcal ## 8 student_t(3, 1, 10) Intercept neocortex ## 9 student_t(3, 0, 10) sigma neocortex With the one-step Bayesian imputation procedure in brms, you might need to use the resp argument when specifying non-defaut priors. Anyway, here we fit the model. b14.3 &lt;- brm(data = data_list, family = gaussian, b_model, # here we insert the model prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2, seed = 14) The imputed neocortex values are indexed by occasion number from the original data. tidy(b14.3) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.52 0.47 -1.29 0.26 ## 2 b_neocortex_Intercept 0.67 0.01 0.65 0.69 ## 3 b_kcal_logmass -0.07 0.02 -0.10 -0.03 ## 4 bsp_kcal_mineocortex 1.89 0.74 0.66 3.07 ## 5 sigma_kcal 0.13 0.02 0.10 0.17 ## 6 sigma_neocortex 0.06 0.01 0.05 0.08 ## 7 Ymi_neocortex[2] 0.63 0.05 0.55 0.72 ## 8 Ymi_neocortex[3] 0.63 0.05 0.54 0.71 ## 9 Ymi_neocortex[4] 0.62 0.05 0.54 0.71 ## 10 Ymi_neocortex[5] 0.65 0.05 0.57 0.73 ## 11 Ymi_neocortex[9] 0.70 0.05 0.62 0.78 ## 12 Ymi_neocortex[14] 0.66 0.05 0.58 0.74 ## 13 Ymi_neocortex[15] 0.69 0.05 0.61 0.77 ## 14 Ymi_neocortex[17] 0.70 0.05 0.62 0.77 ## 15 Ymi_neocortex[19] 0.71 0.05 0.63 0.79 ## 16 Ymi_neocortex[21] 0.65 0.05 0.57 0.73 ## 17 Ymi_neocortex[23] 0.66 0.05 0.58 0.74 ## 18 Ymi_neocortex[26] 0.69 0.05 0.61 0.77 ## 19 lp__ 40.47 4.32 32.63 46.77 Here’s the model that drops the cases with NAs on neocortex. b14.3cc &lt;- brm(data = data_list, family = gaussian, kcal ~ 1 + neocortex + logmass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 1e4, chains = 2, cores = 2, seed = 14) The parameters: tidy(b14.3cc) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.08 0.58 -2.01 -0.15 ## 2 b_neocortex 2.78 0.90 1.36 4.22 ## 3 b_logmass -0.10 0.03 -0.14 -0.05 ## 4 sigma 0.14 0.03 0.10 0.19 ## 5 lp__ -4.25 1.69 -7.47 -2.35 In order to make our versions of Figure 14.4, we’ll need to do a little data wrangling with fitted(). nd &lt;- tibble(neocortex = seq(from = .5, to = .85, length.out = 30), logmass = median(data_list$logmass)) f_b14.3 &lt;- fitted(b14.3, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.3 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.3323251, 0.3550894, 0.3778536, 0.4006179, 0.4233821, 0.4461464, 0.4… ## $ Est.Error.kcal &lt;dbl&gt; 0.12608956, 0.11743842, 0.10882781, 0.10026818, 0.09177382, 0.0833646… ## $ Q2.5.kcal &lt;dbl&gt; 0.0876803, 0.1261868, 0.1661890, 0.2061219, 0.2451970, 0.2841644, 0.3… ## $ Q97.5.kcal &lt;dbl&gt; 0.5891273, 0.5949955, 0.5997681, 0.6044829, 0.6104041, 0.6146529, 0.6… ## $ Estimate.neocortex &lt;dbl&gt; 0.671415, 0.671415, 0.671415, 0.671415, 0.671415, 0.671415, 0.671415,… ## $ Est.Error.neocortex &lt;dbl&gt; 0.01357673, 0.01357673, 0.01357673, 0.01357673, 0.01357673, 0.0135767… ## $ Q2.5.neocortex &lt;dbl&gt; 0.6437403, 0.6437403, 0.6437403, 0.6437403, 0.6437403, 0.6437403, 0.6… ## $ Q97.5.neocortex &lt;dbl&gt; 0.697363, 0.697363, 0.697363, 0.697363, 0.697363, 0.697363, 0.697363,… ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0.5… ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155,… To include the imputed neocortex values in the plot, we’ll extract the information from broom::tidy(). f_b14.3_mi &lt;- tidy(b14.3) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols(data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex)) ) f_b14.3_mi %&gt;% head() ## term estimate std.error lower upper kcal neocortex logmass ## 1 Ymi_neocortex[2] 0.6328085 0.04996795 0.5544665 0.7171928 0.51 NA 0.7371641 ## 2 Ymi_neocortex[3] 0.6253038 0.05076178 0.5442210 0.7099261 0.46 NA 0.9202828 ## 3 Ymi_neocortex[4] 0.6224919 0.05192594 0.5397859 0.7092606 0.48 NA 0.4824261 ## 4 Ymi_neocortex[5] 0.6531597 0.04942642 0.5742102 0.7347481 0.60 NA 0.7839015 ## 5 Ymi_neocortex[9] 0.7008680 0.04973948 0.6208441 0.7817752 0.91 NA -0.3424903 ## 6 Ymi_neocortex[14] 0.6570051 0.04870813 0.5794949 0.7397656 0.71 NA -0.5108256 Data wrangling done–here’s our code for Figure 14.4.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] f_b14.3 %&gt;% ggplot(aes(x = neocortex)) + geom_smooth(aes(y = Estimate.kcal, ymin = Q2.5.kcal, ymax = Q97.5.kcal), stat = &quot;identity&quot;, fill = color, color = color, alpha = 1/3, size = 1/2) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.3_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.3_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) + labs(subtitle = &quot;Note: For the regression line in this plot, log(mass)\\nhas been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) Figure 14.4.b. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.3_mi, aes(y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(&quot;log(mass)&quot;, breaks = -2:4) + ylab(&quot;neocortex proportion&quot;) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) 14.2.2 Improving the imputation model Like McElreath, we’ll update the imputation line of our statistical model to: \\[\\begin{align*} \\text{neocortex}_i &amp; \\sim \\text{Normal} (\\nu_i, \\sigma_\\text{neocortex}) \\\\ \\nu_i &amp; = \\alpha_\\text{neocortex} + \\gamma_1 \\text{logmass}_i \\\\ \\end{align*}\\] which includes the updated priors \\[\\begin{align*} \\alpha_\\text{neocortex} &amp; \\sim \\text{Normal} (0.5, 1) \\\\ \\gamma_1 &amp; \\sim \\text{Normal} (0, 10) \\end{align*}\\] As far as the brms code goes, adding logmass as a predictor to the neocortex submodel is pretty simple. # define the model b_model &lt;- bf(kcal ~ 1 + mi(neocortex) + logmass) + bf(neocortex | mi() ~ 1 + logmass) + # here&#39;s the big difference set_rescor(FALSE) # fit the model b14.4 &lt;- brm(data = data_list, family = gaussian, b_model, prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2, seed = 14) Behold the parameter estimates. tidy(b14.4) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.87 0.48 -1.62 -0.05 ## 2 b_neocortex_Intercept 0.64 0.01 0.62 0.66 ## 3 b_kcal_logmass -0.09 0.02 -0.12 -0.05 ## 4 b_neocortex_logmass 0.02 0.01 0.01 0.03 ## 5 bsp_kcal_mineocortex 2.44 0.75 1.16 3.62 ## 6 sigma_kcal 0.13 0.02 0.10 0.17 ## 7 sigma_neocortex 0.04 0.01 0.03 0.06 ## 8 Ymi_neocortex[2] 0.63 0.03 0.57 0.69 ## 9 Ymi_neocortex[3] 0.63 0.04 0.57 0.69 ## 10 Ymi_neocortex[4] 0.62 0.04 0.56 0.68 ## 11 Ymi_neocortex[5] 0.65 0.03 0.59 0.70 ## 12 Ymi_neocortex[9] 0.66 0.04 0.60 0.72 ## 13 Ymi_neocortex[14] 0.63 0.03 0.57 0.68 ## 14 Ymi_neocortex[15] 0.68 0.03 0.62 0.74 ## 15 Ymi_neocortex[17] 0.70 0.03 0.64 0.75 ## 16 Ymi_neocortex[19] 0.71 0.04 0.66 0.77 ## 17 Ymi_neocortex[21] 0.66 0.03 0.61 0.72 ## 18 Ymi_neocortex[23] 0.68 0.03 0.62 0.73 ## 19 Ymi_neocortex[26] 0.74 0.04 0.68 0.80 ## 20 lp__ 48.73 4.14 41.22 54.69 Here’s our pre-Figure 14.5 data wrangling. f_b14.4 &lt;- fitted(b14.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.4_mi &lt;- tidy(b14.4) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols(data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex)) ) f_b14.4 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.2409327, 0.2703352, 0.2997377, 0.3291402, 0.3585428, 0.3879453, 0.4… ## $ Est.Error.kcal &lt;dbl&gt; 0.12719373, 0.11839639, 0.10963475, 0.10091810, 0.09225919, 0.0836759… ## $ Q2.5.kcal &lt;dbl&gt; -0.008257873, 0.038346076, 0.084220864, 0.130458783, 0.176254857, 0.2… ## $ Q97.5.kcal &lt;dbl&gt; 0.5053176, 0.5156579, 0.5259997, 0.5364897, 0.5472565, 0.5586155, 0.5… ## $ Estimate.neocortex &lt;dbl&gt; 0.6671826, 0.6671826, 0.6671826, 0.6671826, 0.6671826, 0.6671826, 0.6… ## $ Est.Error.neocortex &lt;dbl&gt; 0.009527667, 0.009527667, 0.009527667, 0.009527667, 0.009527667, 0.00… ## $ Q2.5.neocortex &lt;dbl&gt; 0.6483752, 0.6483752, 0.6483752, 0.6483752, 0.6483752, 0.6483752, 0.6… ## $ Q97.5.neocortex &lt;dbl&gt; 0.6857586, 0.6857586, 0.6857586, 0.6857586, 0.6857586, 0.6857586, 0.6… ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0.5… ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155,… f_b14.4_mi %&gt;% glimpse() ## Observations: 12 ## Variables: 8 ## $ term &lt;chr&gt; &quot;Ymi_neocortex[2]&quot;, &quot;Ymi_neocortex[3]&quot;, &quot;Ymi_neocortex[4]&quot;, &quot;Ymi_neocortex[5]&quot;,… ## $ estimate &lt;dbl&gt; 0.6312932, 0.6282494, 0.6198307, 0.6467207, 0.6640903, 0.6275220, 0.6797096, 0.… ## $ std.error &lt;dbl&gt; 0.03485311, 0.03584387, 0.03565912, 0.03394558, 0.03617280, 0.03462905, 0.03400… ## $ lower &lt;dbl&gt; 0.5737403, 0.5699527, 0.5618990, 0.5913583, 0.6046591, 0.5713660, 0.6249627, 0.… ## $ upper &lt;dbl&gt; 0.6883740, 0.6859330, 0.6778127, 0.7021121, 0.7234962, 0.6849341, 0.7354359, 0.… ## $ kcal &lt;dbl&gt; 0.51, 0.46, 0.48, 0.60, 0.91, 0.71, 0.73, 0.72, 0.79, 0.48, 0.51, 0.53 ## $ neocortex &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ## $ logmass &lt;dbl&gt; 0.7371641, 0.9202828, 0.4824261, 0.7839015, -0.3424903, -0.5108256, 1.2441546, … For our final plots, let’s play around with colors from viridis_pal(option = &quot;D&quot;). Figure 14.5.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] f_b14.4 %&gt;% ggplot(aes(x = neocortex)) + geom_smooth(aes(y = Estimate.kcal, ymin = Q2.5.kcal, ymax = Q97.5.kcal), stat = &quot;identity&quot;, fill = color, color = color, alpha = 1/2, size = 1/2) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.4_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.4_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) + labs(subtitle = &quot;Note: For the regression line in this plot, log(mass)\\nhas been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) Figure 14.5.b. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.4_mi, aes(y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(&quot;log(mass)&quot;, breaks = -2:4) + ylab(&quot;neocortex proportion&quot;) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) If modern missing data methods are new to you, you might also check out van Burren’s great online text Flexible Imputation of Missing Data. Second Edition. I’m also a fan of Enders’ Applied Missing Data Analysis, for which you can find a free sample chapter here. I’ll also quickly mention that brms accommodates multiple imputation, too. 14.3 Summary Bonus: Meta-analysis If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too. Before we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Meta-analysis is a special case of Bayesian multilevel modeling. And since McElreath’s text doesn’t directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s Bayesian data analysis, Third edition. We’ll let Gelman and colleagues introduce the topic: Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another.… this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis… The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125—126, emphasis in the original) The basic version of a Bayesian meta-analysis follows the form \\[y_i \\sim \\text{Normal}(\\theta_i, \\sigma_i)\\] where \\(y_i\\) = the point estimate for the effect size of a single study, \\(i\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_i\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study \\(i\\) is specified \\(\\sigma_i\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_i\\), here. Those values we take directly from the original studies. Building on the model, we further presume that study \\(i\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume \\(\\theta_i\\) itself has a distribution following the form \\[\\theta_i \\sim \\text{Normal} (\\mu, \\tau)\\] where \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\). Since there’s no example of a meta-analysis in the text, we’ll have to get our data elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, Spanking and Child Outcomes: Old Controversies and New Meta-Analyses. From their introduction, we read: Around the world, most children (80%) are spanked or otherwise physically punished by their parents (UNICEF, 2014). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (Gershoff, 2013). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453) Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called spank.xlsx. You can find the data in this project’s GitHub repository. Let’s load them and glimpse(). spank &lt;- readxl::read_excel(&quot;spank.xlsx&quot;) glimpse(spank) ## Observations: 111 ## Variables: 8 ## $ study &lt;chr&gt; &quot;Bean and Roberts (1981)&quot;, &quot;Day and Roberts (1983)&quot;, &quot;Minton, Kagan, and Levine (… ## $ year &lt;dbl&gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1986, 2012, 1979, 200… ## $ outcome &lt;chr&gt; &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defi… ## $ between &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ within &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, … ## $ d &lt;dbl&gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18, 1.18, 0.70, 0.63, … ## $ ll &lt;dbl&gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -0.49, 0.15, 0.35, -… ## $ ul &lt;dbl&gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.22, 1.05, 1.71, 0.2… In this paper, the effect size of interest is a Cohen’s d, derived from the formula \\[d = \\frac{\\mu_\\text{treatment} - \\mu_\\text{comparison}}{\\sigma_\\text{pooled}}\\] where \\[\\sigma_\\text{pooled} = \\sqrt{\\frac{((n_1 - 1) \\sigma_1^2) + ((n_2 - 1) \\sigma_2^2)}{n_1 + n_2 -2}}\\] To help make the equation for \\(d\\) clearer for our example, we might re-express it as \\[d = \\frac{\\mu_\\text{spanked} - \\mu_\\text{not spanked}}{\\sigma_\\text{pooled}}\\] McElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s On effect size. But in words, Cohen’s d is a standardized mean difference between two groups. So if you look back up at the results of glimpse(spank) you’ll notice the column d, which is indeed a vector of Cohen’s d effect sizes. The last two columns, ll and ul, are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our d-values; we want their standard errors. Fortunately, we can compute those with the following formula \\[SE = \\frac{\\text{upper limit} - \\text{lower limit}}{3.92}\\] Here it is in code. spank &lt;- spank %&gt;% mutate(se = (ul - ll) / 3.92) glimpse(spank) ## Observations: 111 ## Variables: 9 ## $ study &lt;chr&gt; &quot;Bean and Roberts (1981)&quot;, &quot;Day and Roberts (1983)&quot;, &quot;Minton, Kagan, and Levine (… ## $ year &lt;dbl&gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1986, 2012, 1979, 200… ## $ outcome &lt;chr&gt; &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defi… ## $ between &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ within &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, … ## $ d &lt;dbl&gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18, 1.18, 0.70, 0.63, … ## $ ll &lt;dbl&gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -0.49, 0.15, 0.35, -… ## $ ul &lt;dbl&gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.22, 1.05, 1.71, 0.2… ## $ se &lt;dbl&gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.47193878, 0.23979592, 0.1709183… Now are data are ready, we can express our first Bayesian meta-analysis with the formula \\[\\begin{align*} \\text{d}_i &amp; \\sim \\text{Normal}(\\theta_i, \\sigma_i = \\text{se}_i) \\\\ \\theta_i &amp; \\sim \\text{Normal} (\\mu, \\tau) \\\\ \\mu &amp; \\sim \\text{Normal} (0, 1) \\\\ \\tau &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] The last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see Cohen’s d-values greater than the absolute value of \\(\\pm 1\\). So in the absence of more specific domain knowledge–which I don’t have–, it seems like \\(\\text{Normal} (0, 1)\\) is a reasonable place to start. And just like McElreath used \\(\\text{HalfCauchy} (0, 1)\\) as the default prior for the group-level standard deviations, it makes sense to use it here for our meta-analytic \\(\\tau\\) parameter. Here’s the code for the first model. b14.5 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14) One thing you might notice is our se(se) function excluded the sigma argument. If you recall from section 14.1, we specified sigma = T in our measurement-error models. The brms default is that within se(), sigma = FALSE. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the d-value for each study \\(i\\) has already been encoded in the data as se. This brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publically available. So effect size summaries were the best we typically had. However, times are changing (e.g., here, here). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by \\(\\sigma\\) by taking the distributional modeling approach and specify something like sigma ~ 0 + study or even sigma ~ 1 + (1 | study). But enough technical talk. Let’s look at the model results. print(b14.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.26 0.03 0.21 0.32 2626 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.37 0.04 0.30 0.45 1496 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Thus, in our simple Bayesian meta-analysis, we have a population Cohen’s d of about 0.37. Our estimate for \\(\\tau\\), 0.26, suggests we have quite a bit of between-study variability. One question you might ask is: What exactly are these Cohen’s ds measuring, anyways? We’ve encoded that in the outcome vector of the spank data. spank %&gt;% distinct(outcome) %&gt;% knitr::kable() outcome Immediate defiance Low moral internalization Child aggression Child antisocial behavior Child externalizing behavior problems Child internalizing behavior problems Child mental health problems Child alcohol or substance abuse Negative parent–child relationship Impaired cognitive ability Low self-esteem Low self-regulation Victim of physical abuse Adult antisocial behavior Adult mental health problems Adult alcohol or substance abuse Adult support for physical punishment There are a few things to note. First, with the possible exception of Adult support for physical punishment, all of the outcomes are negative. We prefer conditions associated with lower values for things like Child aggression and Adult mental health problems. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies. spank %&gt;% distinct(study) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 76 In other words, some studies have multiple outcomes. In order to better accommodate the study- and outcome-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13. b14.6 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14) print(b14.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.08 0.03 0.04 0.14 2575 1.00 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.25 0.03 0.20 0.32 1991 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.36 0.04 0.27 0.44 1703 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we have two \\(\\tau\\) parameters. We might plot them to get a sense of where the variance is at. # we&#39;ll want this to label the plot label &lt;- tibble(tau = c(.12, .3), y = c(15, 10), label = c(&quot;sigma[&#39;outcome&#39;]&quot;, &quot;sigma[&#39;study&#39;]&quot;)) # wrangle posterior_samples(b14.6) %&gt;% select(starts_with(&quot;sd&quot;)) %&gt;% gather(key, tau) %&gt;% mutate(key = str_remove(key, &quot;sd_&quot;) %&gt;% str_remove(., &quot;__Intercept&quot;)) %&gt;% # plot ggplot(aes(x = tau)) + geom_density(aes(fill = key), color = &quot;transparent&quot;) + geom_text(data = label, aes(y = y, label = label, color = label), parse = T, size = 5) + scale_fill_viridis_d(NULL, option = &quot;B&quot;, begin = .5) + scale_color_viridis_d(NULL, option = &quot;B&quot;, begin = .5) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(tau)) + theme(panel.grid = element_blank()) So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use tidybayes::geom_halfeyeh() to help us make our version of a forest plot and tidybayes::spread_draws() to help with the initial wrangling. library(tidybayes) b14.6 %&gt;% spread_draws(b_Intercept, r_outcome[outcome,]) %&gt;% # add the grand mean to the group-specific deviations mutate(mu = b_Intercept + r_outcome) %&gt;% ungroup() %&gt;% mutate(outcome = str_replace_all(outcome, &quot;[.]&quot;, &quot; &quot;)) %&gt;% # plot ggplot(aes(x = mu, y = reorder(outcome, mu), fill = reorder(outcome, mu))) + geom_vline(xintercept = fixef(b14.6)[1, 1], color = &quot;grey33&quot;, size = 1) + geom_vline(xintercept = fixef(b14.6)[1, 3:4], color = &quot;grey33&quot;, linetype = 2) + geom_halfeyeh(.width = .95, size = 2/3, color = &quot;white&quot;) + scale_fill_viridis_d(option = &quot;B&quot;, begin = .2) + labs(x = expression(italic(&quot;Cohen&#39;s d&quot;)), y = NULL) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) The solid and dashed vertical white lines in the background mark off the grand mean (i.e., the meta-analytic effect) and its 95% intervals. But anyway, there’s not a lot of variability across the outcomes. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read: When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data \\((n, y)\\)) is available to distinguish among the \\(J\\) studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126) One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the spank data, we’ve dummy coded that information with the between and within vectors. Both are dummy variables and \\(\\text{within} = 1 - \\text{between}\\). Here are the counts. spank %&gt;% count(between) ## # A tibble: 2 x 2 ## between n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 71 ## 2 1 40 When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the between variable to the model. While we’re at it, we’ll practice using the 0 + intercept syntax. b14.7 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14) Behold the summary. print(b14.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.08 0.03 0.04 0.14 5169 1.00 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.25 0.03 0.20 0.32 3909 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 0.38 0.05 0.29 0.48 3211 1.00 ## between -0.08 0.07 -0.22 0.06 3322 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s take a closer look at b_between. color &lt;- viridis_pal(option = &quot;B&quot;)(7)[5] posterior_samples(b14.7) %&gt;% ggplot(aes(x = b_between, y = 0)) + geom_halfeyeh(.width = c(.5, .95), color = &quot;white&quot;, fill = color) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Overall difference for between- vs within-participant designs&quot;) + theme(panel.grid = element_blank()) That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know? There are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started. If you’d like to learn more about these methods, do check out Vourre’s Meta-analysis is a special case of Bayesian multilevel modeling. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the brmstools::forest() function and how our Bayesian brms method compares with Frequentist meta-analyses via the metafor package. You might also check out Williams, Rast, and Bürkner’s manuscript, Bayesian Meta-Analysis with Weakly Informative Prior Distributions to give you an empirical justification for using a half-Cauchy prior for your meta-analysis \\(\\tau\\) parameters. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 broom_0.5.2 viridis_0.5.1 viridisLite_0.3.0 brms_2.9.0 ## [6] Rcpp_1.0.1 dagitty_0.2-2 rstan_2.18.2 StanHeaders_2.18.1 forcats_0.4.0 ## [11] stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 ## [16] tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] ggstance_0.3.1 markdown_1.0 base64enc_0.1-3 ## [7] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 ## [22] shiny_1.3.2 compiler_3.6.3 httr_1.4.0 ## [25] backports_1.1.4 assertthat_0.2.1 Matrix_1.2-17 ## [28] lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [31] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.3 ## [34] igraph_1.2.4.1 coda_0.19-2 gtable_0.3.0 ## [37] glue_1.3.1 reshape2_1.4.3 V8_2.2 ## [40] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-144 ## [43] crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [46] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.5 ## [52] zoo_1.8-6 scales_1.1.1.9000 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] curl_3.3 gridExtra_2.3 loo_2.1.0 ## [64] stringi_1.4.3 highr_0.8 dygraphs_1.1.1.6 ## [67] boot_1.3-24 pkgbuild_1.0.3 shape_1.4.4 ## [70] rlang_0.4.0 pkgconfig_2.0.2 matrixStats_0.54.0 ## [73] evaluate_0.14 lattice_0.20-38 labeling_0.3 ## [76] rstantools_1.5.1 htmlwidgets_1.3 processx_3.3.1 ## [79] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [82] bookdown_0.11 R6_2.4.0 generics_0.0.2 ## [85] pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [88] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [94] rmarkdown_1.13 grid_3.6.3 readxl_1.3.1 ## [97] callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [100] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.3 ## [103] munsell_0.5.0 shinyjs_1.0 "],
["horoscopes-insights.html", "15 Horoscopes Insights 15.1 Use R Notebooks 15.2 Save your model fits 15.3 Build your models slowly 15.4 Look at your data 15.5 Use the 0 + intercept syntax 15.6 Annotate your workflow 15.7 Annotate your code 15.8 Break up your workflow 15.9 Read Gelman’s blog 15.10 Check out other social media, too 15.11 Parting wisdom Reference Session info", " 15 Horoscopes Insights Statistical inference is indeed critically important. But only as much as every other part of research. Scientific discovery is not an additive process, in which sin in one part can be atoned by virtue in another. Everything interacts. So equally when science works as intended as when it does not, every part of the process deserves attention. (p. 441) In this final chapter, there are no models for us to fit and no figures for use to reimagine. McElreath took the opportunity to comment more broadly on the scientific process. He made a handful of great points, some of which I’ll quote in a bit. But for the bulk of this chapter, I’d like to take the opportunity to pass on a few of my own insights about workflow. I hope they’re of use. 15.1 Use R Notebooks OMG I first started using R in the winter of 2015/2016. Right from the start, I learned how to code from within the R Studio environment. But within R Studio I was using simple scripts. No longer. I now use R Notebooks for just about everything, including my scientific projects, this bookdown project, and even my academic webpage and blog. Nathan Stephens wrote a nice blog on Why I love R Notebooks. I agree. This has fundamentally changed my workflow as a scientist. I only wish I’d learned about this before starting my dissertation project. So it goes… Do yourself a favor, adopt R Notebooks into your workflow. Do it today. If you prefer to learn with videos, here’s a nice intro by Kristine Yu and another one by JJ Allaire. Try it out for like one afternoon and you’ll be hooked. 15.2 Save your model fits It’s embarrassing how long it took for this to dawn on me. Unlike classical statistics, Bayesian models using MCMC take a while to compute. Most of the simple models in McElreath’s text take 30 seconds up to a couple minutes. If your data are small, well-behaved and of a simple structure, you might have a lot of wait times in that range in your future. It hasn’t been that way, for me. Most of my data have a complicated multilevel structure and often aren’t very well behaved. It’s normal for my models to take an hour or several to fit. Once you start measuring your model fit times in hours, you do not want to fit these things more than once. So, it’s not enough to document my code in a nice R Notebook file. I need to save my brm() fit objects in external files. Consider this model. It’s taken from Bürkner’s vignette, Estimating Multivariate Models with brms. It took about five minutes for my several-year-old laptop to fit. library(brms) data(&quot;BTdata&quot;, package = &quot;MCMCglmm&quot;) fit1 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex + hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15) Five minutes isn’t terribly long to wait, but still. I’d prefer to never have to wait for another five minutes, again. Sure, if I save my code in a document like this, I will always be able to fit the model again. But I can work smarter. Here I’ll save my fit1 object outside of R with the save() function. save(fit1, file = &quot;fit1.rda&quot;) Hopefully y’all are savvy Bayesian R users and find this insultingly remedial. But if it’s new to you like it was me, you can learn more about .rda files here. Now fit1 is saved outside of R, I can safely remove it and then reload it. rm(fit1) load(&quot;fit1.rda&quot;) The file took a fraction of a second to reload. Once reloaded, I can perform typical operations, like examine summaries of the model parameters or refreshing my memory on what data I used. print(fit1) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Group-Level Effects: ## ~dam (Number of levels: 106) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(tarsus_Intercept) 0.48 0.05 0.39 0.58 844 1.00 ## sd(back_Intercept) 0.24 0.07 0.10 0.39 312 1.00 ## cor(tarsus_Intercept,back_Intercept) -0.51 0.22 -0.91 -0.05 546 1.00 ## ## ~fosternest (Number of levels: 104) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(tarsus_Intercept) 0.27 0.05 0.17 0.37 615 1.00 ## sd(back_Intercept) 0.35 0.06 0.24 0.47 579 1.00 ## cor(tarsus_Intercept,back_Intercept) 0.70 0.20 0.22 0.98 308 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## tarsus_Intercept -0.41 0.07 -0.54 -0.28 1327 1.00 ## back_Intercept -0.01 0.07 -0.14 0.12 2910 1.00 ## tarsus_sexMale 0.77 0.06 0.66 0.88 4928 1.00 ## tarsus_sexUNK 0.23 0.13 -0.03 0.47 3727 1.00 ## tarsus_hatchdate -0.04 0.06 -0.15 0.07 1272 1.00 ## back_sexMale 0.01 0.07 -0.13 0.14 3483 1.00 ## back_sexUNK 0.15 0.15 -0.14 0.44 4080 1.00 ## back_hatchdate -0.09 0.05 -0.18 0.01 2682 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma_tarsus 0.76 0.02 0.72 0.80 2014 1.00 ## sigma_back 0.90 0.02 0.85 0.95 2928 1.00 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## rescor(tarsus,back) -0.05 0.04 -0.13 0.02 2893 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). library(tidyverse) fit1$data %&gt;% head() ## tarsus sex hatchdate fosternest dam back ## 1 -1.89229718 Fem -0.6874021 F2102 R187557 1.1464212 ## 2 1.13610981 Male -0.6874021 F1902 R187559 -0.7596521 ## 3 0.98468946 Male -0.4279814 A602 R187568 0.1449373 ## 4 0.37900806 Male -1.4656641 A1302 R187518 0.2555847 ## 5 -0.07525299 Fem -1.4656641 A2602 R187528 -0.3006992 ## 6 -1.13519543 Fem 0.3502805 C2302 R187945 1.5577219 As an alternative, Bürkner recently added a file argument in brms:brm() that will help you do this, too. The origins of the argument live here. By default, file is set to NULL. To make use of the argument, specify a character string. file will then save your fitted model object in an external .rds file via the saveRDS() function. Let’s give it a whirl, this time with an interaction. fit2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex*hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15, file = &quot;fit2&quot;) Now fit2 is saved outside of R, I can safely remove it and then reload it. rm(fit2) We might load fit2 with the readRDS() function. fit2 &lt;- readRDS(&quot;fit2.rds&quot;) Now we can work with fit2 as desired. fixef(fit2) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## tarsus_Intercept -0.409 0.070 -0.547 -0.268 ## back_Intercept -0.011 0.067 -0.144 0.113 ## tarsus_sexMale 0.771 0.057 0.658 0.884 ## tarsus_sexUNK 0.194 0.147 -0.104 0.485 ## tarsus_hatchdate -0.054 0.068 -0.186 0.077 ## tarsus_sexMale:hatchdate 0.013 0.058 -0.101 0.125 ## tarsus_sexUNK:hatchdate 0.058 0.122 -0.174 0.299 ## back_sexMale 0.005 0.069 -0.128 0.138 ## back_sexUNK 0.145 0.170 -0.190 0.472 ## back_hatchdate -0.052 0.061 -0.173 0.061 ## back_sexMale:hatchdate -0.079 0.069 -0.209 0.053 ## back_sexUNK:hatchdate -0.033 0.143 -0.321 0.253 The file method has another handy feature. Let’s remove fit2 one more time to see. rm(fit2) If you’ve fit a brm() model once and saved the results with file, executing the same brm() code will not re-fit the model. Rather, it will just load and return the model from the .rds file. fit2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex*hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15, file = &quot;fit2&quot;) It takes just a fraction of a second. Once again, we’re ready to work with the fit. fit2$formula ## tarsus ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) And if you’d like to remind yourself what the name of that external file was, you can extract it from the brm() fit object. fit2$file ## [1] &quot;fit2.rds&quot; Also, see Gavin Simpson’s blog post A better way of saving and loading objects in R for a discussion on the distinction between .rda and .rds files. 15.3 Build your models slowly The model from Bürkner’s vignette, fit1, was no joke. If you wanted to be verbose about it, it was a multilevel, multivariate, multivariable model. It had a cross-classified multilevel structure, two predictors (for each criterion), and two criteria. Not only is that a lot to keep track of, there’s a whole lot of places for things to go wrong. Even if that was the final model I was interested in as a scientist, I still wouldn’t start with it. I’d build up incrementally, just to make sure nothing looked fishy. One place to start would be a simple intercepts-only model. fit0 &lt;- brm(mvbind(tarsus, back) ~ 1, data = BTdata, chains = 2, cores = 2, file = &quot;fit0&quot;) plot(fit0) print(fit0) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ 1 ## back ~ 1 ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## tarsus_Intercept -0.00 0.03 -0.07 0.07 2563 1.00 ## back_Intercept -0.00 0.04 -0.07 0.07 2242 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma_tarsus 1.00 0.02 0.96 1.05 2566 1.00 ## sigma_back 1.00 0.03 0.95 1.05 2675 1.00 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## rescor(tarsus,back) -0.03 0.03 -0.10 0.04 2453 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If the chains look good and the summary statistics look like what I’d expect, I’m on good footing to keep building up to the model I really care about. The results from this model, for example, suggest that both criteria were standardized (i.e., intercepts at 0 and \\(\\sigma\\)s at 1). If that wasn’t what I intended, I’d rather catch it here than spend five minutes fitting the more complicated fit1 model, the parameters for which are sufficiently complicated that I may have had trouble telling what scale the data were on. Note, this is not the same as \\(p\\)-hacking or wandering aimlessly down the garden of forking paths. We are not chasing the flashiest model to put in a paper. Rather, this is just good pragmatic data science. If you start off with a theoretically-justified but complicated model and run into computation problems or produce odd-looking estimates, it won’t be clear where things went awry. When you build up, step by step, it’s easier to catch data cleaning failures, coding goofs and the like. So, when I’m working on a project, I fit one or a few simplified models before fitting my complicated model of theoretical interest. This is especially the case when I’m working with model types that are new to me or that I haven’t worked with in a while. I document each step in my R Notebook files and I save the fit objects for each in external files. I have caught surprises this way. Hopefully this will help you catch your mistakes, too. 15.4 Look at your data Relatedly, and perhaps even a precursor, you should always plot your data before fitting a model. There were plenty examples of this in the text, but it’s worth of making explicit. Simple summary statistics are great, but they’re not enough. For an entetrtaining exposition, check out Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. Though it might make for a great cocktail party story, I’d hate to pollute the literature with a linear model based on a set of dinosaur-shaped data. 15.5 Use the 0 + intercept syntax We covered this a little in the last couple chapters, but it’s easy to miss. If your real-world model has predictors (i.e., isn’t an intercept-only model), it’s important to keep track of how you have centered those predictors. When you specify a prior for a brms Intercept (i.e., an intercept resulting from the y ~ x or y ~ 1 + x style of syntax), that prior is applied under the presumption all the predictors are mean centered. In the Population-level (‘fixed’) effects subsection of the set_prior section of the brms reference manual (version 2.8.0), we read: Note that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means. To treat the intercept as an ordinary population-level effect and avoid the centering parameterization, use 0 + intercept on the right-hand side of the model formula. (p. 153) We get a little more information from the Parameterization of the population-level intercept subsection of the brmsformula section: This behavior can be avoided by using the reserved (and internally generated) variable intercept. Instead of y ~ x, you may write y ~ 0 + intercept + x. This way, priors can be defined on the real intercept, directly. In addition, the intercept is just treated as an ordinary population-level effect and thus priors defined on b will also apply to it. Note that this parameterization may be less efficient than the default parameterization discussed above. (p. 32) We didn’t bother with this for most of the project because our priors on the Intercept were often vague and the predictors were often on small enough scales (e.g., the mean of a dummy variable is close to 0) that it just didn’t matter. But this will not always be the case. Set your Intercept priors with care. There’s also the flip side of the issue. If there’s no strong reason not to, consider mean-centering or even standardizing your predictors. Not only will that solve the Intercept prior issue, but it often results in more meaningful parameter estimates. 15.6 Annotate your workflow In a typical model-fitting file, I’ll load my data, perhaps transform the data a bit, fit several models, and examine the output of each with trace plots, model summaries, information criteria, and the like. In my early days, I just figured each of these steps were self-explanatory. Nope. “In every project you have at least one other collaborator; future-you. You don’t want future-you to curse past-you.” My experience was that even a couple weeks between taking a break from a project and restarting it was enough time to make my earlier files confusing. And they were my files. I now start each R Notebook document with an introductory paragraph or two explaining exactly what the purpose of the file is. I separate my major sections by headers and subheaders. My working R Notebook files are peppered with bullets, sentences, and full on paragraphs between code blocks. 15.7 Annotate your code This idea is implicit in McElreath’s text. But it’s easy to miss the message. I know I did, at first. I find this is especially important for data wrangling. I’m a tidyverse guy and, for me, the big-money verbs like mutate(), gather(), select(), filter(), group_by(), and summarise() take care of the bulk of my data wrangling. But every once and a while I need to do something less common, like with str_extract() or case_when(). And when I end up using a new or less familiar function, I typically annotate right in the code and even sometimes leave a hyperlink to some R-bloggers post or stackoverflow question that explained how to use it. 15.8 Break up your workflow I’ve also learned to break up my projects into multiple R Notebook files. If you have a small project for which you just want a quick and dirty plot, fine, do it all in one file. My typical project has: A primary data cleaning file A file with basic descriptive statistics and the like At least one primary analysis file Possible secondary and tertiary analysis files A file or two for my major figures A file explaining and depicting my priors, often accompanied by my posteriors, for comparison Putting all that information in one R Notebook file would be overwhelming. Your workflow might well look different, but hopefully you get the idea. You don’t want working files with thousands of lines of code. And mainly to keep Jenny Bryan from setting my computer on fire, I’m also getting into the habit of organizing all these interconnected files with help from R Studio Projects, which you can learn even more about from this chapter in R4DS. 15.9 Read Gelman’s blog Yes, that Gelman. Actually, I started reading Gelman’s blog around the same time I dove into McElreath’s text. But if this isn’t the case for you, it’s time to correct that evil. My graduate mentor often recalled how transformative his first academic conference was. He was an undergrad at the time and it was his first experience meeting and talking with the people whose names he’d seen in his text books. He learned that science was an ongoing conversation among living scientists and–at that time–the best place to take part in that conversation was at conferences. Times keep changing. Nowadays, the living conversation of science occurs online on social media and in blogs. One of the hottest places to find scientists conversing about Bayesian statistics and related methods is Gelman’s blog. The posts are great. But a lot of the action is in the comments sections, too. 15.10 Check out other social media, too If you’re not on it, consider joining academic twitter. The word on the street is correct. Twitter can be rage-fueled dumpster fire. But if you’re selective about who you follow, it’s a great place to lean from and connect with your academic heroes. If you’re a fan of this project, here’s a list of some of the people you might want to follow: Richard McElreath Paul Bürkner Aki Vehtari Dan Simpson Michael Bentacourt Hadley Wickham Yihui Xie Jenny Bryan Roger Peng Mara Averick Matthew Kay Matti Vuorre Tristan Mahr Danielle Navarro I’m on twitter, too. If you’re on facebook and in the social sciences, you might check out the Bayesian Inference in Psychology group. It hasn’t been terribly active, as of late. But there are a lot of great folks to connect with, there. I’ve already mentioned Gelman’s blog. McElreath has one, too. He posts infrequently, but it’s usually pretty good when he does. Also, do check out the Stan Forums. They have a special brms tag there, under which you can find all kinds of hot brms talk. But if you’re new to the world of asking for help with your code online, you might acquaint yourself with the notion of a minimally reproducible example. In short, a good minimally reproducible example helps others help you. If you fail to do this, prepare for some skark. 15.11 Parting wisdom Okay, that’s enough from me. Let’s start wrapping this project up with some McElreath. There is an aspect of science that you do personally control: openness. Pre-plan your research together with the statistical analysis. Doing so will improve both the research design and the statistics. Document it in the form of a mock analysis that you would not be ashamed to share with a colleague. Register it publicly, perhaps in a simple repository, like Github or any other. But your webpage will do just fine, as well. Then collect the data. Then analyze the data as planned. If you must change the plan, that’s fine. But document the changes and justify them. Provide all of the data and scripts necessary to repeat your analysis. Do not provide scripts and data “on request,” but rather put them online so reviewers of your paper can access them without your interaction. There are of course cases in which full data cannot be released, due to privacy concerns. But the bulk of science is not of that sort. The data and its analysis are the scientific product. The paper is just an advertisement. If you do your honest best to design, conduct, and document your research, so that others can build directly upon it, you can make a difference. (p. 443) Toward that end, also check out the OSF and their YouTube channel, here. Katie Corker gets the last words: “Open science is stronger because we’re doing this together.” Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=C ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 ## [7] tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 brms_2.9.0 Rcpp_1.0.1 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-144 matrixStats_0.54.0 xts_0.11-2 lubridate_1.7.4 ## [5] threejs_0.3.1 httr_1.4.0 rstan_2.18.2 tools_3.6.3 ## [9] backports_1.1.4 R6_2.4.0 DT_0.7 lazyeval_0.2.2 ## [13] colorspace_1.4-1 withr_2.1.2 tidyselect_0.2.5 gridExtra_2.3 ## [17] prettyunits_1.0.2 processx_3.3.1 Brobdingnag_1.2-6 compiler_3.6.3 ## [21] rvest_0.3.4 cli_1.1.0 xml2_1.2.0 shinyjs_1.0 ## [25] labeling_0.3 colourpicker_1.0 bookdown_0.11 scales_1.1.1.9000 ## [29] dygraphs_1.1.1.6 mvtnorm_1.0-10 ggridges_0.5.1 callr_3.2.0 ## [33] digest_0.6.19 StanHeaders_2.18.1 rmarkdown_1.13 base64enc_0.1-3 ## [37] pkgconfig_2.0.2 htmltools_0.3.6 readxl_1.3.1 htmlwidgets_1.3 ## [41] rlang_0.4.0 rstudioapi_0.10 shiny_1.3.2 farver_2.0.3 ## [45] generics_0.0.2 zoo_1.8-6 jsonlite_1.6 crosstalk_1.0.0 ## [49] gtools_3.8.1 inline_0.3.15 magrittr_1.5 loo_2.1.0 ## [53] bayesplot_1.7.0 Matrix_1.2-17 munsell_0.5.0 abind_1.4-5 ## [57] lifecycle_0.1.0 stringi_1.4.3 yaml_2.2.0 pkgbuild_1.0.3 ## [61] plyr_1.8.4 grid_3.6.3 parallel_3.6.3 promises_1.0.1 ## [65] crayon_1.3.4 miniUI_0.1.1.1 lattice_0.20-38 haven_2.1.0 ## [69] hms_0.4.2 knitr_1.23 ps_1.3.0 pillar_1.4.1 ## [73] igraph_1.2.4.1 markdown_1.0 shinystan_2.5.0 reshape2_1.4.3 ## [77] codetools_0.2-16 stats4_3.6.3 rstantools_1.5.1 glue_1.3.1 ## [81] evaluate_0.14 modelr_0.1.4 httpuv_1.5.1 cellranger_1.1.0 ## [85] gtable_0.3.0 assertthat_0.2.1 xfun_0.7 mime_0.7 ## [89] xtable_1.8-4 broom_0.5.2 coda_0.19-2 later_0.8.0 ## [93] rsconnect_0.8.13 shinythemes_1.1.2 bridgesampling_0.6-0 "]
]
